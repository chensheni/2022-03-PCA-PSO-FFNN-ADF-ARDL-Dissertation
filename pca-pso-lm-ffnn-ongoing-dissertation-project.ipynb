{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "6931b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# https://www.5axxw.com/wiki/content/aeduov\n",
    "import pandas_ta as ta\n",
    "import pandas_datareader.data as pdr\n",
    "\n",
    "import datetime   \n",
    "import fix_yahoo_finance as yf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns',None) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1924d90",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "c3bb1940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from previously downloaded files\n",
    "\n",
    "# 7 advanced economies\n",
    "df_US          = pd.read_csv('./US.csv')\n",
    "df_Canada      = pd.read_csv('./Canada.csv')\n",
    "df_Japan       = pd.read_csv('./Japan.csv')\n",
    "df_Hong_Kong   = pd.read_csv('./Hong_Kong.csv')\n",
    "df_Spain       = pd.read_csv('./Spain.csv')\n",
    "df_France      = pd.read_csv('./France.csv')\n",
    "df_Netherlands = pd.read_csv('./Netherlands.csv')\n",
    "\n",
    "# 6 emerging economies\n",
    "df_Brazil      = pd.read_csv('./Brazil.csv')\n",
    "df_Mexico      = pd.read_csv('./Mexico.csv')\n",
    "df_China       = pd.read_csv('./China.csv')\n",
    "df_Turkey      = pd.read_csv('./Turkey.csv')\n",
    "df_Indonesia   = pd.read_csv('./Indonesia.csv')\n",
    "df_India       = pd.read_csv('./India.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "edbfbfdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>return_next_day</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>1133.869995</td>\n",
       "      <td>1116.560059</td>\n",
       "      <td>1116.560059</td>\n",
       "      <td>1132.989990</td>\n",
       "      <td>3991400000</td>\n",
       "      <td>1136.520020</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>1136.630005</td>\n",
       "      <td>1129.660034</td>\n",
       "      <td>1132.660034</td>\n",
       "      <td>1136.520020</td>\n",
       "      <td>2491020000</td>\n",
       "      <td>1137.140015</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>1139.189941</td>\n",
       "      <td>1133.949951</td>\n",
       "      <td>1135.709961</td>\n",
       "      <td>1137.140015</td>\n",
       "      <td>4972660000</td>\n",
       "      <td>1141.689941</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>1142.459961</td>\n",
       "      <td>1131.319946</td>\n",
       "      <td>1136.270020</td>\n",
       "      <td>1141.689941</td>\n",
       "      <td>5270680000</td>\n",
       "      <td>1144.979980</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-08</td>\n",
       "      <td>1145.390015</td>\n",
       "      <td>1136.219971</td>\n",
       "      <td>1140.520020</td>\n",
       "      <td>1144.979980</td>\n",
       "      <td>4389590000</td>\n",
       "      <td>1146.979980</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2764</th>\n",
       "      <td>2020-12-24</td>\n",
       "      <td>3703.820068</td>\n",
       "      <td>3689.320068</td>\n",
       "      <td>3694.030029</td>\n",
       "      <td>3703.060059</td>\n",
       "      <td>1885090000</td>\n",
       "      <td>3735.360107</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2765</th>\n",
       "      <td>2020-12-28</td>\n",
       "      <td>3740.510010</td>\n",
       "      <td>3723.030029</td>\n",
       "      <td>3723.030029</td>\n",
       "      <td>3735.360107</td>\n",
       "      <td>3527460000</td>\n",
       "      <td>3727.040039</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2766</th>\n",
       "      <td>2020-12-29</td>\n",
       "      <td>3756.120117</td>\n",
       "      <td>3723.310059</td>\n",
       "      <td>3750.010010</td>\n",
       "      <td>3727.040039</td>\n",
       "      <td>3387030000</td>\n",
       "      <td>3732.040039</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2767</th>\n",
       "      <td>2020-12-30</td>\n",
       "      <td>3744.629883</td>\n",
       "      <td>3730.209961</td>\n",
       "      <td>3736.189941</td>\n",
       "      <td>3732.040039</td>\n",
       "      <td>3145200000</td>\n",
       "      <td>3756.070068</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2768</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>3760.199951</td>\n",
       "      <td>3726.879883</td>\n",
       "      <td>3733.270020</td>\n",
       "      <td>3756.070068</td>\n",
       "      <td>3172510000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2769 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        datetime         high          low         open        close  \\\n",
       "0     2010-01-04  1133.869995  1116.560059  1116.560059  1132.989990   \n",
       "1     2010-01-05  1136.630005  1129.660034  1132.660034  1136.520020   \n",
       "2     2010-01-06  1139.189941  1133.949951  1135.709961  1137.140015   \n",
       "3     2010-01-07  1142.459961  1131.319946  1136.270020  1141.689941   \n",
       "4     2010-01-08  1145.390015  1136.219971  1140.520020  1144.979980   \n",
       "...          ...          ...          ...          ...          ...   \n",
       "2764  2020-12-24  3703.820068  3689.320068  3694.030029  3703.060059   \n",
       "2765  2020-12-28  3740.510010  3723.030029  3723.030029  3735.360107   \n",
       "2766  2020-12-29  3756.120117  3723.310059  3750.010010  3727.040039   \n",
       "2767  2020-12-30  3744.629883  3730.209961  3736.189941  3732.040039   \n",
       "2768  2020-12-31  3760.199951  3726.879883  3733.270020  3756.070068   \n",
       "\n",
       "          volume  return_next_day region  \n",
       "0     3991400000      1136.520020     US  \n",
       "1     2491020000      1137.140015     US  \n",
       "2     4972660000      1141.689941     US  \n",
       "3     5270680000      1144.979980     US  \n",
       "4     4389590000      1146.979980     US  \n",
       "...          ...              ...    ...  \n",
       "2764  1885090000      3735.360107     US  \n",
       "2765  3527460000      3727.040039     US  \n",
       "2766  3387030000      3732.040039     US  \n",
       "2767  3145200000      3756.070068     US  \n",
       "2768  3172510000              NaN     US  \n",
       "\n",
       "[2769 rows x 8 columns]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create df_full containing data of all the economies\n",
    "df_list     = [df_US,df_Canada,df_Japan,df_Hong_Kong,df_Spain,df_France,df_Netherlands,\n",
    "               df_Brazil,df_Mexico,df_China,df_Turkey,df_Indonesia,df_India]\n",
    "region_list = ['US','Canada','Japan','Hong_Kong','Spain','France','Netherlands',\n",
    "               'Brazil','Mexico','China','Turkey','Indonesia','India']\n",
    "\n",
    "# add region name and change col names\n",
    "for df, region in zip(df_list, region_list):\n",
    "    df['region'] = region\n",
    "    df.rename(columns = {'Adj Close':'return_next_day','Date':'datetime','High':'high',\n",
    "                         'Low':'low','Open':'open','Close':'close','Volume':'volume'}, \n",
    "              inplace = True)\n",
    "    \n",
    "# select the data within the observation period 2010-2020\n",
    "for i, df in enumerate(df_list):\n",
    "    selection_logic = (df.datetime >= '2010-01-01') & (df.datetime <= '2020-12-31')\n",
    "    df_list[i] = df.loc[selection_logic,:].reset_index(drop=True)\n",
    "    # create col 'return_next_day'\n",
    "    df_list[i]['return_next_day'] = np.nan\n",
    "    df_list[i].loc[:,'return_next_day'] = df_list[i]['close'].shift(-1)\n",
    "    # df_list[i].loc[:,'return_next_day'] = df_list[i]['close'].shift(-1) - df_list[i]['close']\n",
    "    \n",
    "# df_full = pd.concat(df_list, ignore_index = True)\n",
    "df_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "e950ed4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many instances in each dataset\n",
      "US:           2769 \n",
      "Canada:       2760 \n",
      "Japan:        2690 \n",
      "Hong_Kong:    2705 \n",
      "Spain:        2812 \n",
      "France:       2810 \n",
      "Netherlands:  2813 \n",
      "Brazil:       2716 \n",
      "Mexico:       2753 \n",
      "China:        2670 \n",
      "Turkey:       2762 \n",
      "Indonesia:    2680 \n",
      "India:        2701 \n"
     ]
    }
   ],
   "source": [
    "# count how many instances in each region\n",
    "print('How many instances in each dataset')\n",
    "for region, df in zip(region_list, df_list):\n",
    "    print('{:<12} {:>5d} '.format(region+':', df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08589297",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2796b1",
   "metadata": {},
   "source": [
    "## technical indicators generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "9d55a034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pandas_ta package as the origion paper indicates\n",
    "n = 10; k = 15\n",
    "\n",
    "# create the custom strategy for 20 technical indicators\n",
    "CustomStrategy = ta.Strategy(\n",
    "    name = 'could use help(ta.xxx) to check the definition & calculation',\n",
    "    ta=[\n",
    "        {\"kind\": \"sma\",  \"length\": n},\n",
    "        {\"kind\": \"ema\",  \"length\": n},\n",
    "        {\"kind\": \"macd\", \"fast\"  : n , \"slow\": k},\n",
    "        {\"kind\": \"adx\",  \"length\": n},\n",
    "        {\"kind\": \"cci\",  \"length\": n},\n",
    "        {\"kind\": \"mom\",  \"length\": n},\n",
    "        {\"kind\": \"roc\",  \"length\": n},\n",
    "        {\"kind\": \"rsi\",  \"length\": n},\n",
    "        {\"kind\": \"tsi\",  \"length\": n},\n",
    "        {\"kind\": \"kdj\",  \"length\": n}, # it includes k%, d%, j%\n",
    "        {\"kind\": \"atr\",  \"length\": n},\n",
    "        {\"kind\": \"ui\" ,  \"length\": n},\n",
    "        {\"kind\": \"ad\" ,  \"length\": n},\n",
    "        {\"kind\": \"obv\",  \"length\": n}]\n",
    ")\n",
    "\n",
    "# generate technical indicators\n",
    "for i, df in enumerate(df_list):\n",
    "    df_list[i].ta.strategy(CustomStrategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "f2da4f60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>datetime</th>\n",
       "      <th>return_next_day</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>SMA_10</th>\n",
       "      <th>EMA_10</th>\n",
       "      <th>MACD_10_15_9</th>\n",
       "      <th>ADX_10</th>\n",
       "      <th>CCI_10_0.015</th>\n",
       "      <th>MOM_10</th>\n",
       "      <th>ROC_10</th>\n",
       "      <th>RSI_10</th>\n",
       "      <th>TSI_13_25_13</th>\n",
       "      <th>K_10_3</th>\n",
       "      <th>D_10_3</th>\n",
       "      <th>J_10_3</th>\n",
       "      <th>ATRr_10</th>\n",
       "      <th>UI_10</th>\n",
       "      <th>AD</th>\n",
       "      <th>OBV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>2010-02-08</td>\n",
       "      <td>1070.520020</td>\n",
       "      <td>1071.199951</td>\n",
       "      <td>1056.510010</td>\n",
       "      <td>1065.510010</td>\n",
       "      <td>1056.739990</td>\n",
       "      <td>1082.389990</td>\n",
       "      <td>1082.543306</td>\n",
       "      <td>-10.285084</td>\n",
       "      <td>57.587779</td>\n",
       "      <td>-126.654054</td>\n",
       "      <td>-40.040039</td>\n",
       "      <td>-3.650690</td>\n",
       "      <td>29.557233</td>\n",
       "      <td>-31.395445</td>\n",
       "      <td>23.118601</td>\n",
       "      <td>22.250833</td>\n",
       "      <td>24.854138</td>\n",
       "      <td>18.309764</td>\n",
       "      <td>4.539277</td>\n",
       "      <td>7.000802e+09</td>\n",
       "      <td>6.047570e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>2010-02-09</td>\n",
       "      <td>1068.130005</td>\n",
       "      <td>1079.280029</td>\n",
       "      <td>1060.060059</td>\n",
       "      <td>1060.060059</td>\n",
       "      <td>1070.520020</td>\n",
       "      <td>1080.224988</td>\n",
       "      <td>1080.357254</td>\n",
       "      <td>-9.682590</td>\n",
       "      <td>55.953294</td>\n",
       "      <td>-62.030614</td>\n",
       "      <td>-21.650024</td>\n",
       "      <td>-1.982294</td>\n",
       "      <td>38.281123</td>\n",
       "      <td>-29.437032</td>\n",
       "      <td>29.819572</td>\n",
       "      <td>24.779520</td>\n",
       "      <td>39.899674</td>\n",
       "      <td>18.765509</td>\n",
       "      <td>4.352115</td>\n",
       "      <td>7.453144e+09</td>\n",
       "      <td>1.116183e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>2010-02-10</td>\n",
       "      <td>1078.469971</td>\n",
       "      <td>1073.670044</td>\n",
       "      <td>1059.339966</td>\n",
       "      <td>1069.680054</td>\n",
       "      <td>1068.130005</td>\n",
       "      <td>1077.287988</td>\n",
       "      <td>1078.134118</td>\n",
       "      <td>-9.166996</td>\n",
       "      <td>54.612874</td>\n",
       "      <td>-63.139191</td>\n",
       "      <td>-29.369995</td>\n",
       "      <td>-2.676082</td>\n",
       "      <td>37.388797</td>\n",
       "      <td>-28.068852</td>\n",
       "      <td>32.959493</td>\n",
       "      <td>27.510335</td>\n",
       "      <td>43.857808</td>\n",
       "      <td>18.291329</td>\n",
       "      <td>4.225706</td>\n",
       "      <td>8.417354e+09</td>\n",
       "      <td>6.910380e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>2010-02-11</td>\n",
       "      <td>1075.510010</td>\n",
       "      <td>1080.040039</td>\n",
       "      <td>1060.589966</td>\n",
       "      <td>1067.099976</td>\n",
       "      <td>1078.469971</td>\n",
       "      <td>1076.681982</td>\n",
       "      <td>1078.195182</td>\n",
       "      <td>-8.002039</td>\n",
       "      <td>52.043317</td>\n",
       "      <td>-22.071395</td>\n",
       "      <td>-6.060059</td>\n",
       "      <td>-0.558773</td>\n",
       "      <td>43.697528</td>\n",
       "      <td>-25.530705</td>\n",
       "      <td>40.776666</td>\n",
       "      <td>31.936939</td>\n",
       "      <td>58.456122</td>\n",
       "      <td>18.414358</td>\n",
       "      <td>3.886038</td>\n",
       "      <td>1.210772e+10</td>\n",
       "      <td>1.131125e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>1094.869995</td>\n",
       "      <td>1077.810059</td>\n",
       "      <td>1062.969971</td>\n",
       "      <td>1075.949951</td>\n",
       "      <td>1075.510010</td>\n",
       "      <td>1076.845984</td>\n",
       "      <td>1077.706969</td>\n",
       "      <td>-7.154351</td>\n",
       "      <td>49.815438</td>\n",
       "      <td>-23.171415</td>\n",
       "      <td>1.640015</td>\n",
       "      <td>0.152720</td>\n",
       "      <td>42.340556</td>\n",
       "      <td>-23.735646</td>\n",
       "      <td>44.347519</td>\n",
       "      <td>36.076600</td>\n",
       "      <td>60.889358</td>\n",
       "      <td>18.106827</td>\n",
       "      <td>3.365921</td>\n",
       "      <td>1.497868e+10</td>\n",
       "      <td>7.150570e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  region    datetime  return_next_day         high          low         open  \\\n",
       "0     US  2010-02-08      1070.520020  1071.199951  1056.510010  1065.510010   \n",
       "1     US  2010-02-09      1068.130005  1079.280029  1060.060059  1060.060059   \n",
       "2     US  2010-02-10      1078.469971  1073.670044  1059.339966  1069.680054   \n",
       "3     US  2010-02-11      1075.510010  1080.040039  1060.589966  1067.099976   \n",
       "4     US  2010-02-12      1094.869995  1077.810059  1062.969971  1075.949951   \n",
       "\n",
       "         close       SMA_10       EMA_10  MACD_10_15_9     ADX_10  \\\n",
       "0  1056.739990  1082.389990  1082.543306    -10.285084  57.587779   \n",
       "1  1070.520020  1080.224988  1080.357254     -9.682590  55.953294   \n",
       "2  1068.130005  1077.287988  1078.134118     -9.166996  54.612874   \n",
       "3  1078.469971  1076.681982  1078.195182     -8.002039  52.043317   \n",
       "4  1075.510010  1076.845984  1077.706969     -7.154351  49.815438   \n",
       "\n",
       "   CCI_10_0.015     MOM_10    ROC_10     RSI_10  TSI_13_25_13     K_10_3  \\\n",
       "0   -126.654054 -40.040039 -3.650690  29.557233    -31.395445  23.118601   \n",
       "1    -62.030614 -21.650024 -1.982294  38.281123    -29.437032  29.819572   \n",
       "2    -63.139191 -29.369995 -2.676082  37.388797    -28.068852  32.959493   \n",
       "3    -22.071395  -6.060059 -0.558773  43.697528    -25.530705  40.776666   \n",
       "4    -23.171415   1.640015  0.152720  42.340556    -23.735646  44.347519   \n",
       "\n",
       "      D_10_3     J_10_3    ATRr_10     UI_10            AD           OBV  \n",
       "0  22.250833  24.854138  18.309764  4.539277  7.000802e+09  6.047570e+09  \n",
       "1  24.779520  39.899674  18.765509  4.352115  7.453144e+09  1.116183e+10  \n",
       "2  27.510335  43.857808  18.291329  4.225706  8.417354e+09  6.910380e+09  \n",
       "3  31.936939  58.456122  18.414358  3.886038  1.210772e+10  1.131125e+10  \n",
       "4  36.076600  60.889358  18.106827  3.365921  1.497868e+10  7.150570e+09  "
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean dataframes\n",
    "for i, df in enumerate(df_list):\n",
    "    cols = ['region','datetime','return_next_day','high','low','open','close','SMA_10',\n",
    "            'EMA_10','MACD_10_15_9','ADX_10','CCI_10_0.015','MOM_10','ROC_10','RSI_10',\n",
    "            'TSI_13_25_13','K_10_3','D_10_3','J_10_3','ATRr_10','UI_10','AD','OBV']\n",
    "    # delete rows b/c loosing obeservations\n",
    "    df_list[i] = df_list[i].loc[24:df_list[i].shape[0]-2,cols].reset_index(drop=True)\n",
    "\n",
    "df_list[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8855f56d",
   "metadata": {},
   "source": [
    "## normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "c9973512",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>datetime</th>\n",
       "      <th>return_next_day</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>SMA_10</th>\n",
       "      <th>EMA_10</th>\n",
       "      <th>MACD_10_15_9</th>\n",
       "      <th>ADX_10</th>\n",
       "      <th>CCI_10_0.015</th>\n",
       "      <th>MOM_10</th>\n",
       "      <th>ROC_10</th>\n",
       "      <th>RSI_10</th>\n",
       "      <th>TSI_13_25_13</th>\n",
       "      <th>K_10_3</th>\n",
       "      <th>D_10_3</th>\n",
       "      <th>J_10_3</th>\n",
       "      <th>ATRr_10</th>\n",
       "      <th>UI_10</th>\n",
       "      <th>AD</th>\n",
       "      <th>OBV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>2010-02-08</td>\n",
       "      <td>1070.520020</td>\n",
       "      <td>0.014046</td>\n",
       "      <td>0.016769</td>\n",
       "      <td>0.013907</td>\n",
       "      <td>0.012592</td>\n",
       "      <td>0.011971</td>\n",
       "      <td>0.011981</td>\n",
       "      <td>0.663575</td>\n",
       "      <td>0.852338</td>\n",
       "      <td>0.299200</td>\n",
       "      <td>0.597410</td>\n",
       "      <td>0.465038</td>\n",
       "      <td>0.234001</td>\n",
       "      <td>0.070086</td>\n",
       "      <td>0.198053</td>\n",
       "      <td>0.140825</td>\n",
       "      <td>0.341286</td>\n",
       "      <td>0.065068</td>\n",
       "      <td>0.242227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>2010-02-09</td>\n",
       "      <td>1068.130005</td>\n",
       "      <td>0.017013</td>\n",
       "      <td>0.018075</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.017672</td>\n",
       "      <td>0.011157</td>\n",
       "      <td>0.011159</td>\n",
       "      <td>0.667457</td>\n",
       "      <td>0.823227</td>\n",
       "      <td>0.412781</td>\n",
       "      <td>0.613287</td>\n",
       "      <td>0.504351</td>\n",
       "      <td>0.346376</td>\n",
       "      <td>0.089145</td>\n",
       "      <td>0.269842</td>\n",
       "      <td>0.170089</td>\n",
       "      <td>0.443280</td>\n",
       "      <td>0.067823</td>\n",
       "      <td>0.232220</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.005466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>2010-02-10</td>\n",
       "      <td>1078.469971</td>\n",
       "      <td>0.014953</td>\n",
       "      <td>0.017810</td>\n",
       "      <td>0.015439</td>\n",
       "      <td>0.016791</td>\n",
       "      <td>0.010053</td>\n",
       "      <td>0.010322</td>\n",
       "      <td>0.670780</td>\n",
       "      <td>0.799354</td>\n",
       "      <td>0.410833</td>\n",
       "      <td>0.606622</td>\n",
       "      <td>0.488003</td>\n",
       "      <td>0.334881</td>\n",
       "      <td>0.102459</td>\n",
       "      <td>0.303481</td>\n",
       "      <td>0.201692</td>\n",
       "      <td>0.470113</td>\n",
       "      <td>0.064957</td>\n",
       "      <td>0.225461</td>\n",
       "      <td>0.001039</td>\n",
       "      <td>0.000922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>2010-02-11</td>\n",
       "      <td>1075.510010</td>\n",
       "      <td>0.017292</td>\n",
       "      <td>0.018269</td>\n",
       "      <td>0.014491</td>\n",
       "      <td>0.020602</td>\n",
       "      <td>0.009825</td>\n",
       "      <td>0.010345</td>\n",
       "      <td>0.678287</td>\n",
       "      <td>0.753590</td>\n",
       "      <td>0.483013</td>\n",
       "      <td>0.626746</td>\n",
       "      <td>0.537893</td>\n",
       "      <td>0.416145</td>\n",
       "      <td>0.127159</td>\n",
       "      <td>0.387229</td>\n",
       "      <td>0.252919</td>\n",
       "      <td>0.569076</td>\n",
       "      <td>0.065700</td>\n",
       "      <td>0.207301</td>\n",
       "      <td>0.003744</td>\n",
       "      <td>0.005626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>1094.869995</td>\n",
       "      <td>0.016473</td>\n",
       "      <td>0.019145</td>\n",
       "      <td>0.017742</td>\n",
       "      <td>0.019511</td>\n",
       "      <td>0.009887</td>\n",
       "      <td>0.010162</td>\n",
       "      <td>0.683750</td>\n",
       "      <td>0.713911</td>\n",
       "      <td>0.481080</td>\n",
       "      <td>0.633394</td>\n",
       "      <td>0.554659</td>\n",
       "      <td>0.398666</td>\n",
       "      <td>0.144628</td>\n",
       "      <td>0.425485</td>\n",
       "      <td>0.300826</td>\n",
       "      <td>0.585571</td>\n",
       "      <td>0.063842</td>\n",
       "      <td>0.179492</td>\n",
       "      <td>0.005849</td>\n",
       "      <td>0.001179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  region    datetime  return_next_day      high       low      open     close  \\\n",
       "0     US  2010-02-08      1070.520020  0.014046  0.016769  0.013907  0.012592   \n",
       "1     US  2010-02-09      1068.130005  0.017013  0.018075  0.011905  0.017672   \n",
       "2     US  2010-02-10      1078.469971  0.014953  0.017810  0.015439  0.016791   \n",
       "3     US  2010-02-11      1075.510010  0.017292  0.018269  0.014491  0.020602   \n",
       "4     US  2010-02-12      1094.869995  0.016473  0.019145  0.017742  0.019511   \n",
       "\n",
       "     SMA_10    EMA_10  MACD_10_15_9    ADX_10  CCI_10_0.015    MOM_10  \\\n",
       "0  0.011971  0.011981      0.663575  0.852338      0.299200  0.597410   \n",
       "1  0.011157  0.011159      0.667457  0.823227      0.412781  0.613287   \n",
       "2  0.010053  0.010322      0.670780  0.799354      0.410833  0.606622   \n",
       "3  0.009825  0.010345      0.678287  0.753590      0.483013  0.626746   \n",
       "4  0.009887  0.010162      0.683750  0.713911      0.481080  0.633394   \n",
       "\n",
       "     ROC_10    RSI_10  TSI_13_25_13    K_10_3    D_10_3    J_10_3   ATRr_10  \\\n",
       "0  0.465038  0.234001      0.070086  0.198053  0.140825  0.341286  0.065068   \n",
       "1  0.504351  0.346376      0.089145  0.269842  0.170089  0.443280  0.067823   \n",
       "2  0.488003  0.334881      0.102459  0.303481  0.201692  0.470113  0.064957   \n",
       "3  0.537893  0.416145      0.127159  0.387229  0.252919  0.569076  0.065700   \n",
       "4  0.554659  0.398666      0.144628  0.425485  0.300826  0.585571  0.063842   \n",
       "\n",
       "      UI_10        AD       OBV  \n",
       "0  0.242227  0.000000  0.000000  \n",
       "1  0.232220  0.000332  0.005466  \n",
       "2  0.225461  0.001039  0.000922  \n",
       "3  0.207301  0.003744  0.005626  \n",
       "4  0.179492  0.005849  0.001179  "
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalization\n",
    "for i, df in enumerate(df_list):\n",
    "    df_list[i].iloc[:,3:] = df_list[i].iloc[:,3:].apply(\n",
    "        lambda x: (x - np.min(x)) / (np.max(x) - np.min(x)))\n",
    "    \n",
    "df_list[0].head()\n",
    "\n",
    "#array_X = np.array(df_X_nor.T)\n",
    "#array_y = np.array(df_y_nor.T)/1000 \n",
    "# i think it would fit ANN more? ie., more close to 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "1777ccf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many missing values in each dataset\n",
      "US:              0 \n",
      "Canada:          0 \n",
      "Japan:           2 \n",
      "Hong_Kong:       5 \n",
      "Spain:           0 \n",
      "France:          0 \n",
      "Netherlands:     0 \n",
      "Brazil:          2 \n",
      "Mexico:          0 \n",
      "China:           0 \n",
      "Turkey:          5 \n",
      "Indonesia:       0 \n",
      "India:           0 \n",
      "\n",
      "and all blanks have been filled\n"
     ]
    }
   ],
   "source": [
    "# there's a missing value issue, but with a very small proportion (5/2000)\n",
    "# df_list_n[10]['UI_10'][df_list_n[10]['UI_10'].isna()==True]\n",
    "print('How many missing values in each dataset')\n",
    "for region, df in zip(region_list, df_list):\n",
    "    print('{:<12} {:>5d} '.format(region+':', df.isna().sum().sum()))\n",
    "    \n",
    "# fill blanks using the most recent values\n",
    "for i, df in enumerate(df_list):\n",
    "    df_list[i]['UI_10'] = df_list[i]['UI_10'].fillna(method = 'ffill')\n",
    "print('\\nand all blanks have been filled')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e5cf77",
   "metadata": {},
   "source": [
    "## principal components analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "da14e5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeromean(dataframe):   \n",
    "    \"\"\" \n",
    "    Args:    \n",
    "    dataframe(pd.DataFrame)\n",
    "    \n",
    "    Returns:    \n",
    "    dataframe_new(pd.DataFrame): de-meaned dataframe    \n",
    "    mean(float): mean of each features\n",
    "    \"\"\"\n",
    "    mean = np.mean(dataframe, axis = 0)     \n",
    "    dataframe_new = dataframe - mean\n",
    "    return dataframe_new, mean\n",
    "\n",
    "def percentage_to_n(eig_vals, percentage):\n",
    "    \"\"\"\n",
    "    Args:    \n",
    "    eig_vals(np.array): eigenvalues of a matrix\n",
    "    percentage(float): how much explanation needed\n",
    "    \n",
    "    Returns:    \n",
    "    pca_num(int): how many eigenvectors needed to achieve the explanation\n",
    "    \"\"\"\n",
    "    sort_eig_vals = np.sort(eig_vals)\n",
    "    sort_eig_vals = sort_eig_vals[-1::-1]\n",
    "    sum_eig_vals  = sum(sort_eig_vals)\n",
    "    \n",
    "    pca_sum = 0\n",
    "    pca_num = 0\n",
    "    \n",
    "    for i in sort_eig_vals:\n",
    "        pca_sum += i\n",
    "        pca_num += 1\n",
    "        if pca_sum >= sum_eig_vals * percentage:\n",
    "            break\n",
    "    \n",
    "    return pca_num\n",
    " \n",
    "def pca(dataframe, percentage = 0.99):\n",
    "    \"\"\"\n",
    "    Args:    \n",
    "    dataframe(pd.DataFrame)\n",
    "    percentage(float): how much explanation needed\n",
    "    \n",
    "    Returns:    \n",
    "    low_dimen_data(pd.DataFrame): dataframe after pca\n",
    "    n(int): how many eigenvectors needed to achieve the explanation\n",
    "    \"\"\"\n",
    "    dataframe_new, mean = zeromean(dataframe)\n",
    "    \n",
    "    cov_matrix = np.cov(dataframe_new, rowvar = 0)    \n",
    "    eig_vals,eig_vects = np.linalg.eig(np.mat(cov_matrix))\n",
    "    \n",
    "    # choose how many pcs needed to explain 99%\n",
    "    n = percentage_to_n(eig_vals, percentage)\n",
    "\n",
    "    # choose top n\n",
    "    eig_val_indice = np.argsort(eig_vals)\n",
    "    n_eig_val_indice = eig_val_indice[-1:-(n+1):-1] \n",
    "    \n",
    "    # reconstruct data\n",
    "    n_eig_vect = eig_vects[:, n_eig_val_indice]\n",
    "    low_dimen_data = dataframe_new @ n_eig_vect\n",
    "    return low_dimen_data, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "1f6ed591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many PCs extracted in each dataset for 99% explanation\n",
      "US:              8 PCs \n",
      "Canada:          8 PCs \n",
      "Japan:           8 PCs \n",
      "Hong_Kong:       9 PCs \n",
      "Spain:           9 PCs \n",
      "France:          9 PCs \n",
      "Netherlands:     8 PCs \n",
      "Brazil:          8 PCs \n",
      "Mexico:          9 PCs \n",
      "China:           8 PCs \n",
      "Turkey:          9 PCs \n",
      "Indonesia:       9 PCs \n",
      "India:           8 PCs \n"
     ]
    }
   ],
   "source": [
    "# x partition and y partition\n",
    "array_list_x = []\n",
    "array_list_y = []\n",
    "\n",
    "print('How many PCs extracted in each dataset for 99% explanation')\n",
    "for region, df in zip(region_list, df_list):\n",
    "    array_list_x.append(np.array(pca(df.iloc[:,3:])[0].T))\n",
    "    print('{:<12} {:>5d} PCs '.format(region+':', pca(df.iloc[:,3:])[1]))\n",
    "\n",
    "for region, df in zip(region_list, df_list):\n",
    "    array_list_y.append(np.array(df.iloc[:,2].T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a124cfb",
   "metadata": {},
   "source": [
    "# FFNN: Feed-Foreward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "18213529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function defines sigmoid activation function: f(x) = 1 / (1 + e^(-x))\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# this function defines the derivative of sigmoid function\n",
    "def deriv_sigmoid(x):\n",
    "    fx = sigmoid(x)\n",
    "    return fx * (1 - fx)\n",
    "\n",
    "# this function defines the loss function, Mean Squared Error (MSE)\n",
    "def MSE(y_true, y_prediction):\n",
    "    return ((y_true - y_prediction) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "7e34f6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function initialises the network with the structure of 5 layers\n",
    "# n1 is the number of neurons at the 1st hidden layer\n",
    "# n2 is the number of neurons at the 2nd hidden layer\n",
    "# n3 is the number of neurons at the 3rd hidden layer\n",
    "\n",
    "## could change the neurons at each layer here\n",
    "n1,  n2,  n3  = 5, 5, 5\n",
    "ann_structure = [n1, n2, n3]\n",
    "\n",
    "def parameters_reset (n1, n2, n3) :\n",
    "    \"\"\" \n",
    "    Args:    \n",
    "    n1,n2,n3(int): \n",
    "    how many neurons on each hidden layer\n",
    "    \n",
    "    Returns:    \n",
    "    W1,W2,W3,W4,b1,b2,b3,b4(float): \n",
    "    parameters set (same at the each time)\n",
    "    \"\"\"\n",
    "    # show that we are changing the global variables\n",
    "    global W1, W2, W3, W4, b1, b2, b3, b4\n",
    "    \n",
    "    # to keep the initial parameters the same\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    W1 = np.random.randn(n1, num_input) / 2\n",
    "    W2 = np.random.randn(n2, n1) / 2\n",
    "    W3 = np.random.randn(n3, n2) / 2\n",
    "    W4 = np.random.randn(1 , n3) / 2\n",
    "    \n",
    "    b1 = np.random.randn(n1, 1 ) / 2\n",
    "    b2 = np.random.randn(n2, 1 ) / 2\n",
    "    b3 = np.random.randn(n3, 1 ) / 2\n",
    "    b4 = np.random.randn(1 , 1 ) / 2\n",
    "    \n",
    "# parameters_reset (n1, n2, n3)\n",
    "    \n",
    "# dimensions = n1 * num_input + n2 * n1 + n3 * n2 + 1 * n3 + n1 + n2 + n3 + 1\n",
    "# print('hi, under this setting, totally {:} dimensions of parameter'.format(dimensions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "bc36dbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function realizes the Feedforward\n",
    "def forward_prop (a0, W1, W2, W3, W4, b1, b2, b3, b4) :\n",
    "    \"\"\" \n",
    "    Args:    \n",
    "    a0(np.array): dataset (* which should be transposed)\n",
    "    W1,W2,W3,W4,b1,b2,b3,b4(float): parameters\n",
    "    \n",
    "    Returns:    \n",
    "    a0(np.array): dataset as the same as the input\n",
    "    s1,a1,s2,a2,s3,a3,s4(np.array): results of FFNN\n",
    "    \"\"\"\n",
    "    s1 = W1 @ a0 + b1\n",
    "    a1 = sigmoid(s1) \n",
    "    s2 = W2 @ a1 + b2\n",
    "    a2 = sigmoid(s2)\n",
    "    s3 = W3 @ a2 + b3\n",
    "    a3 = sigmoid(s3)\n",
    "    s4 = W4 @ a3 + b4\n",
    "    # since this is not a classification problem\n",
    "    # I do not put activation function on the output layer\n",
    "    \n",
    "    # return multiple values\n",
    "    return a0, s1, a1, s2, a2, s3, a3, s4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae0bcee",
   "metadata": {},
   "source": [
    "# Particle Swarm Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "fc06704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function calculate MSE without gradient components\n",
    "def MSE_calculation_in_PSO(array_x, array_y, paras):\n",
    "    \"\"\" \n",
    "    Args:    \n",
    "    array_x(np.array): dataset (* which should be transposed)\n",
    "    array_y(np.array): true values (* which should be transposed)\n",
    "    paras(np.array): parameters\n",
    "    \n",
    "    Returns:    \n",
    "    MSE(float): MSE at the input parameters\n",
    "    \"\"\"\n",
    "    num_input = array_x.shape[0]\n",
    "    # transform parameters from vector to matrix\n",
    "    t1 = n1 * num_input\n",
    "    t2 = t1 + n2 * n1\n",
    "    t3 = t2 + n3 * n2\n",
    "    t4 = t3 + n3\n",
    "    t5 = t4 + n1\n",
    "    t6 = t5 + n2\n",
    "    t7 = t6 + n3\n",
    "    t8 = t7 + 1\n",
    "    \n",
    "    return MSE(array_y, forward_prop(array_x, \n",
    "                                        paras[0  : t1].reshape(n1 , num_input), \n",
    "                                        paras[t1 : t2].reshape(n2 , n1), \n",
    "                                        paras[t2 : t3].reshape(n3 , n2), \n",
    "                                        paras[t3 : t4].reshape(1  , n3), \n",
    "                                        paras[t4 : t5].reshape(n1 ,  1), \n",
    "                                        paras[t5 : t6].reshape(n2 ,  1), \n",
    "                                        paras[t6 : t7].reshape(n3 ,  1), \n",
    "                                        paras[t7 : t8].reshape(1  ,  1))[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "8e9c4fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pso(array_x, array_y):\n",
    "    \"\"\" \n",
    "    Args:    \n",
    "    array_x(np.array): dataset (* which should be transposed)\n",
    "    array_y(np.array): true values (* which should be transposed)\n",
    "    \n",
    "    Returns:    \n",
    "    W1_pso, W2_pso, W3_pso, W4_pso, b1_pso, b2_pso, b3_pso, b4_pso(np.array): \n",
    "    parameters found by PSO\n",
    "    \"\"\"\n",
    "    # 1. initialize the particles\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    n_particles = 50\n",
    "    num_input   = array_x.shape[0]\n",
    "    dimensions  = n1 * num_input + n2 * n1 + n3 * n2 + 1 * n3 + n1 + n2 + n3 + 1\n",
    "\n",
    "    # coordinate, [0,1]\n",
    "    X = np.random.rand (dimensions, n_particles) / 2\n",
    "    # velocity, from normal distribution\n",
    "    V = np.random.randn(dimensions, n_particles) / 10\n",
    "\n",
    "    # 2. period 1 initialization\n",
    "    \n",
    "    # p_best_loc: personal best location\n",
    "    # p_best_obj: personal best objective function value\n",
    "    # g_best_loc: global best location\n",
    "    # g_best_obj: global best objective function value\n",
    "\n",
    "    p_best_loc = X\n",
    "    p_best_obj = np.array([MSE_calculation_in_PSO(array_x, array_y, paras) for paras in X.T])\n",
    "    g_best_loc = p_best_loc[:,p_best_obj.argmin()]\n",
    "    g_best_obj = p_best_obj.min()\n",
    "    \n",
    "    # print('hi, at the initialization, PSO finds {:4f} as MSE'.format(g_best_obj))\n",
    "    \n",
    "    # 3. algorithm\n",
    "    \n",
    "    # parameters setting\n",
    "    # w: the inertia weight constant\n",
    "    # it decides how much the particle should keep on with its previous velocity\n",
    "    # c1: cognitive coefficient\n",
    "    # it decides how much the particle should insist the search result of the particle\n",
    "    # c2: social coefficients\n",
    "    # it decides how much the particle should insist the search result of the swarm\n",
    "\n",
    "    w, c1, c2, seed = 0.8, 0.1, 0.1, 0\n",
    "\n",
    "    # iterations\n",
    "    for i in range(100):\n",
    "    \n",
    "        # randomness in c1, c2\n",
    "        np.random.seed(seed)\n",
    "        r = np.random.rand(2)\n",
    "    \n",
    "        # velocity update\n",
    "        V = w * V + c1 * r[0] * (p_best_loc - X) + \\\n",
    "                    c2 * r[1] * (g_best_loc.reshape(-1,1) - X)\n",
    "        # position update\n",
    "        X = X + V\n",
    "    \n",
    "        # objective at this round\n",
    "        obj = np.array([MSE_calculation_in_PSO(array_x, array_y, paras) for paras in X.T])\n",
    "    \n",
    "        # personal best location update\n",
    "        p_best_loc[:, (obj <= p_best_obj)] = X[:, (obj <= p_best_obj)]\n",
    "        # personal best value update\n",
    "        p_best_obj = np.array([p_best_obj, obj]).min(axis=0)\n",
    "        # global best location update\n",
    "        g_best_loc = p_best_loc[:,p_best_obj.argmin()]\n",
    "        # global best value update\n",
    "        g_best_obj = p_best_obj.min()\n",
    "    \n",
    "        seed += 1\n",
    "    \n",
    "        # print('hi, at the interation of {:}, PSO finds {:.4f} as MSE'.format(i+1, g_best_obj))\n",
    "    \n",
    "    t1 = n1 * num_input\n",
    "    t2 = t1 + n2 * n1\n",
    "    t3 = t2 + n3 * n2\n",
    "    t4 = t3 + n3\n",
    "    t5 = t4 + n1\n",
    "    t6 = t5 + n2\n",
    "    t7 = t6 + n3\n",
    "    t8 = t7 + 1\n",
    "\n",
    "        \n",
    "    W1_pso = g_best_loc[0  : t1].reshape(n1 , num_input)\n",
    "    W2_pso = g_best_loc[t1 : t2].reshape(n2 , n1)\n",
    "    W3_pso = g_best_loc[t2 : t3].reshape(n3 , n2)\n",
    "    W4_pso = g_best_loc[t3 : t4].reshape(1  , n3)\n",
    "    b1_pso = g_best_loc[t4 : t5].reshape(n1 ,  1)\n",
    "    b2_pso = g_best_loc[t5 : t6].reshape(n2 ,  1)\n",
    "    b3_pso = g_best_loc[t6 : t7].reshape(n3 ,  1)\n",
    "    b4_pso = g_best_loc[t7 : t8].reshape(1  ,  1)\n",
    "    \n",
    "    return W1_pso, W2_pso, W3_pso, W4_pso, b1_pso, b2_pso, b3_pso, b4_pso\n",
    "\n",
    "    # print('\\nthe optimal parameters have been kept within those suffixed as \"_pso\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "d183ade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pso(array_x, array_y):\n",
    "    \"\"\" \n",
    "    Args:    \n",
    "    array_x(np.array): dataset (* which should be transposed)\n",
    "    array_y(np.array): true values (* which should be transposed)\n",
    "    \n",
    "    Returns:    \n",
    "    W1_pso, W2_pso, W3_pso, W4_pso, b1_pso, b2_pso, b3_pso, b4_pso(np.array): \n",
    "    parameters found by PSO\n",
    "    \"\"\"\n",
    "    # 1. initialize the particles\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    n_particles = 50\n",
    "    num_input   = array_x.shape[0]\n",
    "    dimensions  = n1 * num_input + n2 * n1 + n3 * n2 + 1 * n3 + n1 + n2 + n3 + 1\n",
    "\n",
    "    # coordinate, [0,1]\n",
    "    X = np.random.rand (dimensions, n_particles)\n",
    "    # velocity, from normal distribution\n",
    "    V = np.random.randn(dimensions, n_particles) * 0.1\n",
    "\n",
    "    # 2. period 1 initialization\n",
    "    \n",
    "    # p_best_loc: personal best location\n",
    "    # p_best_obj: personal best objective function value\n",
    "    # g_best_loc: global best location\n",
    "    # g_best_obj: global best objective function value\n",
    "\n",
    "    p_best_loc = X\n",
    "    p_best_obj = np.array([MSE_calculation_in_PSO(array_x, array_y, paras) for paras in X.T])\n",
    "    g_best_loc = p_best_loc[:,p_best_obj.argmin()]\n",
    "    g_best_obj = p_best_obj.min()\n",
    "    \n",
    "    # print('hi, at the initialization, PSO finds {:4f} as MSE'.format(g_best_obj))\n",
    "    \n",
    "    # 3. algorithm\n",
    "    \n",
    "    # parameters setting\n",
    "    # w: the inertia weight constant\n",
    "    # it decides how much the particle should keep on with its previous velocity\n",
    "    # c1: cognitive coefficient\n",
    "    # it decides how much the particle should insist the search result of the particle\n",
    "    # c2: social coefficients\n",
    "    # it decides how much the particle should insist the search result of the swarm\n",
    "\n",
    "    w, c1, c2, seed = 0.8, 0.1, 0.1, 0\n",
    "\n",
    "    # iterations\n",
    "    for i in range(100):\n",
    "    \n",
    "        # randomness in c1, c2\n",
    "        np.random.seed(seed)\n",
    "        r = np.random.rand(2)\n",
    "    \n",
    "        # velocity update\n",
    "        V = w * V + c1 * r[0] * (p_best_loc - X) + \\\n",
    "                    c2 * r[1] * (g_best_loc.reshape(-1,1) - X)\n",
    "        # position update\n",
    "        X = X + V\n",
    "    \n",
    "        # objective at this round\n",
    "        obj = np.array([MSE_calculation_in_PSO(array_x, array_y, paras) for paras in X.T])\n",
    "    \n",
    "        # personal best location update\n",
    "        p_best_loc[:, (obj <= p_best_obj)] = X[:, (obj <= p_best_obj)]\n",
    "        # personal best value update\n",
    "        p_best_obj = np.array([p_best_obj, obj]).min(axis=0)\n",
    "        # global best location update\n",
    "        g_best_loc = p_best_loc[:,p_best_obj.argmin()]\n",
    "        # global best value update\n",
    "        g_best_obj = p_best_obj.min()\n",
    "    \n",
    "        seed += 1\n",
    "    \n",
    "        # print('hi, at the interation of {:}, PSO finds {:.4f} as MSE'.format(i+1, g_best_obj))\n",
    "    \n",
    "    t1 = n1 * num_input\n",
    "    t2 = t1 + n2 * n1\n",
    "    t3 = t2 + n3 * n2\n",
    "    t4 = t3 + n3\n",
    "    t5 = t4 + n1\n",
    "    t6 = t5 + n2\n",
    "    t7 = t6 + n3\n",
    "    t8 = t7 + 1\n",
    "\n",
    "        \n",
    "    W1_pso = g_best_loc[0  : t1].reshape(n1 , num_input)\n",
    "    W2_pso = g_best_loc[t1 : t2].reshape(n2 , n1)\n",
    "    W3_pso = g_best_loc[t2 : t3].reshape(n3 , n2)\n",
    "    W4_pso = g_best_loc[t3 : t4].reshape(1  , n3)\n",
    "    b1_pso = g_best_loc[t4 : t5].reshape(n1 ,  1)\n",
    "    b2_pso = g_best_loc[t5 : t6].reshape(n2 ,  1)\n",
    "    b3_pso = g_best_loc[t6 : t7].reshape(n3 ,  1)\n",
    "    b4_pso = g_best_loc[t7 : t8].reshape(1  ,  1)\n",
    "    \n",
    "    return W1_pso, W2_pso, W3_pso, W4_pso, b1_pso, b2_pso, b3_pso, b4_pso\n",
    "\n",
    "    # print('\\nthe optimal parameters have been kept within those suffixed as \"_pso\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5a1b85",
   "metadata": {},
   "source": [
    "# Jacobian Matrixes Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "4a58dd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jacobian for the weights of the output layer\n",
    "def J_W4 (x, y) :\n",
    "    # d c /s4 = - 2 * (y - s4) \n",
    "    J = - 2 * (y - s4) \n",
    "    # d s4/w4 = a3\n",
    "    J = J @ a3.T / x.shape[1]\n",
    "    return J\n",
    "\n",
    "# Jacobian for the bias of the output layer\n",
    "def J_b4 (x, y) :\n",
    "    # d c /s4 = - 2 * (y - s4) \n",
    "    J = - 2 * (y - s4)\n",
    "    # d s4/b4 = 1\n",
    "    J = np.sum(J, axis=1, keepdims=True) / x.shape[1]\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "4d8a3baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jacobian for the weights of the 3rd hidden layer\n",
    "def J_W3 (x, y) :\n",
    "    # d c /s4 = - 2 * (y - s4) \n",
    "    J = - 2 * (y - s4)\n",
    "    # d s4/a3 = W4\n",
    "    J = (J.T @ W4).T\n",
    "    # d a3/s3 = s'(s3)\n",
    "    J = J * deriv_sigmoid(s3)\n",
    "    # d s3/W3 = a2\n",
    "    J = J @ a2.T / x.shape[1]\n",
    "    return J\n",
    "\n",
    "# Jacobian for the bias of the 3rd hidden layer\n",
    "def J_b3 (x, y) :\n",
    "    # d c /s4 = - 2 * (y - s4) \n",
    "    J = - 2 * (y - s4)\n",
    "    # d s4/a3 = W4\n",
    "    J = (J.T @ W4).T\n",
    "    # d a3/s3 = s'(s3)\n",
    "    J = J * deriv_sigmoid(s3)   \n",
    "    # d s3/b3 = 1\n",
    "    J = np.sum(J, axis=1, keepdims=True) / x.shape[1]\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "14aa2f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jacobian for the weights of the 2nd hidden layer\n",
    "def J_W2 (x, y) :\n",
    "    # d c /s4 = - 2 * (y - s4) \n",
    "    J = - 2 * (y - s4)\n",
    "    # d s4/a3 = W4\n",
    "    J = (J.T @ W4).T\n",
    "    # d a3/s3 = s'(s3)\n",
    "    J = J * deriv_sigmoid(s3)\n",
    "    # d s3/a2 = W3\n",
    "    J = (J.T @ W3).T    \n",
    "    # d a2/s2 = s'(s2)\n",
    "    J = J * deriv_sigmoid(s2)  \n",
    "    # d s2/W2 = a1\n",
    "    J = J @ a1.T / x.shape[1]\n",
    "    return J\n",
    "\n",
    "# Jacobian for the biases of the 2nd hidden layer\n",
    "def J_b2 (x, y) :\n",
    "    # d c /s4 = - 2 * (y - s4) \n",
    "    J = - 2 * (y - s4)\n",
    "    # d s4/a3 = W4\n",
    "    J = (J.T @ W4).T\n",
    "    # d a3/s3 = s'(s3)\n",
    "    J = J * deriv_sigmoid(s3)\n",
    "    # d s3/a2 = W3\n",
    "    J = (J.T @ W3).T    \n",
    "    # d a2/s2 = s'(s2)\n",
    "    J = J * deriv_sigmoid(s2)  \n",
    "    # d s2/b2 = 1\n",
    "    J = np.sum(J, axis=1, keepdims=True) / x.shape[1]\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "161380ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jacobian for the weights of the 1st hidden layer\n",
    "def J_W1 (x, y) :\n",
    "    # d c /s4 = - 2 * (y - s4) \n",
    "    J = - 2 * (y - s4)\n",
    "    # d s4/a3 = W4\n",
    "    J = (J.T @ W4).T\n",
    "    # d a3/s3 = s'(s3)\n",
    "    J = J * deriv_sigmoid(s3)\n",
    "    # d s3/a2 = W3\n",
    "    J = (J.T @ W3).T    \n",
    "    # d a2/s2 = s'(s2)\n",
    "    J = J * deriv_sigmoid(s2)  \n",
    "    # d s2/a1 = W2\n",
    "    J = (J.T @ W2).T   \n",
    "    # d a1/s1 = s'(s1)\n",
    "    J = J * deriv_sigmoid(s1)   \n",
    "    # d s1/W1 = a0\n",
    "    J = J @ a0.T / x.shape[1]\n",
    "    return J    \n",
    "\n",
    "# Jacobian for the biases of the 1st hidden layer\n",
    "def J_b1 (x, y) :\n",
    "    # d c /s4 = - 2 * (y - s4) \n",
    "    J = - 2 * (y - s4)\n",
    "    # d s4/a3 = W4\n",
    "    J = (J.T @ W4).T\n",
    "    # d a3/s3 = s'(s3)\n",
    "    J = J * deriv_sigmoid(s3)\n",
    "    # d s3/a2 = W3\n",
    "    J = (J.T @ W3).T    \n",
    "    # d a2/s2 = s'(s2)\n",
    "    J = J * deriv_sigmoid(s2)  \n",
    "    # d s2/a1 = W2\n",
    "    J = (J.T @ W2).T   \n",
    "    # d a1/s1 = s'(s1)\n",
    "    J = J * deriv_sigmoid(s1)   \n",
    "    # d s1/b1 = 1\n",
    "    J = np.sum(J, axis=1, keepdims=True) / x.shape[1]\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c49048",
   "metadata": {},
   "source": [
    "# Mini-Batch Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "bd21ed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function that randomly sorts the data batches (size > 1)\n",
    "# note: the data strcuture of X and y should be DataFrame\n",
    "def batches(X, y, size, seed):\n",
    "    \"\"\" \n",
    "    Args:    \n",
    "    X(pd.DataFrame): dataset (* which should NOT be transposed)\n",
    "    y(pd.DataFrame): true values (* which should NOT be transposed)\n",
    "    \n",
    "    Returns:    \n",
    "    batches_full(list): data batches\n",
    "    \"\"\"\n",
    "    # 1.fix this random seed at this moment, and this will be changed for each iteration\n",
    "    np.random.seed(seed)\n",
    "    # 2.number of total instances\n",
    "    num_instances = X.shape[0]\n",
    "    # 3.list initialized\n",
    "    batches_full  = []\n",
    "    \n",
    "    # 4.shuffle X and y \n",
    "    shuf_order = list(np.random.permutation(num_instances))\n",
    "    shuf_X     = X.iloc[shuf_order,:]\n",
    "    shuf_y     = y.iloc[shuf_order,:]\n",
    "    \n",
    "    # 5.get the maximum number of the batches could be reached\n",
    "    num_batch  = math.floor(num_instances / size) \n",
    "    \n",
    "    # 6.get the new sorted batches\n",
    "    for k in range(num_batch):\n",
    "        batches_X = shuf_X.iloc[k*size : (k+1)*size-1 ,:]\n",
    "        batches_y = shuf_y.iloc[k*size : (k+1)*size-1 ,:]\n",
    "        batches   = (batches_X, batches_y)\n",
    "        batches_full.append(batches)\n",
    "        \n",
    "    # 7.get the last remaining batch\n",
    "    if num_instances % size != 0:\n",
    "        batches_X = shuf_X.iloc[size * num_batch:,:]\n",
    "        batches_y = shuf_y.iloc[size * num_batch:,:]\n",
    "        batches   = (batches_X, batches_y)\n",
    "        batches_full.append(batches)\n",
    "        \n",
    "    return batches_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc50db66",
   "metadata": {},
   "source": [
    "# FFNN: Back-Propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "cb8d2564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimze FFNN using MBSGD\n",
    "\n",
    "# test\n",
    "array_x = array_list_x[0]\n",
    "array_y = array_list_y[0]/1000\n",
    "\n",
    "# 1.MBSGD parameters setting\n",
    "learn_rate      = 0.1\n",
    "total_iteration = 100\n",
    "batch_size = 512\n",
    "seed_bp   = 0\n",
    "\n",
    "# num_input = 8\n",
    "# parameters_reset (n1, n2, n3)\n",
    "\n",
    "# 2.initialize parameters using _pso\n",
    "W1, W2, W3, W4, b1, b2, b3, b4 = pso(array_x, array_y)\n",
    "\n",
    "# 3.read ANN parameters from PSO and feedforeward\n",
    "a0, s1, a1, s2, a2, s3, a3, s4 = forward_prop(array_x, W1, W2, W3, W4, b1, b2, b3, b4)\n",
    "\n",
    "# 4.keep the initial loss\n",
    "loss_bp = [MSE(array_y, s4)]\n",
    "\n",
    "# 5.reset dataset for MINIBATCH algorithm\n",
    "df_X  = pd.DataFrame(array_x.T)\n",
    "df_y  = pd.DataFrame(array_y.T)\n",
    "\n",
    "# 6.backpropogation using MBSGD\n",
    "for i in range (total_iteration): \n",
    "    # split data into batches\n",
    "    for Batch in batches (df_X, df_y, size = batch_size, seed = seed_bp):\n",
    "        \n",
    "        # get data batches\n",
    "        (Batch_X, Batch_y) = Batch\n",
    "        # transform datches into arrays\n",
    "        Batch_X = np.array(Batch_X).T\n",
    "        Batch_y = np.array(Batch_y).T\n",
    "        \n",
    "        # parameters update\n",
    "        a0, s1, a1, s2, a2, s3, a3, s4 = forward_prop(Batch_X, W1, W2, W3, W4, b1, b2, b3, b4)\n",
    "        W1 -= learn_rate * J_W1 (a0, Batch_y)\n",
    "        W2 -= learn_rate * J_W2 (a0, Batch_y)\n",
    "        W3 -= learn_rate * J_W3 (a0, Batch_y)\n",
    "        W4 -= learn_rate * J_W4 (a0, Batch_y)\n",
    "        b1 -= learn_rate * J_b1 (a0, Batch_y)\n",
    "        b2 -= learn_rate * J_b2 (a0, Batch_y)\n",
    "        b3 -= learn_rate * J_b3 (a0, Batch_y)\n",
    "        b4 -= learn_rate * J_b4 (a0, Batch_y)\n",
    "        \n",
    "    # at the end of each iteration, we keep the loss for the all the instances\n",
    "    a0, s1, a1, s2, a2, s3, a3, s4 = forward_prop(array_x, W1, W2, W3, W4, b1, b2, b3, b4)\n",
    "    loss = MSE(array_y, s4)\n",
    "    loss_bp.append(loss)\n",
    "    \n",
    "    seed_bp += 1\n",
    "    #print('at the interation of {:}, MBSGD finds {:.4f} as MSE'.format(i+1, Loss))\n",
    "\n",
    "# 6.plot the diagram of iteration and lossfuntion\n",
    "# plt.figure(figsize = (7,4))\n",
    "# plt.title(\"Loss Diminishing of Back_Propagation Using MBSGD\")\n",
    "# plt.xlabel('Number of Iteration')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.plot(range(101), loss_bp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2b1641",
   "metadata": {},
   "source": [
    "# Augmented Dickyâ€“Fuller Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7282d3c4",
   "metadata": {},
   "source": [
    "# Auto-Regressive Distributed Lag Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b4c0f8",
   "metadata": {},
   "source": [
    "# Visualizations of Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65c431e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d4b535e",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e9acee",
   "metadata": {},
   "source": [
    "## data extraction at the first semester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c55cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_datareader.data as pdr\n",
    "\n",
    "import datetime   \n",
    "import fix_yahoo_finance as yf   \n",
    "  \n",
    "start = datetime.datetime(2009,  1,  1)   \n",
    "end   = datetime.datetime(2021,  9,  1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a1aeb5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_dict = {'US':'^GSPC',        'Canada':'^GSPTSE',  'Japan':'^N225',     'Hong Kong':'^HSI',\n",
    "               'Spain':'^OMX',      'France':'^FCHI',    'Netherlands':'^AEX','Spain':'^IBEX',\n",
    "               # advanced\n",
    "               'Brazil':'^BVSP',    'Mexico':'^MXX',     'China':'000001.SS', 'Turkey':'XU100.IS',\n",
    "               'Indonesia':'^JKII', 'India':'^BSESN'}\n",
    "               # emerging\n",
    "\n",
    "data_list   = [i for i in range(14)]\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e5bc84b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US ^GSPC 2008-12-31 2021-09-01\n",
      "Canada ^GSPTSE 2008-12-31 2021-09-01\n",
      "Japan ^N225 2009-01-05 2021-09-01\n",
      "Hong Kong ^HSI 2009-01-02 2021-09-01\n",
      "Spain ^IBEX 2009-01-02 2021-09-01\n",
      "France ^FCHI 2008-12-31 2021-09-01\n",
      "Netherlands ^AEX 2008-12-31 2021-09-01\n",
      "Brazil ^BVSP 2009-01-02 2021-09-01\n",
      "Mexico ^MXX 2008-12-31 2021-09-01\n",
      "China 000001.SS 2009-01-05 2021-09-01\n",
      "Turkey XU100.IS 2008-12-31 2021-09-01\n",
      "Indonesia ^JKII 2009-01-05 2021-09-01\n",
      "India ^BSESN 2009-01-02 2021-09-01\n"
     ]
    }
   ],
   "source": [
    "for con, sym in symbol_dict.items():\n",
    "    data_list[i] = pd.DataFrame(pdr.get_data_yahoo(sym,start,end))\n",
    "    i = i + 1\n",
    "    #print(\"country: {}, symbol: {}, first record: {}, last record: {}\".format(\n",
    "    #con, sym, con.index[0].date(), con.index[-1].date()))\n",
    "    print(con, sym, pdr.get_data_yahoo(sym,start,end).index[0].date(), pdr.get_data_yahoo(sym,start,end).index[-1].date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "549ea2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_US          = data_list[0]\n",
    "df_Canada      = data_list[1]\n",
    "df_Japan       = data_list[2]\n",
    "df_Hong_Kong   = data_list[3]\n",
    "df_Spain       = data_list[4]\n",
    "df_France      = data_list[5]\n",
    "df_Netherlands = data_list[6]\n",
    "df_Brazil      = data_list[7]\n",
    "df_Mexico      = data_list[8]\n",
    "df_China       = data_list[9]\n",
    "df_Turkey      = data_list[10]\n",
    "df_Indonesia   = data_list[11]\n",
    "df_India       = data_list[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c8cea17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_US.to_csv('./US.csv')\n",
    "df_Canada.to_csv('./Canada.csv')\n",
    "df_Japan.to_csv('./Japan.csv')\n",
    "df_Hong_Kong.to_csv('./Hong_Kong.csv')\n",
    "df_Spain.to_csv('./Spain.csv')\n",
    "df_France.to_csv('./France.csv')\n",
    "df_Netherlands.to_csv('./Netherlands.csv')\n",
    "df_Brazil.to_csv('./Brazil.csv')\n",
    "df_Mexico.to_csv('./Mexico.csv')\n",
    "df_China.to_csv('./China.csv')\n",
    "df_Turkey.to_csv('./Turkey.csv')\n",
    "df_Indonesia.to_csv('./Indonesia.csv')\n",
    "df_India.to_csv('./India.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d7c7cb",
   "metadata": {},
   "source": [
    "## generate 20 technical indicators (dirty work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d6ae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "k = 15\n",
    "\n",
    "# 1.SMAt(n)\n",
    "for region in region_list: \n",
    "    df_full.loc[df_full.Region == region,'SMA'] = \\\n",
    "    df_full[df_full.Region == region]['Close'].rolling(window = n).mean()\n",
    "\n",
    "# 2.EMAt(n)\n",
    "alpha_1 = 2/(n+1)\n",
    "for region in region_list: \n",
    "    df_full.loc[df_full.Region == region,'EMA_10'] = \\\n",
    "    df_full.loc[df_full.Region == region,'Close'].ewm(alpha=alpha_1,adjust=False).mean()\n",
    "\n",
    "alpha_2 = 2/(k+1)\n",
    "for region in region_list: \n",
    "    df_full.loc[df_full.Region == region,'EMA_15'] = \\\n",
    "    df_full.loc[df_full.Region == region,'Close'].ewm(alpha=alpha_2,adjust=False).mean()\n",
    "    \n",
    "# 3.MACDt(n,k)\n",
    "for region in region_list: \n",
    "    df_full.loc[df_full.Region == region,'MACD'] = \\\n",
    "    df_full.loc[df_full.Region == region,'EMA_10'] - \\\n",
    "    df_full.loc[df_full.Region == region,'EMA_15']\n",
    "\n",
    "# 4.ATRt(n)\n",
    "# TR\n",
    "for region in region_list: \n",
    "    df_full.loc[df_full.Region == region, 'TR'] = \\\n",
    "    pd.concat([(df_full.loc[df_full.Region == region]['High'] - df_full.loc[df_full.Region == region]['Low']),\n",
    "                abs(df_full.loc[df_full.Region == region]['High'] - df_full.loc[df_full.Region == region]['Close'].shift(+1)),\n",
    "                abs(df_full.loc[df_full.Region == region]['Low']  - df_full.loc[df_full.Region == region]['Close'].shift(+1))], \\\n",
    "              axis = 1).max(axis=1)\n",
    "    \n",
    "df_full.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "286px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
