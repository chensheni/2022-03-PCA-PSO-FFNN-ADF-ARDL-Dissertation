{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d306364",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-loading\" data-toc-modified-id=\"Data-loading-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data loading</a></span></li><li><span><a href=\"#Data-pre-processing\" data-toc-modified-id=\"Data-pre-processing-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data pre-processing</a></span><ul class=\"toc-item\"><li><span><a href=\"#technical-indicators-generation\" data-toc-modified-id=\"technical-indicators-generation-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>technical indicators generation</a></span></li><li><span><a href=\"#normalization\" data-toc-modified-id=\"normalization-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>normalization</a></span></li><li><span><a href=\"#principal-components-analysis\" data-toc-modified-id=\"principal-components-analysis-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>principal components analysis</a></span></li></ul></li><li><span><a href=\"#FFNN:-feed-foreward\" data-toc-modified-id=\"FFNN:-feed-foreward-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>FFNN: feed-foreward</a></span></li><li><span><a href=\"#FFNN:-Jacobian-matrixes-set-up\" data-toc-modified-id=\"FFNN:-Jacobian-matrixes-set-up-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>FFNN: Jacobian matrixes set-up</a></span></li><li><span><a href=\"#FFNN:-particle-swarm-optimization\" data-toc-modified-id=\"FFNN:-particle-swarm-optimization-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>FFNN: particle swarm optimization</a></span></li><li><span><a href=\"#FFNN:-mini-batch-stochastic-GD\" data-toc-modified-id=\"FFNN:-mini-batch-stochastic-GD-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>FFNN: mini-batch stochastic GD</a></span></li><li><span><a href=\"#ARDL:-augmented-Dicky–Fuller-test\" data-toc-modified-id=\"ARDL:-augmented-Dicky–Fuller-test-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>ARDL: augmented Dicky–Fuller test</a></span></li><li><span><a href=\"#Train,-validation,-test-split\" data-toc-modified-id=\"Train,-validation,-test-split-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Train, validation, test split</a></span><ul class=\"toc-item\"><li><span><a href=\"#for-ARDL-model\" data-toc-modified-id=\"for-ARDL-model-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>for ARDL model</a></span></li><li><span><a href=\"#for-FFNN-model\" data-toc-modified-id=\"for-FFNN-model-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>for FFNN model</a></span></li><li><span><a href=\"#clarifications-on-datasets\" data-toc-modified-id=\"clarifications-on-datasets-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>clarifications on datasets</a></span></li></ul></li><li><span><a href=\"#FFNN:-back-propogation\" data-toc-modified-id=\"FFNN:-back-propogation-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>FFNN: back-propogation</a></span></li><li><span><a href=\"#FFNN:-hyperparameter-tunning\" data-toc-modified-id=\"FFNN:-hyperparameter-tunning-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>FFNN: hyperparameter tunning</a></span></li><li><span><a href=\"#FFNN:-model-performance-on-test-set\" data-toc-modified-id=\"FFNN:-model-performance-on-test-set-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>FFNN: model performance on test set</a></span></li><li><span><a href=\"#ARDL:-hyperparameter-tunning\" data-toc-modified-id=\"ARDL:-hyperparameter-tunning-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>ARDL: hyperparameter tunning</a></span></li><li><span><a href=\"#ARDL:-model-performance-on-test-set\" data-toc-modified-id=\"ARDL:-model-performance-on-test-set-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>ARDL: model performance on test set</a></span></li><li><span><a href=\"#Summary-of-model-performance\" data-toc-modified-id=\"Summary-of-model-performance-14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;</span>Summary of model performance</a></span></li><li><span><a href=\"#Appendix\" data-toc-modified-id=\"Appendix-15\"><span class=\"toc-item-num\">15&nbsp;&nbsp;</span>Appendix</a></span><ul class=\"toc-item\"><li><span><a href=\"#data-extraction-at-the-first-semester\" data-toc-modified-id=\"data-extraction-at-the-first-semester-15.1\"><span class=\"toc-item-num\">15.1&nbsp;&nbsp;</span>data extraction at the first semester</a></span></li><li><span><a href=\"#generate-20-technical-indicators-(dirty-work)\" data-toc-modified-id=\"generate-20-technical-indicators-(dirty-work)-15.2\"><span class=\"toc-item-num\">15.2&nbsp;&nbsp;</span>generate 20 technical indicators (dirty work)</a></span></li><li><span><a href=\"#code-of-MBSGD-with-plotting\" data-toc-modified-id=\"code-of-MBSGD-with-plotting-15.3\"><span class=\"toc-item-num\">15.3&nbsp;&nbsp;</span>code of MBSGD with plotting</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6931b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import datetime \n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# https://www.5axxw.com/wiki/content/aeduov\n",
    "import pandas_ta as ta\n",
    "\n",
    "import fix_yahoo_finance as yf\n",
    "import pandas_datareader.data as pdr\n",
    "\n",
    "# https://www.statsmodels.org/stable/tsa.html#tsa-tools\n",
    "from statsmodels.tsa.api import ARDL\n",
    "from statsmodels.tsa.ardl import ardl_select_order\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "pd.set_option('display.max_columns',None) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1924d90",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3bb1940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from previously downloaded files\n",
    "\n",
    "# 7 advanced economies\n",
    "df_US          = pd.read_csv('./US.csv')\n",
    "df_Canada      = pd.read_csv('./Canada.csv')\n",
    "df_Japan       = pd.read_csv('./Japan.csv')\n",
    "df_Hong_Kong   = pd.read_csv('./Hong_Kong.csv')\n",
    "df_Spain       = pd.read_csv('./Spain.csv')\n",
    "df_France      = pd.read_csv('./France.csv')\n",
    "df_Netherlands = pd.read_csv('./Netherlands.csv')\n",
    "\n",
    "# 6 emerging economies\n",
    "df_Brazil      = pd.read_csv('./Brazil.csv')\n",
    "df_Mexico      = pd.read_csv('./Mexico.csv')\n",
    "df_China       = pd.read_csv('./China.csv')\n",
    "df_Turkey      = pd.read_csv('./Turkey.csv')\n",
    "df_Indonesia   = pd.read_csv('./Indonesia.csv')\n",
    "df_India       = pd.read_csv('./India.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edbfbfdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>return_next_day</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>1133.869995</td>\n",
       "      <td>1116.560059</td>\n",
       "      <td>1116.560059</td>\n",
       "      <td>1132.989990</td>\n",
       "      <td>3991400000</td>\n",
       "      <td>1136.520020</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>1136.630005</td>\n",
       "      <td>1129.660034</td>\n",
       "      <td>1132.660034</td>\n",
       "      <td>1136.520020</td>\n",
       "      <td>2491020000</td>\n",
       "      <td>1137.140015</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>1139.189941</td>\n",
       "      <td>1133.949951</td>\n",
       "      <td>1135.709961</td>\n",
       "      <td>1137.140015</td>\n",
       "      <td>4972660000</td>\n",
       "      <td>1141.689941</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>1142.459961</td>\n",
       "      <td>1131.319946</td>\n",
       "      <td>1136.270020</td>\n",
       "      <td>1141.689941</td>\n",
       "      <td>5270680000</td>\n",
       "      <td>1144.979980</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-08</td>\n",
       "      <td>1145.390015</td>\n",
       "      <td>1136.219971</td>\n",
       "      <td>1140.520020</td>\n",
       "      <td>1144.979980</td>\n",
       "      <td>4389590000</td>\n",
       "      <td>1146.979980</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2764</th>\n",
       "      <td>2020-12-24</td>\n",
       "      <td>3703.820068</td>\n",
       "      <td>3689.320068</td>\n",
       "      <td>3694.030029</td>\n",
       "      <td>3703.060059</td>\n",
       "      <td>1885090000</td>\n",
       "      <td>3735.360107</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2765</th>\n",
       "      <td>2020-12-28</td>\n",
       "      <td>3740.510010</td>\n",
       "      <td>3723.030029</td>\n",
       "      <td>3723.030029</td>\n",
       "      <td>3735.360107</td>\n",
       "      <td>3527460000</td>\n",
       "      <td>3727.040039</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2766</th>\n",
       "      <td>2020-12-29</td>\n",
       "      <td>3756.120117</td>\n",
       "      <td>3723.310059</td>\n",
       "      <td>3750.010010</td>\n",
       "      <td>3727.040039</td>\n",
       "      <td>3387030000</td>\n",
       "      <td>3732.040039</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2767</th>\n",
       "      <td>2020-12-30</td>\n",
       "      <td>3744.629883</td>\n",
       "      <td>3730.209961</td>\n",
       "      <td>3736.189941</td>\n",
       "      <td>3732.040039</td>\n",
       "      <td>3145200000</td>\n",
       "      <td>3756.070068</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2768</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>3760.199951</td>\n",
       "      <td>3726.879883</td>\n",
       "      <td>3733.270020</td>\n",
       "      <td>3756.070068</td>\n",
       "      <td>3172510000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2769 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        datetime         high          low         open        close  \\\n",
       "0     2010-01-04  1133.869995  1116.560059  1116.560059  1132.989990   \n",
       "1     2010-01-05  1136.630005  1129.660034  1132.660034  1136.520020   \n",
       "2     2010-01-06  1139.189941  1133.949951  1135.709961  1137.140015   \n",
       "3     2010-01-07  1142.459961  1131.319946  1136.270020  1141.689941   \n",
       "4     2010-01-08  1145.390015  1136.219971  1140.520020  1144.979980   \n",
       "...          ...          ...          ...          ...          ...   \n",
       "2764  2020-12-24  3703.820068  3689.320068  3694.030029  3703.060059   \n",
       "2765  2020-12-28  3740.510010  3723.030029  3723.030029  3735.360107   \n",
       "2766  2020-12-29  3756.120117  3723.310059  3750.010010  3727.040039   \n",
       "2767  2020-12-30  3744.629883  3730.209961  3736.189941  3732.040039   \n",
       "2768  2020-12-31  3760.199951  3726.879883  3733.270020  3756.070068   \n",
       "\n",
       "          volume  return_next_day region  \n",
       "0     3991400000      1136.520020     US  \n",
       "1     2491020000      1137.140015     US  \n",
       "2     4972660000      1141.689941     US  \n",
       "3     5270680000      1144.979980     US  \n",
       "4     4389590000      1146.979980     US  \n",
       "...          ...              ...    ...  \n",
       "2764  1885090000      3735.360107     US  \n",
       "2765  3527460000      3727.040039     US  \n",
       "2766  3387030000      3732.040039     US  \n",
       "2767  3145200000      3756.070068     US  \n",
       "2768  3172510000              NaN     US  \n",
       "\n",
       "[2769 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create df_full containing data of all the economies\n",
    "df_list     = [df_US,df_Canada,df_Japan,df_Hong_Kong,df_Spain,df_France,df_Netherlands,\n",
    "               df_Brazil,df_Mexico,df_China,df_Turkey,df_Indonesia,df_India]\n",
    "region_list = ['US','Canada','Japan','Hong_Kong','Spain','France','Netherlands',\n",
    "               'Brazil','Mexico','China','Turkey','Indonesia','India']\n",
    "\n",
    "# add region name and change col names\n",
    "for df, region in zip(df_list, region_list):\n",
    "    df['region'] = region\n",
    "    df.rename(columns = {'Adj Close':'return_next_day','Date':'datetime','High':'high',\n",
    "                         'Low':'low','Open':'open','Close':'close','Volume':'volume'}, \n",
    "              inplace = True)\n",
    "    \n",
    "# select the data within the observation period 2010-2020\n",
    "for i, df in enumerate(df_list):\n",
    "    selection_logic = (df.datetime >= '2010-01-01') & (df.datetime <= '2020-12-31')\n",
    "    df_list[i] = df.loc[selection_logic,:].reset_index(drop=True)\n",
    "    # create col 'return_next_day'\n",
    "    df_list[i]['return_next_day'] = np.nan\n",
    "    df_list[i].loc[:,'return_next_day'] = df_list[i]['close'].shift(-1)\n",
    "    # df_list[i].loc[:,'return_next_day'] = df_list[i]['close'].shift(-1) - df_list[i]['close']\n",
    "    \n",
    "# df_full = pd.concat(df_list, ignore_index = True)\n",
    "df_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e950ed4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many instances in each dataset\n",
      "US:           2769 \n",
      "Canada:       2760 \n",
      "Japan:        2690 \n",
      "Hong_Kong:    2705 \n",
      "Spain:        2812 \n",
      "France:       2810 \n",
      "Netherlands:  2813 \n",
      "Brazil:       2716 \n",
      "Mexico:       2753 \n",
      "China:        2670 \n",
      "Turkey:       2762 \n",
      "Indonesia:    2680 \n",
      "India:        2701 \n"
     ]
    }
   ],
   "source": [
    "# count how many instances in each region\n",
    "print('How many instances in each dataset')\n",
    "for region, df in zip(region_list, df_list):\n",
    "    print('{:<12} {:>5d} '.format(region+':', df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08589297",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2796b1",
   "metadata": {},
   "source": [
    "## technical indicators generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d55a034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pandas_ta package as the origion paper indicates\n",
    "n = 10; k = 15\n",
    "\n",
    "# create the custom strategy for 20 technical indicators\n",
    "CustomStrategy = ta.Strategy(\n",
    "    name = 'could use help(ta.xxx) to check the definition & calculation',\n",
    "    ta=[\n",
    "        {\"kind\": \"sma\",  \"length\": n},\n",
    "        {\"kind\": \"ema\",  \"length\": n},\n",
    "        {\"kind\": \"macd\", \"fast\"  : n , \"slow\": k},\n",
    "        {\"kind\": \"adx\",  \"length\": n},\n",
    "        {\"kind\": \"cci\",  \"length\": n},\n",
    "        {\"kind\": \"mom\",  \"length\": n},\n",
    "        {\"kind\": \"roc\",  \"length\": n},\n",
    "        {\"kind\": \"rsi\",  \"length\": n},\n",
    "        {\"kind\": \"tsi\",  \"length\": n},\n",
    "        {\"kind\": \"kdj\",  \"length\": n}, # it includes k%, d%, j%\n",
    "        {\"kind\": \"atr\",  \"length\": n},\n",
    "        {\"kind\": \"ui\" ,  \"length\": n},\n",
    "        {\"kind\": \"ad\" ,  \"length\": n},\n",
    "        {\"kind\": \"obv\",  \"length\": n}]\n",
    ")\n",
    "\n",
    "# generate technical indicators\n",
    "for i, df in enumerate(df_list):\n",
    "    df_list[i].ta.strategy(CustomStrategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2da4f60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>datetime</th>\n",
       "      <th>return_next_day</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>SMA_10</th>\n",
       "      <th>EMA_10</th>\n",
       "      <th>MACD_10_15_9</th>\n",
       "      <th>ADX_10</th>\n",
       "      <th>CCI_10_0.015</th>\n",
       "      <th>MOM_10</th>\n",
       "      <th>ROC_10</th>\n",
       "      <th>RSI_10</th>\n",
       "      <th>TSI_13_25_13</th>\n",
       "      <th>K_10_3</th>\n",
       "      <th>D_10_3</th>\n",
       "      <th>J_10_3</th>\n",
       "      <th>ATRr_10</th>\n",
       "      <th>UI_10</th>\n",
       "      <th>AD</th>\n",
       "      <th>OBV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>2010-02-08</td>\n",
       "      <td>1070.520020</td>\n",
       "      <td>1071.199951</td>\n",
       "      <td>1056.510010</td>\n",
       "      <td>1065.510010</td>\n",
       "      <td>1056.739990</td>\n",
       "      <td>1082.389990</td>\n",
       "      <td>1082.543306</td>\n",
       "      <td>-10.285084</td>\n",
       "      <td>57.587779</td>\n",
       "      <td>-126.654054</td>\n",
       "      <td>-40.040039</td>\n",
       "      <td>-3.650690</td>\n",
       "      <td>29.557233</td>\n",
       "      <td>-31.395445</td>\n",
       "      <td>23.118601</td>\n",
       "      <td>22.250833</td>\n",
       "      <td>24.854138</td>\n",
       "      <td>18.309764</td>\n",
       "      <td>4.539277</td>\n",
       "      <td>7.000802e+09</td>\n",
       "      <td>6.047570e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>2010-02-09</td>\n",
       "      <td>1068.130005</td>\n",
       "      <td>1079.280029</td>\n",
       "      <td>1060.060059</td>\n",
       "      <td>1060.060059</td>\n",
       "      <td>1070.520020</td>\n",
       "      <td>1080.224988</td>\n",
       "      <td>1080.357254</td>\n",
       "      <td>-9.682590</td>\n",
       "      <td>55.953294</td>\n",
       "      <td>-62.030614</td>\n",
       "      <td>-21.650024</td>\n",
       "      <td>-1.982294</td>\n",
       "      <td>38.281123</td>\n",
       "      <td>-29.437032</td>\n",
       "      <td>29.819572</td>\n",
       "      <td>24.779520</td>\n",
       "      <td>39.899674</td>\n",
       "      <td>18.765509</td>\n",
       "      <td>4.352115</td>\n",
       "      <td>7.453144e+09</td>\n",
       "      <td>1.116183e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>2010-02-10</td>\n",
       "      <td>1078.469971</td>\n",
       "      <td>1073.670044</td>\n",
       "      <td>1059.339966</td>\n",
       "      <td>1069.680054</td>\n",
       "      <td>1068.130005</td>\n",
       "      <td>1077.287988</td>\n",
       "      <td>1078.134118</td>\n",
       "      <td>-9.166996</td>\n",
       "      <td>54.612874</td>\n",
       "      <td>-63.139191</td>\n",
       "      <td>-29.369995</td>\n",
       "      <td>-2.676082</td>\n",
       "      <td>37.388797</td>\n",
       "      <td>-28.068852</td>\n",
       "      <td>32.959493</td>\n",
       "      <td>27.510335</td>\n",
       "      <td>43.857808</td>\n",
       "      <td>18.291329</td>\n",
       "      <td>4.225706</td>\n",
       "      <td>8.417354e+09</td>\n",
       "      <td>6.910380e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>2010-02-11</td>\n",
       "      <td>1075.510010</td>\n",
       "      <td>1080.040039</td>\n",
       "      <td>1060.589966</td>\n",
       "      <td>1067.099976</td>\n",
       "      <td>1078.469971</td>\n",
       "      <td>1076.681982</td>\n",
       "      <td>1078.195182</td>\n",
       "      <td>-8.002039</td>\n",
       "      <td>52.043317</td>\n",
       "      <td>-22.071395</td>\n",
       "      <td>-6.060059</td>\n",
       "      <td>-0.558773</td>\n",
       "      <td>43.697528</td>\n",
       "      <td>-25.530705</td>\n",
       "      <td>40.776666</td>\n",
       "      <td>31.936939</td>\n",
       "      <td>58.456122</td>\n",
       "      <td>18.414358</td>\n",
       "      <td>3.886038</td>\n",
       "      <td>1.210772e+10</td>\n",
       "      <td>1.131125e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>1094.869995</td>\n",
       "      <td>1077.810059</td>\n",
       "      <td>1062.969971</td>\n",
       "      <td>1075.949951</td>\n",
       "      <td>1075.510010</td>\n",
       "      <td>1076.845984</td>\n",
       "      <td>1077.706969</td>\n",
       "      <td>-7.154351</td>\n",
       "      <td>49.815438</td>\n",
       "      <td>-23.171415</td>\n",
       "      <td>1.640015</td>\n",
       "      <td>0.152720</td>\n",
       "      <td>42.340556</td>\n",
       "      <td>-23.735646</td>\n",
       "      <td>44.347519</td>\n",
       "      <td>36.076600</td>\n",
       "      <td>60.889358</td>\n",
       "      <td>18.106827</td>\n",
       "      <td>3.365921</td>\n",
       "      <td>1.497868e+10</td>\n",
       "      <td>7.150570e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  region    datetime  return_next_day         high          low         open  \\\n",
       "0     US  2010-02-08      1070.520020  1071.199951  1056.510010  1065.510010   \n",
       "1     US  2010-02-09      1068.130005  1079.280029  1060.060059  1060.060059   \n",
       "2     US  2010-02-10      1078.469971  1073.670044  1059.339966  1069.680054   \n",
       "3     US  2010-02-11      1075.510010  1080.040039  1060.589966  1067.099976   \n",
       "4     US  2010-02-12      1094.869995  1077.810059  1062.969971  1075.949951   \n",
       "\n",
       "         close       SMA_10       EMA_10  MACD_10_15_9     ADX_10  \\\n",
       "0  1056.739990  1082.389990  1082.543306    -10.285084  57.587779   \n",
       "1  1070.520020  1080.224988  1080.357254     -9.682590  55.953294   \n",
       "2  1068.130005  1077.287988  1078.134118     -9.166996  54.612874   \n",
       "3  1078.469971  1076.681982  1078.195182     -8.002039  52.043317   \n",
       "4  1075.510010  1076.845984  1077.706969     -7.154351  49.815438   \n",
       "\n",
       "   CCI_10_0.015     MOM_10    ROC_10     RSI_10  TSI_13_25_13     K_10_3  \\\n",
       "0   -126.654054 -40.040039 -3.650690  29.557233    -31.395445  23.118601   \n",
       "1    -62.030614 -21.650024 -1.982294  38.281123    -29.437032  29.819572   \n",
       "2    -63.139191 -29.369995 -2.676082  37.388797    -28.068852  32.959493   \n",
       "3    -22.071395  -6.060059 -0.558773  43.697528    -25.530705  40.776666   \n",
       "4    -23.171415   1.640015  0.152720  42.340556    -23.735646  44.347519   \n",
       "\n",
       "      D_10_3     J_10_3    ATRr_10     UI_10            AD           OBV  \n",
       "0  22.250833  24.854138  18.309764  4.539277  7.000802e+09  6.047570e+09  \n",
       "1  24.779520  39.899674  18.765509  4.352115  7.453144e+09  1.116183e+10  \n",
       "2  27.510335  43.857808  18.291329  4.225706  8.417354e+09  6.910380e+09  \n",
       "3  31.936939  58.456122  18.414358  3.886038  1.210772e+10  1.131125e+10  \n",
       "4  36.076600  60.889358  18.106827  3.365921  1.497868e+10  7.150570e+09  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean dataframes\n",
    "for i, df in enumerate(df_list):\n",
    "    cols = ['region','datetime','return_next_day','high','low','open','close','SMA_10',\n",
    "            'EMA_10','MACD_10_15_9','ADX_10','CCI_10_0.015','MOM_10','ROC_10','RSI_10',\n",
    "            'TSI_13_25_13','K_10_3','D_10_3','J_10_3','ATRr_10','UI_10','AD','OBV']\n",
    "    # delete rows b/c loosing obeservations\n",
    "    df_list[i] = df_list[i].loc[24:df_list[i].shape[0]-2,cols].reset_index(drop=True)\n",
    "\n",
    "df_list[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8855f56d",
   "metadata": {},
   "source": [
    "## normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9973512",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>datetime</th>\n",
       "      <th>return_next_day</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>SMA_10</th>\n",
       "      <th>EMA_10</th>\n",
       "      <th>MACD_10_15_9</th>\n",
       "      <th>ADX_10</th>\n",
       "      <th>CCI_10_0.015</th>\n",
       "      <th>MOM_10</th>\n",
       "      <th>ROC_10</th>\n",
       "      <th>RSI_10</th>\n",
       "      <th>TSI_13_25_13</th>\n",
       "      <th>K_10_3</th>\n",
       "      <th>D_10_3</th>\n",
       "      <th>J_10_3</th>\n",
       "      <th>ATRr_10</th>\n",
       "      <th>UI_10</th>\n",
       "      <th>AD</th>\n",
       "      <th>OBV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>2010-02-08</td>\n",
       "      <td>1070.520020</td>\n",
       "      <td>0.014046</td>\n",
       "      <td>0.016769</td>\n",
       "      <td>0.013907</td>\n",
       "      <td>0.012592</td>\n",
       "      <td>0.011971</td>\n",
       "      <td>0.011981</td>\n",
       "      <td>0.663575</td>\n",
       "      <td>0.852338</td>\n",
       "      <td>0.299200</td>\n",
       "      <td>0.597410</td>\n",
       "      <td>0.465038</td>\n",
       "      <td>0.234001</td>\n",
       "      <td>0.070086</td>\n",
       "      <td>0.198053</td>\n",
       "      <td>0.140825</td>\n",
       "      <td>0.341286</td>\n",
       "      <td>0.065068</td>\n",
       "      <td>0.242227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>2010-02-09</td>\n",
       "      <td>1068.130005</td>\n",
       "      <td>0.017013</td>\n",
       "      <td>0.018075</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.017672</td>\n",
       "      <td>0.011157</td>\n",
       "      <td>0.011159</td>\n",
       "      <td>0.667457</td>\n",
       "      <td>0.823227</td>\n",
       "      <td>0.412781</td>\n",
       "      <td>0.613287</td>\n",
       "      <td>0.504351</td>\n",
       "      <td>0.346376</td>\n",
       "      <td>0.089145</td>\n",
       "      <td>0.269842</td>\n",
       "      <td>0.170089</td>\n",
       "      <td>0.443280</td>\n",
       "      <td>0.067823</td>\n",
       "      <td>0.232220</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.005466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>2010-02-10</td>\n",
       "      <td>1078.469971</td>\n",
       "      <td>0.014953</td>\n",
       "      <td>0.017810</td>\n",
       "      <td>0.015439</td>\n",
       "      <td>0.016791</td>\n",
       "      <td>0.010053</td>\n",
       "      <td>0.010322</td>\n",
       "      <td>0.670780</td>\n",
       "      <td>0.799354</td>\n",
       "      <td>0.410833</td>\n",
       "      <td>0.606622</td>\n",
       "      <td>0.488003</td>\n",
       "      <td>0.334881</td>\n",
       "      <td>0.102459</td>\n",
       "      <td>0.303481</td>\n",
       "      <td>0.201692</td>\n",
       "      <td>0.470113</td>\n",
       "      <td>0.064957</td>\n",
       "      <td>0.225461</td>\n",
       "      <td>0.001039</td>\n",
       "      <td>0.000922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>2010-02-11</td>\n",
       "      <td>1075.510010</td>\n",
       "      <td>0.017292</td>\n",
       "      <td>0.018269</td>\n",
       "      <td>0.014491</td>\n",
       "      <td>0.020602</td>\n",
       "      <td>0.009825</td>\n",
       "      <td>0.010345</td>\n",
       "      <td>0.678287</td>\n",
       "      <td>0.753590</td>\n",
       "      <td>0.483013</td>\n",
       "      <td>0.626746</td>\n",
       "      <td>0.537893</td>\n",
       "      <td>0.416145</td>\n",
       "      <td>0.127159</td>\n",
       "      <td>0.387229</td>\n",
       "      <td>0.252919</td>\n",
       "      <td>0.569076</td>\n",
       "      <td>0.065700</td>\n",
       "      <td>0.207301</td>\n",
       "      <td>0.003744</td>\n",
       "      <td>0.005626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>2010-02-12</td>\n",
       "      <td>1094.869995</td>\n",
       "      <td>0.016473</td>\n",
       "      <td>0.019145</td>\n",
       "      <td>0.017742</td>\n",
       "      <td>0.019511</td>\n",
       "      <td>0.009887</td>\n",
       "      <td>0.010162</td>\n",
       "      <td>0.683750</td>\n",
       "      <td>0.713911</td>\n",
       "      <td>0.481080</td>\n",
       "      <td>0.633394</td>\n",
       "      <td>0.554659</td>\n",
       "      <td>0.398666</td>\n",
       "      <td>0.144628</td>\n",
       "      <td>0.425485</td>\n",
       "      <td>0.300826</td>\n",
       "      <td>0.585571</td>\n",
       "      <td>0.063842</td>\n",
       "      <td>0.179492</td>\n",
       "      <td>0.005849</td>\n",
       "      <td>0.001179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  region    datetime  return_next_day      high       low      open     close  \\\n",
       "0     US  2010-02-08      1070.520020  0.014046  0.016769  0.013907  0.012592   \n",
       "1     US  2010-02-09      1068.130005  0.017013  0.018075  0.011905  0.017672   \n",
       "2     US  2010-02-10      1078.469971  0.014953  0.017810  0.015439  0.016791   \n",
       "3     US  2010-02-11      1075.510010  0.017292  0.018269  0.014491  0.020602   \n",
       "4     US  2010-02-12      1094.869995  0.016473  0.019145  0.017742  0.019511   \n",
       "\n",
       "     SMA_10    EMA_10  MACD_10_15_9    ADX_10  CCI_10_0.015    MOM_10  \\\n",
       "0  0.011971  0.011981      0.663575  0.852338      0.299200  0.597410   \n",
       "1  0.011157  0.011159      0.667457  0.823227      0.412781  0.613287   \n",
       "2  0.010053  0.010322      0.670780  0.799354      0.410833  0.606622   \n",
       "3  0.009825  0.010345      0.678287  0.753590      0.483013  0.626746   \n",
       "4  0.009887  0.010162      0.683750  0.713911      0.481080  0.633394   \n",
       "\n",
       "     ROC_10    RSI_10  TSI_13_25_13    K_10_3    D_10_3    J_10_3   ATRr_10  \\\n",
       "0  0.465038  0.234001      0.070086  0.198053  0.140825  0.341286  0.065068   \n",
       "1  0.504351  0.346376      0.089145  0.269842  0.170089  0.443280  0.067823   \n",
       "2  0.488003  0.334881      0.102459  0.303481  0.201692  0.470113  0.064957   \n",
       "3  0.537893  0.416145      0.127159  0.387229  0.252919  0.569076  0.065700   \n",
       "4  0.554659  0.398666      0.144628  0.425485  0.300826  0.585571  0.063842   \n",
       "\n",
       "      UI_10        AD       OBV  \n",
       "0  0.242227  0.000000  0.000000  \n",
       "1  0.232220  0.000332  0.005466  \n",
       "2  0.225461  0.001039  0.000922  \n",
       "3  0.207301  0.003744  0.005626  \n",
       "4  0.179492  0.005849  0.001179  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalization\n",
    "for i, df in enumerate(df_list):\n",
    "    df_list[i].iloc[:,3:] = df_list[i].iloc[:,3:].apply(\n",
    "        lambda x: (x - np.min(x)) / (np.max(x) - np.min(x)))\n",
    "    \n",
    "df_list[0].head()\n",
    "\n",
    "#array_X = np.array(df_X_nor.T)\n",
    "#array_y = np.array(df_y_nor.T)/1000 \n",
    "# i think it would fit ANN more? ie., more close to 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1777ccf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many missing values in each dataset\n",
      "US:              0 \n",
      "Canada:          0 \n",
      "Japan:           2 \n",
      "Hong_Kong:       5 \n",
      "Spain:           0 \n",
      "France:          0 \n",
      "Netherlands:     0 \n",
      "Brazil:          2 \n",
      "Mexico:          0 \n",
      "China:           0 \n",
      "Turkey:          5 \n",
      "Indonesia:       0 \n",
      "India:           0 \n",
      "\n",
      "and all blanks have been filled\n"
     ]
    }
   ],
   "source": [
    "# there's a missing value issue, but with a very small proportion (5/2000)\n",
    "# df_list_n[10]['UI_10'][df_list_n[10]['UI_10'].isna()==True]\n",
    "print('How many missing values in each dataset')\n",
    "for region, df in zip(region_list, df_list):\n",
    "    print('{:<12} {:>5d} '.format(region+':', df.isna().sum().sum()))\n",
    "    \n",
    "# fill blanks using the most recent values\n",
    "for i, df in enumerate(df_list):\n",
    "    df_list[i]['UI_10'] = df_list[i]['UI_10'].fillna(method = 'ffill')\n",
    "print('\\nand all blanks have been filled')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e5cf77",
   "metadata": {},
   "source": [
    "## principal components analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da14e5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeromean(dataframe):   \n",
    "    \"\"\" \n",
    "    Args:    \n",
    "    dataframe(pd.DataFrame)\n",
    "    \n",
    "    Returns:    \n",
    "    dataframe_new(pd.DataFrame): de-meaned dataframe    \n",
    "    mean(float): mean of each features\n",
    "    \"\"\"\n",
    "    mean = np.mean(dataframe, axis = 0)     \n",
    "    dataframe_new = dataframe - mean\n",
    "    return dataframe_new, mean\n",
    "\n",
    "def percentage_to_n(eig_vals, percentage):\n",
    "    \"\"\"\n",
    "    Args:    \n",
    "    eig_vals(np.array): eigenvalues of a matrix\n",
    "    percentage(float): how much explanation needed\n",
    "    \n",
    "    Returns:    \n",
    "    pca_num(int): how many eigenvectors needed to achieve the explanation\n",
    "    \"\"\"\n",
    "    sort_eig_vals = np.sort(eig_vals)\n",
    "    sort_eig_vals = sort_eig_vals[-1::-1]\n",
    "    sum_eig_vals  = sum(sort_eig_vals)\n",
    "    \n",
    "    pca_sum = 0\n",
    "    pca_num = 0\n",
    "    \n",
    "    for i in sort_eig_vals:\n",
    "        pca_sum += i\n",
    "        pca_num += 1\n",
    "        if pca_sum >= sum_eig_vals * percentage:\n",
    "            break\n",
    "    \n",
    "    return pca_num\n",
    " \n",
    "def pca(dataframe, percentage = 0.99):\n",
    "    \"\"\"\n",
    "    Args:    \n",
    "    dataframe(pd.DataFrame)\n",
    "    percentage(float): how much explanation needed\n",
    "    \n",
    "    Returns:    \n",
    "    low_dimen_data(pd.DataFrame): dataframe after pca\n",
    "    n(int): how many eigenvectors needed to achieve the explanation\n",
    "    \"\"\"\n",
    "    dataframe_new, mean = zeromean(dataframe)\n",
    "    \n",
    "    cov_matrix = np.cov(dataframe_new, rowvar = 0)    \n",
    "    eig_vals,eig_vects = np.linalg.eig(np.mat(cov_matrix))\n",
    "    \n",
    "    # choose how many pcs needed to explain 99%\n",
    "    n = percentage_to_n(eig_vals, percentage)\n",
    "\n",
    "    # choose top n\n",
    "    eig_val_indice = np.argsort(eig_vals)\n",
    "    n_eig_val_indice = eig_val_indice[-1:-(n+1):-1] \n",
    "    \n",
    "    # reconstruct data\n",
    "    n_eig_vect = eig_vects[:, n_eig_val_indice]\n",
    "    low_dimen_data = dataframe_new @ n_eig_vect\n",
    "    return low_dimen_data, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f6ed591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many PCs extracted in each dataset for 99% explanation\n",
      "US:              8 PCs \n",
      "Canada:          8 PCs \n",
      "Japan:           8 PCs \n",
      "Hong_Kong:       9 PCs \n",
      "Spain:           9 PCs \n",
      "France:          9 PCs \n",
      "Netherlands:     8 PCs \n",
      "Brazil:          8 PCs \n",
      "Mexico:          9 PCs \n",
      "China:           8 PCs \n",
      "Turkey:          9 PCs \n",
      "Indonesia:       9 PCs \n",
      "India:           8 PCs \n"
     ]
    }
   ],
   "source": [
    "# x partition and y partition\n",
    "array_list_x = []\n",
    "array_list_y = []\n",
    "\n",
    "print('How many PCs extracted in each dataset for 99% explanation')\n",
    "for region, df in zip(region_list, df_list):\n",
    "    array_list_x.append(np.array(pca(df.iloc[:,3:])[0].T))\n",
    "    print('{:<12} {:>5d} PCs '.format(region+':', pca(df.iloc[:,3:])[1]))\n",
    "\n",
    "for region, df in zip(region_list, df_list):\n",
    "    array_list_y.append(np.array(df.iloc[:,2].T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c5e4712",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.259054</td>\n",
       "      <td>0.778901</td>\n",
       "      <td>0.216105</td>\n",
       "      <td>0.476754</td>\n",
       "      <td>-0.007245</td>\n",
       "      <td>-0.268125</td>\n",
       "      <td>0.036703</td>\n",
       "      <td>0.162534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.235716</td>\n",
       "      <td>0.599162</td>\n",
       "      <td>0.284994</td>\n",
       "      <td>0.449431</td>\n",
       "      <td>-0.064480</td>\n",
       "      <td>-0.231209</td>\n",
       "      <td>0.065403</td>\n",
       "      <td>0.154753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.232980</td>\n",
       "      <td>0.557111</td>\n",
       "      <td>0.278169</td>\n",
       "      <td>0.427872</td>\n",
       "      <td>-0.042229</td>\n",
       "      <td>-0.234233</td>\n",
       "      <td>0.034970</td>\n",
       "      <td>0.157879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.209271</td>\n",
       "      <td>0.380910</td>\n",
       "      <td>0.317534</td>\n",
       "      <td>0.381648</td>\n",
       "      <td>-0.061154</td>\n",
       "      <td>-0.217395</td>\n",
       "      <td>0.055226</td>\n",
       "      <td>0.154106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.203119</td>\n",
       "      <td>0.326902</td>\n",
       "      <td>0.290218</td>\n",
       "      <td>0.340068</td>\n",
       "      <td>-0.029309</td>\n",
       "      <td>-0.230123</td>\n",
       "      <td>0.050211</td>\n",
       "      <td>0.155465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -1.259054  0.778901  0.216105  0.476754 -0.007245 -0.268125  0.036703   \n",
       "1 -1.235716  0.599162  0.284994  0.449431 -0.064480 -0.231209  0.065403   \n",
       "2 -1.232980  0.557111  0.278169  0.427872 -0.042229 -0.234233  0.034970   \n",
       "3 -1.209271  0.380910  0.317534  0.381648 -0.061154 -0.217395  0.055226   \n",
       "4 -1.203119  0.326902  0.290218  0.340068 -0.029309 -0.230123  0.050211   \n",
       "\n",
       "          7  \n",
       "0  0.162534  \n",
       "1  0.154753  \n",
       "2  0.157879  \n",
       "3  0.154106  \n",
       "4  0.155465  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check dirty work using sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "df_test = df_list[0].iloc[:,3:]\n",
    "X = np.array(df_test)\n",
    "\n",
    "pca = PCA(n_components=8)\n",
    "pca_lib = pca.fit_transform(X)\n",
    "pd.DataFrame(pca_lib).head()\n",
    "# they are the same: proved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a124cfb",
   "metadata": {},
   "source": [
    "# FFNN: feed-foreward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18213529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function defines sigmoid activation function: f(x) = 1 / (1 + e^(-x))\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# this function defines the derivative of sigmoid function\n",
    "def deriv_sigmoid(x):\n",
    "    fx = sigmoid(x)\n",
    "    return fx * (1 - fx)\n",
    "\n",
    "# this function defines the loss function, Mean Squared Error (MSE)\n",
    "def MSE(y_true, y_prediction):\n",
    "    return ((y_true - y_prediction) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e34f6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function initialises the network with the structure of 5 layers\n",
    "# n1 is the number of neurons at the 1st hidden layer\n",
    "# n2 is the number of neurons at the 2nd hidden layer\n",
    "# n3 is the number of neurons at the 3rd hidden layer\n",
    "\n",
    "## could change the neurons at each layer here\n",
    "n1,  n2,  n3  = 5, 5, 5\n",
    "ann_structure = [n1, n2, n3]\n",
    "\n",
    "def parameters_reset (n1, n2, n3) :\n",
    "    \"\"\" \n",
    "    Args:    \n",
    "    n1,n2,n3(int): \n",
    "    how many neurons on each hidden layer\n",
    "    \n",
    "    Returns:    \n",
    "    W1,W2,W3,W4,b1,b2,b3,b4(float): \n",
    "    parameters set (same at the each time)\n",
    "    \"\"\"\n",
    "    # show that we are changing the global variables\n",
    "    global W1, W2, W3, W4, b1, b2, b3, b4\n",
    "    \n",
    "    # to keep the initial parameters the same\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    W1 = np.random.randn(n1, num_input) / 2\n",
    "    W2 = np.random.randn(n2, n1) / 2\n",
    "    W3 = np.random.randn(n3, n2) / 2\n",
    "    W4 = np.random.randn(1 , n3) / 2\n",
    "    \n",
    "    b1 = np.random.randn(n1, 1 ) / 2\n",
    "    b2 = np.random.randn(n2, 1 ) / 2\n",
    "    b3 = np.random.randn(n3, 1 ) / 2\n",
    "    b4 = np.random.randn(1 , 1 ) / 2\n",
    "    \n",
    "# parameters_reset (n1, n2, n3)\n",
    "    \n",
    "# dimensions = n1 * num_input + n2 * n1 + n3 * n2 + 1 * n3 + n1 + n2 + n3 + 1\n",
    "# print('hi, under this setting, totally {:} dimensions of parameter'.format(dimensions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc36dbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function realizes the Feedforward\n",
    "def forward_prop (a0, W1, W2, W3, W4, b1, b2, b3, b4) :\n",
    "    \"\"\" \n",
    "    Args:    \n",
    "    a0(np.array): dataset (* which should be transposed)\n",
    "    W1,W2,W3,W4,b1,b2,b3,b4(float): parameters\n",
    "    \n",
    "    Returns:    \n",
    "    a0(np.array): dataset as the same as the input\n",
    "    s1,a1,s2,a2,s3,a3,s4(np.array): results of FFNN\n",
    "    \"\"\"\n",
    "    s1 = W1 @ a0 + b1\n",
    "    a1 = sigmoid(s1) \n",
    "    s2 = W2 @ a1 + b2\n",
    "    a2 = sigmoid(s2)\n",
    "    s3 = W3 @ a2 + b3\n",
    "    a3 = sigmoid(s3)\n",
    "    s4 = W4 @ a3 + b4\n",
    "    # since this is not a classification problem\n",
    "    # I do not put activation function on the output layer\n",
    "    \n",
    "    # return multiple values\n",
    "    return a0, s1, a1, s2, a2, s3, a3, s4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5a1b85",
   "metadata": {},
   "source": [
    "# FFNN: Jacobian matrixes set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a58dd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jacobian for the weights of the output layer\n",
    "def J_W4 (x, y) :\n",
    "    # d c /s4 = - 2 * (y - s4) \n",
    "    J = - 2 * (y - s4) \n",
    "    # d s4/w4 = a3\n",
    "    J = J @ a3.T / x.shape[1]\n",
    "    return J\n",
    "\n",
    "# Jacobian for the bias of the output layer\n",
    "def J_b4 (x, y) :\n",
    "    # d c /s4 = - 2 * (y - s4) \n",
    "    J = - 2 * (y - s4)\n",
    "    # d s4/b4 = 1\n",
    "    J = np.sum(J, axis=1, keepdims=True) / x.shape[1]\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d8a3baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jacobian for the weights of the 3rd hidden layer\n",
    "def J_W3 (x, y) :\n",
    "    # d c /s4 = - 2 * (y - s4) \n",
    "    J = - 2 * (y - s4)\n",
    "    # d s4/a3 = W4\n",
    "    J = (J.T @ W4).T\n",
    "    # d a3/s3 = s'(s3)\n",
    "    J = J * deriv_sigmoid(s3)\n",
    "    # d s3/W3 = a2\n",
    "    J = J @ a2.T / x.shape[1]\n",
    "    return J\n",
    "\n",
    "# Jacobian for the bias of the 3rd hidden layer\n",
    "def J_b3 (x, y) :\n",
    "    # d c /s4 = - 2 * (y - s4) \n",
    "    J = - 2 * (y - s4)\n",
    "    # d s4/a3 = W4\n",
    "    J = (J.T @ W4).T\n",
    "    # d a3/s3 = s'(s3)\n",
    "    J = J * deriv_sigmoid(s3)   \n",
    "    # d s3/b3 = 1\n",
    "    J = np.sum(J, axis=1, keepdims=True) / x.shape[1]\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14aa2f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jacobian for the weights of the 2nd hidden layer\n",
    "def J_W2 (x, y) :\n",
    "    # d c /s4 = - 2 * (y - s4) \n",
    "    J = - 2 * (y - s4)\n",
    "    # d s4/a3 = W4\n",
    "    J = (J.T @ W4).T\n",
    "    # d a3/s3 = s'(s3)\n",
    "    J = J * deriv_sigmoid(s3)\n",
    "    # d s3/a2 = W3\n",
    "    J = (J.T @ W3).T    \n",
    "    # d a2/s2 = s'(s2)\n",
    "    J = J * deriv_sigmoid(s2)  \n",
    "    # d s2/W2 = a1\n",
    "    J = J @ a1.T / x.shape[1]\n",
    "    return J\n",
    "\n",
    "# Jacobian for the biases of the 2nd hidden layer\n",
    "def J_b2 (x, y) :\n",
    "    # d c /s4 = - 2 * (y - s4) \n",
    "    J = - 2 * (y - s4)\n",
    "    # d s4/a3 = W4\n",
    "    J = (J.T @ W4).T\n",
    "    # d a3/s3 = s'(s3)\n",
    "    J = J * deriv_sigmoid(s3)\n",
    "    # d s3/a2 = W3\n",
    "    J = (J.T @ W3).T    \n",
    "    # d a2/s2 = s'(s2)\n",
    "    J = J * deriv_sigmoid(s2)  \n",
    "    # d s2/b2 = 1\n",
    "    J = np.sum(J, axis=1, keepdims=True) / x.shape[1]\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "161380ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jacobian for the weights of the 1st hidden layer\n",
    "def J_W1 (x, y) :\n",
    "    # d c /s4 = - 2 * (y - s4) \n",
    "    J = - 2 * (y - s4)\n",
    "    # d s4/a3 = W4\n",
    "    J = (J.T @ W4).T\n",
    "    # d a3/s3 = s'(s3)\n",
    "    J = J * deriv_sigmoid(s3)\n",
    "    # d s3/a2 = W3\n",
    "    J = (J.T @ W3).T    \n",
    "    # d a2/s2 = s'(s2)\n",
    "    J = J * deriv_sigmoid(s2)  \n",
    "    # d s2/a1 = W2\n",
    "    J = (J.T @ W2).T   \n",
    "    # d a1/s1 = s'(s1)\n",
    "    J = J * deriv_sigmoid(s1)   \n",
    "    # d s1/W1 = a0\n",
    "    J = J @ a0.T / x.shape[1]\n",
    "    return J    \n",
    "\n",
    "# Jacobian for the biases of the 1st hidden layer\n",
    "def J_b1 (x, y) :\n",
    "    # d c /s4 = - 2 * (y - s4) \n",
    "    J = - 2 * (y - s4)\n",
    "    # d s4/a3 = W4\n",
    "    J = (J.T @ W4).T\n",
    "    # d a3/s3 = s'(s3)\n",
    "    J = J * deriv_sigmoid(s3)\n",
    "    # d s3/a2 = W3\n",
    "    J = (J.T @ W3).T    \n",
    "    # d a2/s2 = s'(s2)\n",
    "    J = J * deriv_sigmoid(s2)  \n",
    "    # d s2/a1 = W2\n",
    "    J = (J.T @ W2).T   \n",
    "    # d a1/s1 = s'(s1)\n",
    "    J = J * deriv_sigmoid(s1)   \n",
    "    # d s1/b1 = 1\n",
    "    J = np.sum(J, axis=1, keepdims=True) / x.shape[1]\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae0bcee",
   "metadata": {},
   "source": [
    "# FFNN: particle swarm optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc06704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function calculates MSE without gradient components\n",
    "def MSE_calculation_in_PSO(ann_structure, array_x, array_y, paras):\n",
    "    \"\"\" \n",
    "    Args:  \n",
    "    ann_structure(list): include 3 numbers, indicating the neurons at each layer\n",
    "    array_x(np.array): dataset (* which should be transposed)\n",
    "    array_y(np.array): true values (* which should be transposed)\n",
    "    paras(np.array): parameters\n",
    "    \n",
    "    Returns:    \n",
    "    MSE(float): MSE at the input parameters\n",
    "    \"\"\"\n",
    "    n1, n2, n3 = ann_structure\n",
    "    num_input  = array_x.shape[0]\n",
    "    # transform parameters from vector to matrix\n",
    "    t1 = n1 * num_input\n",
    "    t2 = t1 + n2 * n1\n",
    "    t3 = t2 + n3 * n2\n",
    "    t4 = t3 + n3\n",
    "    t5 = t4 + n1\n",
    "    t6 = t5 + n2\n",
    "    t7 = t6 + n3\n",
    "    t8 = t7 + 1\n",
    "    \n",
    "    return MSE(array_y, forward_prop(array_x, \n",
    "                                        paras[0  : t1].reshape(n1 , num_input), \n",
    "                                        paras[t1 : t2].reshape(n2 , n1), \n",
    "                                        paras[t2 : t3].reshape(n3 , n2), \n",
    "                                        paras[t3 : t4].reshape(1  , n3), \n",
    "                                        paras[t4 : t5].reshape(n1 ,  1), \n",
    "                                        paras[t5 : t6].reshape(n2 ,  1), \n",
    "                                        paras[t6 : t7].reshape(n3 ,  1), \n",
    "                                        paras[t7 : t8].reshape(1  ,  1))[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e9c4fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pso(ann_structure, array_x, array_y):\n",
    "    \"\"\" \n",
    "    Args:   \n",
    "    ann_structure(list): include 3 numbers, indicating the neurons at each layer\n",
    "    array_x(np.array): dataset (* which should be transposed)\n",
    "    array_y(np.array): true values (* which should be transposed)\n",
    "    \n",
    "    Returns:    \n",
    "    W1_pso, W2_pso, W3_pso, W4_pso, b1_pso, b2_pso, b3_pso, b4_pso(np.array): \n",
    "    parameters found by PSO\n",
    "    \"\"\"\n",
    "    # 1. initialize the particles\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    n_particles = 50\n",
    "    n1, n2, n3  = ann_structure\n",
    "    num_input   = array_x.shape[0]\n",
    "    dimensions  = n1 * num_input + n2 * n1 + n3 * n2 + 1 * n3 + n1 + n2 + n3 + 1\n",
    "\n",
    "    # coordinate, [0,1]\n",
    "    X = np.random.rand (dimensions, n_particles) / 2\n",
    "    # velocity, from normal distribution\n",
    "    V = np.random.randn(dimensions, n_particles) / 10\n",
    "\n",
    "    # 2. period 1 initialization\n",
    "    \n",
    "    # p_best_loc: personal best location\n",
    "    # p_best_obj: personal best objective function value\n",
    "    # g_best_loc: global best location\n",
    "    # g_best_obj: global best objective function value\n",
    "\n",
    "    p_best_loc = X\n",
    "    p_best_obj = np.array([MSE_calculation_in_PSO(ann_structure, \n",
    "                                                  array_x, \n",
    "                                                  array_y, \n",
    "                                                  paras) for paras in X.T])\n",
    "    g_best_loc = p_best_loc[:,p_best_obj.argmin()]\n",
    "    g_best_obj = p_best_obj.min()\n",
    "    \n",
    "    initial_loss = g_best_obj.copy()\n",
    "    \n",
    "    # print('hi, at the initialization, PSO finds {:4f} as MSE'.format(g_best_obj))\n",
    "    \n",
    "    # 3. algorithm\n",
    "    \n",
    "    # parameters setting\n",
    "    # w: the inertia weight constant\n",
    "    # it decides how much the particle should keep on with its previous velocity\n",
    "    # c1: cognitive coefficient\n",
    "    # it decides how much the particle should insist the search result of the particle\n",
    "    # c2: social coefficients\n",
    "    # it decides how much the particle should insist the search result of the swarm\n",
    "\n",
    "    w, c1, c2, seed = 0.8, 0.1, 0.1, 0\n",
    "\n",
    "    # iterations\n",
    "    for i in range(100):\n",
    "    \n",
    "        # randomness in c1, c2\n",
    "        np.random.seed(seed)\n",
    "        r = np.random.rand(2)\n",
    "    \n",
    "        # velocity update\n",
    "        V = w * V + c1 * r[0] * (p_best_loc - X) + \\\n",
    "                    c2 * r[1] * (g_best_loc.reshape(-1,1) - X)\n",
    "        # position update\n",
    "        X = X + V\n",
    "    \n",
    "        # objective at this round\n",
    "        obj = np.array([MSE_calculation_in_PSO(ann_structure,\n",
    "                                               array_x, \n",
    "                                               array_y, \n",
    "                                               paras) for paras in X.T])\n",
    "    \n",
    "        # personal best location update\n",
    "        p_best_loc[:, (obj <= p_best_obj)] = X[:, (obj <= p_best_obj)]\n",
    "        # personal best value update\n",
    "        p_best_obj = np.array([p_best_obj, obj]).min(axis=0)\n",
    "        # global best location update\n",
    "        g_best_loc = p_best_loc[:,p_best_obj.argmin()]\n",
    "        # global best value update\n",
    "        g_best_obj = p_best_obj.min()\n",
    "    \n",
    "        seed += 1\n",
    "    \n",
    "        # print('hi, at the interation of {:}, PSO finds {:.4f} as MSE'.format(i+1, g_best_obj))\n",
    "    \n",
    "    t1 = n1 * num_input\n",
    "    t2 = t1 + n2 * n1\n",
    "    t3 = t2 + n3 * n2\n",
    "    t4 = t3 + n3\n",
    "    t5 = t4 + n1\n",
    "    t6 = t5 + n2\n",
    "    t7 = t6 + n3\n",
    "    t8 = t7 + 1\n",
    "\n",
    "        \n",
    "    W1_pso = g_best_loc[0  : t1].reshape(n1 , num_input)\n",
    "    W2_pso = g_best_loc[t1 : t2].reshape(n2 , n1)\n",
    "    W3_pso = g_best_loc[t2 : t3].reshape(n3 , n2)\n",
    "    W4_pso = g_best_loc[t3 : t4].reshape(1  , n3)\n",
    "    b1_pso = g_best_loc[t4 : t5].reshape(n1 ,  1)\n",
    "    b2_pso = g_best_loc[t5 : t6].reshape(n2 ,  1)\n",
    "    b3_pso = g_best_loc[t6 : t7].reshape(n3 ,  1)\n",
    "    b4_pso = g_best_loc[t7 : t8].reshape(1  ,  1)\n",
    "    \n",
    "    return W1_pso, W2_pso, W3_pso, W4_pso, b1_pso, b2_pso, b3_pso, b4_pso, initial_loss\n",
    "\n",
    "    # print('\\nthe optimal parameters have been kept within those suffixed as \"_pso\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c49048",
   "metadata": {},
   "source": [
    "# FFNN: mini-batch stochastic GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd21ed62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function that randomly sorts the data batches (size > 1)\n",
    "# note: the data strcuture of X and y should be DataFrame\n",
    "def batches(X, y, size, seed):\n",
    "    \"\"\" \n",
    "    Args:    \n",
    "    X(pd.DataFrame): dataset (* which should NOT be transposed)\n",
    "    y(pd.DataFrame): true values (* which should NOT be transposed)\n",
    "    \n",
    "    Returns:    \n",
    "    batches_full(list): data batches\n",
    "    \"\"\"\n",
    "    # 1.fix this random seed at this moment, and this will be changed for each iteration\n",
    "    np.random.seed(seed)\n",
    "    # 2.number of total instances\n",
    "    num_instances = X.shape[0]\n",
    "    # 3.list initialized\n",
    "    batches_full  = []\n",
    "    \n",
    "    # 4.shuffle X and y \n",
    "    shuf_order = list(np.random.permutation(num_instances))\n",
    "    shuf_X     = X.iloc[shuf_order,:]\n",
    "    shuf_y     = y.iloc[shuf_order,:]\n",
    "    \n",
    "    # 5.get the maximum number of the batches could be reached\n",
    "    num_batch  = math.floor(num_instances / size) \n",
    "    \n",
    "    # 6.get the new sorted batches\n",
    "    for k in range(num_batch):\n",
    "        batches_X = shuf_X.iloc[k*size : (k+1)*size-1 ,:]\n",
    "        batches_y = shuf_y.iloc[k*size : (k+1)*size-1 ,:]\n",
    "        batches   = (batches_X, batches_y)\n",
    "        batches_full.append(batches)\n",
    "        \n",
    "    # 7.get the last remaining batch\n",
    "    if num_instances % size != 0:\n",
    "        batches_X = shuf_X.iloc[size * num_batch:,:]\n",
    "        batches_y = shuf_y.iloc[size * num_batch:,:]\n",
    "        batches   = (batches_X, batches_y)\n",
    "        batches_full.append(batches)\n",
    "        \n",
    "    return batches_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2b1641",
   "metadata": {},
   "source": [
    "# ARDL: augmented Dicky–Fuller test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b27916c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make copies for ARDL model\n",
    "array_list_x_ARDL = array_list_x.copy()\n",
    "array_list_y_ARDL = array_list_y.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "58bc8a2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>series</th>\n",
       "      <th>index_1</th>\n",
       "      <th>index_2</th>\n",
       "      <th>t_statistic</th>\n",
       "      <th>p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>CL</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>PC_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>US</td>\n",
       "      <td>PC_8</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>-2.41</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Canada</td>\n",
       "      <td>CL</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>-1.71</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Canada</td>\n",
       "      <td>PC_1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.36</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Japan</td>\n",
       "      <td>CL</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Japan</td>\n",
       "      <td>PC_1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Hong_Kong</td>\n",
       "      <td>CL</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>-2.37</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Hong_Kong</td>\n",
       "      <td>PC_1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.55</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Hong_Kong</td>\n",
       "      <td>PC_4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.52</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Spain</td>\n",
       "      <td>CL</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>-2.70</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Spain</td>\n",
       "      <td>PC_3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>France</td>\n",
       "      <td>CL</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>-1.79</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>France</td>\n",
       "      <td>PC_1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.90</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Netherlands</td>\n",
       "      <td>CL</td>\n",
       "      <td>6</td>\n",
       "      <td>100</td>\n",
       "      <td>-1.01</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Netherlands</td>\n",
       "      <td>PC_1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.06</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>CL</td>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>PC_1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.23</td>\n",
       "      <td>0.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Mexico</td>\n",
       "      <td>CL</td>\n",
       "      <td>8</td>\n",
       "      <td>100</td>\n",
       "      <td>-2.52</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Mexico</td>\n",
       "      <td>PC_1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.49</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>China</td>\n",
       "      <td>CL</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>-2.15</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>China</td>\n",
       "      <td>PC_3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>-2.77</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Turkey</td>\n",
       "      <td>CL</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>-2.01</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Turkey</td>\n",
       "      <td>PC_2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.39</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Turkey</td>\n",
       "      <td>PC_6</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.54</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Indonesia</td>\n",
       "      <td>PC_1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.73</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Indonesia</td>\n",
       "      <td>PC_9</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>-2.75</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>India</td>\n",
       "      <td>CL</td>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>India</td>\n",
       "      <td>PC_1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          region series  index_1  index_2  t_statistic  p_value\n",
       "0             US     CL        0      100         0.57     0.99\n",
       "1             US   PC_1        0        0         0.24     0.97\n",
       "8             US   PC_8        0        7        -2.41     0.14\n",
       "9         Canada     CL        1      100        -1.71     0.43\n",
       "10        Canada   PC_1        1        0        -1.36     0.60\n",
       "18         Japan     CL        2      100        -0.44     0.90\n",
       "19         Japan   PC_1        2        0        -0.57     0.88\n",
       "27     Hong_Kong     CL        3      100        -2.37     0.15\n",
       "28     Hong_Kong   PC_1        3        0        -2.55     0.10\n",
       "31     Hong_Kong   PC_4        3        3        -2.52     0.11\n",
       "37         Spain     CL        4      100        -2.70     0.07\n",
       "40         Spain   PC_3        4        2        -0.98     0.76\n",
       "47        France     CL        5      100        -1.79     0.39\n",
       "48        France   PC_1        5        0        -1.90     0.33\n",
       "57   Netherlands     CL        6      100        -1.01     0.75\n",
       "58   Netherlands   PC_1        6        0        -1.06     0.73\n",
       "66        Brazil     CL        7      100        -0.63     0.86\n",
       "67        Brazil   PC_1        7        0        -1.23     0.66\n",
       "75        Mexico     CL        8      100        -2.52     0.11\n",
       "76        Mexico   PC_1        8        0        -2.49     0.12\n",
       "85         China     CL        9      100        -2.15     0.22\n",
       "88         China   PC_3        9        2        -2.77     0.06\n",
       "94        Turkey     CL       10      100        -2.01     0.28\n",
       "96        Turkey   PC_2       10        1        -2.39     0.14\n",
       "100       Turkey   PC_6       10        5        -0.54     0.88\n",
       "105    Indonesia   PC_1       11        0        -2.73     0.07\n",
       "113    Indonesia   PC_9       11        8        -2.75     0.07\n",
       "114        India     CL       12      100         0.01     0.96\n",
       "115        India   PC_1       12        0        -0.17     0.94"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create ADF table on PCs\n",
    "ADF_result = []\n",
    "\n",
    "for i, region in enumerate(region_list):\n",
    "    ADF_result.append([region, 'CL', int(i), int(100), # index y using 100\n",
    "                       adfuller(array_list_y_ARDL[i], autolag='AIC')[0],\n",
    "                       adfuller(array_list_y_ARDL[i], autolag='AIC')[1]])\n",
    "    for j in range(len(array_list_x_ARDL[i])):\n",
    "        ADF_result.append([region, 'PC' + '_' + str(j+1), int(i), int(j),\n",
    "                           adfuller(array_list_x_ARDL[i][j], autolag='AIC')[0],\n",
    "                           adfuller(array_list_x_ARDL[i][j], autolag='AIC')[1]])\n",
    "\n",
    "ADF_table = pd.DataFrame(ADF_result, columns=[\n",
    "    'region','series','index_1','index_2','t_statistic','p_value'])\n",
    "ADF_table[['t_statistic','p_value']] = ADF_table[['t_statistic','p_value']].round(2)\n",
    "\n",
    "# check stationarity at the significance level of 5%\n",
    "ADF_table[ADF_table.p_value > 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "465722c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make difference series of y\n",
    "ADF_1_diff = []\n",
    "array_list_y_ARDL_1_diff = array_list_y_ARDL.copy()\n",
    "\n",
    "for i in range(len(array_list_y_ARDL)):\n",
    "    array_list_y_ARDL_1_diff[i] = np.diff(array_list_y_ARDL[i])\n",
    "    \n",
    "for i, region in enumerate(region_list):\n",
    "    ADF_1_diff.append([region, 'CL', int(i), int(100),\n",
    "                              adfuller(array_list_y_ARDL_1_diff[i], autolag='AIC')[0],\n",
    "                              adfuller(array_list_y_ARDL_1_diff[i], autolag='AIC')[1]])\n",
    "\n",
    "# make difference series of non-stationary x\n",
    "array_list_x_ARDL_1_diff = array_list_x_ARDL.copy()\n",
    "\n",
    "for i in range(len(array_list_y_ARDL)):\n",
    "    array_list_x_ARDL_1_diff[i] = np.diff(array_list_x_ARDL[i])\n",
    "    \n",
    "index_non_stationary = np.array( \\\n",
    "ADF_table[(ADF_table.p_value > 0.05) & (ADF_table.index_2 != 100)][['index_1','index_2']])\n",
    "\n",
    "for i, j in index_non_stationary:\n",
    "    ADF_1_diff.append([region_list[i], 'PC' + '_' + str(j+1), int(i), int(j),\n",
    "                              adfuller(array_list_x_ARDL_1_diff[i][j], autolag='AIC')[0],\n",
    "                              adfuller(array_list_x_ARDL_1_diff[i][j], autolag='AIC')[1]])  \n",
    "\n",
    "# make a table for these\n",
    "ADF_1_diff_table = pd.DataFrame(ADF_1_diff,\n",
    "                         columns=['region','series','index_1','index_2',\n",
    "                                  't_statistic','p_value'])\n",
    "\n",
    "ADF_1_diff_table[['t_statistic','p_value']] = \\\n",
    "ADF_1_diff_table[['t_statistic','p_value']].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b40f7d2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>series</th>\n",
       "      <th>index_1</th>\n",
       "      <th>index_2</th>\n",
       "      <th>t_statistic</th>\n",
       "      <th>p_value</th>\n",
       "      <th>t_statistic_1st_diff</th>\n",
       "      <th>p_value_1st_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>CL</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.99</td>\n",
       "      <td>-11.22</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>PC_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.97</td>\n",
       "      <td>-12.51</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>US</td>\n",
       "      <td>PC_8</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>-2.41</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-13.95</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Canada</td>\n",
       "      <td>CL</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>-1.71</td>\n",
       "      <td>0.43</td>\n",
       "      <td>-10.81</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Canada</td>\n",
       "      <td>PC_1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.36</td>\n",
       "      <td>0.60</td>\n",
       "      <td>-11.74</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Japan</td>\n",
       "      <td>CL</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>0.90</td>\n",
       "      <td>-35.17</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Japan</td>\n",
       "      <td>PC_1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.57</td>\n",
       "      <td>0.88</td>\n",
       "      <td>-19.87</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Hong_Kong</td>\n",
       "      <td>CL</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>-2.37</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-51.21</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Hong_Kong</td>\n",
       "      <td>PC_1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.55</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-14.58</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Hong_Kong</td>\n",
       "      <td>PC_4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-2.52</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-15.13</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Spain</td>\n",
       "      <td>CL</td>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>-2.70</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-19.26</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Spain</td>\n",
       "      <td>PC_3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>0.76</td>\n",
       "      <td>-20.40</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>France</td>\n",
       "      <td>CL</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>-1.79</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-19.01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>France</td>\n",
       "      <td>PC_1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.90</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-19.85</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Netherlands</td>\n",
       "      <td>CL</td>\n",
       "      <td>6</td>\n",
       "      <td>100</td>\n",
       "      <td>-1.01</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-19.08</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Netherlands</td>\n",
       "      <td>PC_1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.06</td>\n",
       "      <td>0.73</td>\n",
       "      <td>-17.81</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>CL</td>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>0.86</td>\n",
       "      <td>-9.36</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>PC_1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.23</td>\n",
       "      <td>0.66</td>\n",
       "      <td>-13.21</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Mexico</td>\n",
       "      <td>CL</td>\n",
       "      <td>8</td>\n",
       "      <td>100</td>\n",
       "      <td>-2.52</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-23.41</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Mexico</td>\n",
       "      <td>PC_1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.49</td>\n",
       "      <td>0.12</td>\n",
       "      <td>-13.51</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>China</td>\n",
       "      <td>CL</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>-2.15</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-10.23</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>China</td>\n",
       "      <td>PC_3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>-2.77</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-14.59</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Turkey</td>\n",
       "      <td>CL</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>-2.01</td>\n",
       "      <td>0.28</td>\n",
       "      <td>-52.40</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Turkey</td>\n",
       "      <td>PC_2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.39</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-14.71</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Turkey</td>\n",
       "      <td>PC_6</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.54</td>\n",
       "      <td>0.88</td>\n",
       "      <td>-12.20</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Indonesia</td>\n",
       "      <td>PC_1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.73</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-9.94</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Indonesia</td>\n",
       "      <td>PC_9</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>-2.75</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-16.24</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>India</td>\n",
       "      <td>CL</td>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.96</td>\n",
       "      <td>-13.65</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>India</td>\n",
       "      <td>PC_1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.94</td>\n",
       "      <td>-13.34</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          region series  index_1  index_2  t_statistic  p_value  \\\n",
       "0             US     CL        0      100         0.57     0.99   \n",
       "1             US   PC_1        0        0         0.24     0.97   \n",
       "8             US   PC_8        0        7        -2.41     0.14   \n",
       "9         Canada     CL        1      100        -1.71     0.43   \n",
       "10        Canada   PC_1        1        0        -1.36     0.60   \n",
       "18         Japan     CL        2      100        -0.44     0.90   \n",
       "19         Japan   PC_1        2        0        -0.57     0.88   \n",
       "27     Hong_Kong     CL        3      100        -2.37     0.15   \n",
       "28     Hong_Kong   PC_1        3        0        -2.55     0.10   \n",
       "31     Hong_Kong   PC_4        3        3        -2.52     0.11   \n",
       "37         Spain     CL        4      100        -2.70     0.07   \n",
       "40         Spain   PC_3        4        2        -0.98     0.76   \n",
       "47        France     CL        5      100        -1.79     0.39   \n",
       "48        France   PC_1        5        0        -1.90     0.33   \n",
       "57   Netherlands     CL        6      100        -1.01     0.75   \n",
       "58   Netherlands   PC_1        6        0        -1.06     0.73   \n",
       "66        Brazil     CL        7      100        -0.63     0.86   \n",
       "67        Brazil   PC_1        7        0        -1.23     0.66   \n",
       "75        Mexico     CL        8      100        -2.52     0.11   \n",
       "76        Mexico   PC_1        8        0        -2.49     0.12   \n",
       "85         China     CL        9      100        -2.15     0.22   \n",
       "88         China   PC_3        9        2        -2.77     0.06   \n",
       "94        Turkey     CL       10      100        -2.01     0.28   \n",
       "96        Turkey   PC_2       10        1        -2.39     0.14   \n",
       "100       Turkey   PC_6       10        5        -0.54     0.88   \n",
       "105    Indonesia   PC_1       11        0        -2.73     0.07   \n",
       "113    Indonesia   PC_9       11        8        -2.75     0.07   \n",
       "114        India     CL       12      100         0.01     0.96   \n",
       "115        India   PC_1       12        0        -0.17     0.94   \n",
       "\n",
       "     t_statistic_1st_diff  p_value_1st_diff  \n",
       "0                  -11.22               0.0  \n",
       "1                  -12.51               0.0  \n",
       "8                  -13.95               0.0  \n",
       "9                  -10.81               0.0  \n",
       "10                 -11.74               0.0  \n",
       "18                 -35.17               0.0  \n",
       "19                 -19.87               0.0  \n",
       "27                 -51.21               0.0  \n",
       "28                 -14.58               0.0  \n",
       "31                 -15.13               0.0  \n",
       "37                 -19.26               0.0  \n",
       "40                 -20.40               0.0  \n",
       "47                 -19.01               0.0  \n",
       "48                 -19.85               0.0  \n",
       "57                 -19.08               0.0  \n",
       "58                 -17.81               0.0  \n",
       "66                  -9.36               0.0  \n",
       "67                 -13.21               0.0  \n",
       "75                 -23.41               0.0  \n",
       "76                 -13.51               0.0  \n",
       "85                 -10.23               0.0  \n",
       "88                 -14.59               0.0  \n",
       "94                 -52.40               0.0  \n",
       "96                 -14.71               0.0  \n",
       "100                -12.20               0.0  \n",
       "105                 -9.94               0.0  \n",
       "113                -16.24               0.0  \n",
       "114                -13.65               0.0  \n",
       "115                -13.34               0.0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if 1st differenced series work\n",
    "ADF_result_table = pd.merge(left  = ADF_table,\n",
    "                            right = ADF_1_diff_table,\n",
    "                            on = ['region','series','index_1','index_2'],\n",
    "                            how = 'left',\n",
    "                            suffixes=['', '_1st_diff'])\n",
    "\n",
    "ADF_result_table[ADF_result_table.p_value > 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "670d274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stationary dataset\n",
    "array_list_y_ARDL_sta = array_list_y_ARDL_1_diff.copy()\n",
    "array_list_x_ARDL_sta = array_list_x.copy()\n",
    "\n",
    "for i in range(len(region_list)):\n",
    "    # delete first obs for all series, including stationary ones\n",
    "    array_list_x_ARDL_sta[i] = np.delete(array_list_x_ARDL_sta[i], 0, axis=1)\n",
    "    for j in range(len(array_list_x_ARDL[i])):\n",
    "        # transform non-stationary into stationary ones\n",
    "        if (i,j) in [(i[0],i[1]) for i in index_non_stationary]:\n",
    "            array_list_x_ARDL_sta[i][j] = array_list_x_ARDL_1_diff[i][j]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585484b7",
   "metadata": {},
   "source": [
    "# Train, validation, test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc892a25",
   "metadata": {},
   "source": [
    "## for ARDL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb6bf3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize y\n",
    "# in order to keep y the same as in FFNN\n",
    "for i in range(len(array_list_y_ARDL_sta)):\n",
    "    np_min = np.min(array_list_y_ARDL_sta[i])\n",
    "    np_max = np.max(array_list_y_ARDL_sta[i])\n",
    "    array_list_y_ARDL_sta[i] = (array_list_y_ARDL_sta[i] - np_min)/(np_max - np_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "40363d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make split with 0.70:0.15:0.15\n",
    "\n",
    "ARDL_train_x = []; ARDL_train_y = []\n",
    "ARDL_val_x   = []; ARDL_val_y   = []\n",
    "ARDL_test_x  = []; ARDL_test_y  = []\n",
    "\n",
    "ARDL_total = []; ARDL_train = []\n",
    "ARDL_val   = []; ARDL_test  = []\n",
    "\n",
    "for i in range(len(region_list)):\n",
    "    # pin down the size\n",
    "    train_n = math.floor(array_list_x_ARDL_sta[i].shape[1]*0.70)\n",
    "    val_n   = math.floor(array_list_x_ARDL_sta[i].shape[1]*0.15)\n",
    "    # record different parts\n",
    "    ARDL_total.append(array_list_x_ARDL_sta[i].shape[1])\n",
    "    ARDL_train.append(train_n)\n",
    "    ARDL_val.append  (val_n)\n",
    "    ARDL_test.append (array_list_x_ARDL_sta[i].shape[1] - train_n - val_n)\n",
    "    # train\n",
    "    ARDL_train_x.append(array_list_x_ARDL_sta[i][:, :train_n]) \n",
    "    ARDL_train_y.append(array_list_y_ARDL_sta[i][:train_n]) \n",
    "    # validation\n",
    "    ARDL_val_x.append  (array_list_x_ARDL_sta[i][:, train_n : (train_n+val_n)])  \n",
    "    ARDL_val_y.append  (array_list_y_ARDL_sta[i][train_n : (train_n+val_n)])\n",
    "    # test\n",
    "    ARDL_test_x.append (array_list_x_ARDL_sta[i][:, (train_n+val_n):])  \n",
    "    ARDL_test_y.append (array_list_y_ARDL_sta[i][(train_n+val_n) :])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5b16468",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show the sample size\n",
    "ARDL_num    = np.array([region_list, ARDL_total, ARDL_train, ARDL_val, ARDL_test]).T\n",
    "df_ARDL_num = pd.DataFrame(data = ARDL_num,\n",
    "                           columns = ['region','total','train','validation','test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7248afe4",
   "metadata": {},
   "source": [
    "## for FFNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "402444b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change closing price to return\n",
    "# in order to keep y the same as in ARDL\n",
    "\n",
    "FFNN_array_list_x = array_list_x.copy() \n",
    "FFNN_array_list_y = array_list_y.copy() \n",
    "\n",
    "# make 1st diff on y and normalize it\n",
    "for i in range(len(FFNN_array_list_y)):\n",
    "    FFNN_array_list_y[i] = np.diff(FFNN_array_list_y[i])\n",
    "    np_min = np.min(FFNN_array_list_y[i])\n",
    "    np_max = np.max(FFNN_array_list_y[i])\n",
    "    FFNN_array_list_y[i] = (FFNN_array_list_y[i] - np_min)/(np_max - np_min)\n",
    "\n",
    "# delete the final obs at each independent variable\n",
    "for i in range(len(FFNN_array_list_x)):\n",
    "    FFNN_array_list_x[i] = np.delete(FFNN_array_list_x[i], -1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52905c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make split with 0.70:0.15:0.15\n",
    "\n",
    "FFNN_train_x = []; FFNN_train_y = []\n",
    "FFNN_val_x   = []; FFNN_val_y   = []\n",
    "FFNN_test_x  = []; FFNN_test_y  = []\n",
    "\n",
    "FFNN_total = []; FFNN_train = []\n",
    "FFNN_val   = []; FFNN_test  = []\n",
    "\n",
    "for i in range(len(region_list)):\n",
    "    # pin down the size\n",
    "    train_n = math.floor(FFNN_array_list_x[i].shape[1]*0.70)\n",
    "    val_n   = math.floor(FFNN_array_list_x[i].shape[1]*0.15)\n",
    "    # record different parts\n",
    "    FFNN_total.append(FFNN_array_list_x[i].shape[1])\n",
    "    FFNN_train.append(train_n)\n",
    "    FFNN_val.append  (val_n)\n",
    "    FFNN_test.append (FFNN_array_list_x[i].shape[1] - train_n - val_n)\n",
    "    # train\n",
    "    FFNN_train_x.append(FFNN_array_list_x[i][:, :train_n]) \n",
    "    FFNN_train_y.append(FFNN_array_list_y[i][:train_n]) \n",
    "    # validation\n",
    "    FFNN_val_x.append  (FFNN_array_list_x[i][:, train_n : (train_n+val_n)])  \n",
    "    FFNN_val_y.append  (FFNN_array_list_y[i][train_n : (train_n+val_n)])\n",
    "    # test\n",
    "    FFNN_test_x.append (FFNN_array_list_x[i][:, (train_n+val_n):])  \n",
    "    FFNN_test_y.append (FFNN_array_list_y[i][(train_n+val_n) :])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59269c15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>total</th>\n",
       "      <th>train</th>\n",
       "      <th>validation</th>\n",
       "      <th>test</th>\n",
       "      <th>region</th>\n",
       "      <th>total</th>\n",
       "      <th>train</th>\n",
       "      <th>validation</th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>2743</td>\n",
       "      <td>1920</td>\n",
       "      <td>411</td>\n",
       "      <td>412</td>\n",
       "      <td>US</td>\n",
       "      <td>2743</td>\n",
       "      <td>1920</td>\n",
       "      <td>411</td>\n",
       "      <td>412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Canada</td>\n",
       "      <td>2734</td>\n",
       "      <td>1913</td>\n",
       "      <td>410</td>\n",
       "      <td>411</td>\n",
       "      <td>Canada</td>\n",
       "      <td>2734</td>\n",
       "      <td>1913</td>\n",
       "      <td>410</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Japan</td>\n",
       "      <td>2664</td>\n",
       "      <td>1864</td>\n",
       "      <td>399</td>\n",
       "      <td>401</td>\n",
       "      <td>Japan</td>\n",
       "      <td>2664</td>\n",
       "      <td>1864</td>\n",
       "      <td>399</td>\n",
       "      <td>401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hong_Kong</td>\n",
       "      <td>2679</td>\n",
       "      <td>1875</td>\n",
       "      <td>401</td>\n",
       "      <td>403</td>\n",
       "      <td>Hong_Kong</td>\n",
       "      <td>2679</td>\n",
       "      <td>1875</td>\n",
       "      <td>401</td>\n",
       "      <td>403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Spain</td>\n",
       "      <td>2786</td>\n",
       "      <td>1950</td>\n",
       "      <td>417</td>\n",
       "      <td>419</td>\n",
       "      <td>Spain</td>\n",
       "      <td>2786</td>\n",
       "      <td>1950</td>\n",
       "      <td>417</td>\n",
       "      <td>419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>France</td>\n",
       "      <td>2784</td>\n",
       "      <td>1948</td>\n",
       "      <td>417</td>\n",
       "      <td>419</td>\n",
       "      <td>France</td>\n",
       "      <td>2784</td>\n",
       "      <td>1948</td>\n",
       "      <td>417</td>\n",
       "      <td>419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Netherlands</td>\n",
       "      <td>2787</td>\n",
       "      <td>1950</td>\n",
       "      <td>418</td>\n",
       "      <td>419</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>2787</td>\n",
       "      <td>1950</td>\n",
       "      <td>418</td>\n",
       "      <td>419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>2690</td>\n",
       "      <td>1882</td>\n",
       "      <td>403</td>\n",
       "      <td>405</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>2690</td>\n",
       "      <td>1882</td>\n",
       "      <td>403</td>\n",
       "      <td>405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mexico</td>\n",
       "      <td>2727</td>\n",
       "      <td>1908</td>\n",
       "      <td>409</td>\n",
       "      <td>410</td>\n",
       "      <td>Mexico</td>\n",
       "      <td>2727</td>\n",
       "      <td>1908</td>\n",
       "      <td>409</td>\n",
       "      <td>410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>China</td>\n",
       "      <td>2644</td>\n",
       "      <td>1850</td>\n",
       "      <td>396</td>\n",
       "      <td>398</td>\n",
       "      <td>China</td>\n",
       "      <td>2644</td>\n",
       "      <td>1850</td>\n",
       "      <td>396</td>\n",
       "      <td>398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Turkey</td>\n",
       "      <td>2736</td>\n",
       "      <td>1915</td>\n",
       "      <td>410</td>\n",
       "      <td>411</td>\n",
       "      <td>Turkey</td>\n",
       "      <td>2736</td>\n",
       "      <td>1915</td>\n",
       "      <td>410</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Indonesia</td>\n",
       "      <td>2654</td>\n",
       "      <td>1857</td>\n",
       "      <td>398</td>\n",
       "      <td>399</td>\n",
       "      <td>Indonesia</td>\n",
       "      <td>2654</td>\n",
       "      <td>1857</td>\n",
       "      <td>398</td>\n",
       "      <td>399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>India</td>\n",
       "      <td>2675</td>\n",
       "      <td>1872</td>\n",
       "      <td>401</td>\n",
       "      <td>402</td>\n",
       "      <td>India</td>\n",
       "      <td>2675</td>\n",
       "      <td>1872</td>\n",
       "      <td>401</td>\n",
       "      <td>402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         region total train validation test       region total train  \\\n",
       "0            US  2743  1920        411  412           US  2743  1920   \n",
       "1        Canada  2734  1913        410  411       Canada  2734  1913   \n",
       "2         Japan  2664  1864        399  401        Japan  2664  1864   \n",
       "3     Hong_Kong  2679  1875        401  403    Hong_Kong  2679  1875   \n",
       "4         Spain  2786  1950        417  419        Spain  2786  1950   \n",
       "5        France  2784  1948        417  419       France  2784  1948   \n",
       "6   Netherlands  2787  1950        418  419  Netherlands  2787  1950   \n",
       "7        Brazil  2690  1882        403  405       Brazil  2690  1882   \n",
       "8        Mexico  2727  1908        409  410       Mexico  2727  1908   \n",
       "9         China  2644  1850        396  398        China  2644  1850   \n",
       "10       Turkey  2736  1915        410  411       Turkey  2736  1915   \n",
       "11    Indonesia  2654  1857        398  399    Indonesia  2654  1857   \n",
       "12        India  2675  1872        401  402        India  2675  1872   \n",
       "\n",
       "   validation test  \n",
       "0         411  412  \n",
       "1         410  411  \n",
       "2         399  401  \n",
       "3         401  403  \n",
       "4         417  419  \n",
       "5         417  419  \n",
       "6         418  419  \n",
       "7         403  405  \n",
       "8         409  410  \n",
       "9         396  398  \n",
       "10        410  411  \n",
       "11        398  399  \n",
       "12        401  402  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the sample size\n",
    "FFNN_num    = np.array([region_list, FFNN_total, FFNN_train, FFNN_val, FFNN_test]).T\n",
    "df_FFNN_num = pd.DataFrame(data = FFNN_num,\n",
    "                           columns = ['region','total','train','validation','test'])\n",
    "# left is FFNN, right is ARDL\n",
    "pd.concat([df_FFNN_num,df_ARDL_num],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6379ca4",
   "metadata": {},
   "source": [
    "## clarifications on datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "2b858164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see although shapes of datasets are the same for ARDL & FFNN\n",
    "# the y has been kept same (took first difference and normalize)\n",
    "# the inside values x can be largely different\n",
    "\n",
    "# for later use:\n",
    "\n",
    "# FFNN_train_x ; FFNN_train_y \n",
    "# FFNN_val_x   ; FFNN_val_y   \n",
    "# FFNN_test_x  ; FFNN_test_y \n",
    "\n",
    "# ARDL_train_x ; ARDL_train_y \n",
    "# ARDL_val_x   ; ARDL_val_y   \n",
    "# ARDL_test_x  ; ARDL_test_y  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc50db66",
   "metadata": {},
   "source": [
    "# FFNN: back-propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b6bc5e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimze FFNN using MBSGD\n",
    "\n",
    "def FFNN_MBSGD(ann_structure, array_x, array_y):\n",
    "    \"\"\" \n",
    "    Args:   \n",
    "    ann_structure(list): include 3 numbers, indicating the neurons at each layer\n",
    "    array_x(np.array): dataset (* which should be transposed)\n",
    "    array_y(np.array): true values (* which should be transposed)\n",
    "    \n",
    "    Returns:    \n",
    "    W1, W2, W3, W4, b1, b2, b3, b4(float): parameters\n",
    "    loss_bp(list): the loss before/after the MBSGD\n",
    "    initial_loss(float): initial loss before PSO\n",
    "    \"\"\"  \n",
    "    global a0, s1, a1, s2, a2, s3, a3, s4\n",
    "    global W1, W2, W3, W4, b1, b2, b3, b4\n",
    "    \n",
    "    # 1.MBSGD parameters setting\n",
    "    learn_rate      = 0.01\n",
    "    total_iteration = 100\n",
    "    # PSO has found good ones, so MBSGD performs bad\n",
    "    batch_size      = len(array_y)+1\n",
    "    seed_bp         = 0\n",
    "\n",
    "    # 2.initialize parameters using _pso\n",
    "    W1, W2, W3, W4, b1, b2, b3, b4, initial_loss = pso(ann_structure, array_x, array_y)\n",
    "\n",
    "    # 3.read ANN parameters from PSO and feedforeward\n",
    "    a0, s1, a1, s2, a2, s3, a3, s4 = forward_prop(array_x, W1, W2, W3, W4, b1, b2, b3, b4)\n",
    "\n",
    "    # 4.keep the initial loss\n",
    "    loss_bp = [MSE(array_y, s4)]\n",
    "    \n",
    "    # 5.reset dataset for MINIBATCH algorithm\n",
    "    df_X  = pd.DataFrame(array_x.T)\n",
    "    df_y  = pd.DataFrame(array_y.T)\n",
    "\n",
    "    # 6.backpropogation using MBSGD\n",
    "    for i in range (total_iteration): \n",
    "        # split data into batches\n",
    "        for Batch in batches (df_X, df_y, size = batch_size, seed = seed_bp):\n",
    "        \n",
    "            # get data batches\n",
    "            (Batch_X, Batch_y) = Batch\n",
    "            \n",
    "            # transform datches into arrays\n",
    "            Batch_X = np.array(Batch_X.T)\n",
    "            Batch_y = np.array(Batch_y.T)\n",
    "        \n",
    "            # parameters update\n",
    "            a0, s1, a1, s2, a2, s3, a3, s4 = forward_prop(Batch_X, W1, W2, W3, W4, b1, b2, b3, b4)\n",
    "            W1 -= learn_rate * J_W1 (Batch_X, Batch_y)\n",
    "            W2 -= learn_rate * J_W2 (Batch_X, Batch_y)\n",
    "            W3 -= learn_rate * J_W3 (Batch_X, Batch_y)\n",
    "            W4 -= learn_rate * J_W4 (Batch_X, Batch_y)\n",
    "            b1 -= learn_rate * J_b1 (Batch_X, Batch_y)\n",
    "            b2 -= learn_rate * J_b2 (Batch_X, Batch_y)\n",
    "            b3 -= learn_rate * J_b3 (Batch_X, Batch_y)\n",
    "            b4 -= learn_rate * J_b4 (Batch_X, Batch_y)\n",
    "        \n",
    "        # at the end of each iteration, we keep the loss for the all the instances\n",
    "        a0, s1, a1, s2, a2, s3, a3, s4 = forward_prop(array_x, W1, W2, W3, W4, b1, b2, b3, b4)\n",
    "        # loss = MSE(array_y, s4)\n",
    "        # loss_bp.append(loss)\n",
    "    \n",
    "        seed_bp += 1\n",
    "        # print('at the interation of {:}, MBSGD finds {:.4f} as MSE'.format(i+1, loss))\n",
    "    \n",
    "    loss_bp.append(MSE(array_y, s4))\n",
    "    return W1, W2, W3, W4, b1, b2, b3, b4, loss_bp, initial_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6373dbc1",
   "metadata": {},
   "source": [
    "# FFNN: hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4eaf2783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 5, 5], [10, 10, 10], [15, 15, 15], [20, 20, 20], [25, 25, 25]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create all possible hyper-parameters in FFNN\n",
    "list_hyper = [[a,a,a] for a in np.arange(5,26,5)]; list_hyper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c44a6594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FFNN_tunning(i, list_hyper):\n",
    "    \"\"\" \n",
    "    Args:   \n",
    "    i(int): the i-th region in the list of dataframes\n",
    "    list_hyper(list): the list of hyperparameters to attempt\n",
    "    \n",
    "    Returns:    \n",
    "    (int): the index of minimum MSE\n",
    "    \"\"\"  \n",
    "    \n",
    "    global a0, s1, a1, s2, a2, s3, a3, s4\n",
    "    global W1, W2, W3, W4, b1, b2, b3, b4 \n",
    "    \n",
    "    MSE_FFNN_tunning = np.empty(len(list_hyper))\n",
    "    \n",
    "    # fit model on training set and validate performance on validation set\n",
    "    for k, structure in enumerate(list_hyper):\n",
    "        # fit model\n",
    "        W1, W2, W3, W4, b1, b2, b3, b4, loss_bp, initial_loss = FFNN_MBSGD(\n",
    "            structure,\n",
    "            FFNN_train_x[i], \n",
    "            FFNN_train_y[i])\n",
    "        # validate on validation set\n",
    "        a0, s1, a1, s2, a2, s3, a3, s4 = forward_prop(\n",
    "            FFNN_val_x[i], \n",
    "            W1, W2, W3, W4, b1, b2, b3, b4)\n",
    "        MSE_FFNN_tunning[k] = MSE(FFNN_val_y[i], s4)\n",
    "        #print(MSE_FFNN_tunning[k])\n",
    "        \n",
    "    # report the index of minimum MSE\n",
    "    return np.argsort(MSE_FFNN_tunning)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "4bb688f9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US time consuming : 180.73s\n",
      "Canada time consuming : 189.37s\n",
      "Japan time consuming : 185.52s\n",
      "Hong_Kong time consuming : 193.53s\n",
      "Spain time consuming : 194.59s\n",
      "France time consuming : 183.08s\n",
      "Netherlands time consuming : 184.40s\n",
      "Brazil time consuming : 179.71s\n",
      "Mexico time consuming : 170.69s\n",
      "China time consuming : 169.42s\n",
      "Turkey time consuming : 177.63s\n",
      "Indonesia time consuming : 175.38s\n",
      "India time consuming : 176.43s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[15, 15, 15],\n",
       " [10, 10, 10],\n",
       " [15, 15, 15],\n",
       " [5, 5, 5],\n",
       " [15, 15, 15],\n",
       " [15, 15, 15],\n",
       " [25, 25, 25],\n",
       " [15, 15, 15],\n",
       " [5, 5, 5],\n",
       " [15, 15, 15],\n",
       " [25, 25, 25],\n",
       " [10, 10, 10],\n",
       " [25, 25, 25]]"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep the optimal structure for each region data\n",
    "FFNN_optimal_structure = []\n",
    "print('record of time consuming')\n",
    "\n",
    "for i in range(len(region_list)):\n",
    "    start = time.process_time()\n",
    "    index = FFNN_tunning(i, list_hyper)\n",
    "    end   = time.process_time()\n",
    "    print('{:<12} {:.2f} '.format(region_list[i]+':', (end - start)))\n",
    "    FFNN_optimal_structure.append(list_hyper[index])\n",
    "    \n",
    "FFNN_optimal_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f67bfdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually keep the results since the computational cost is too much\n",
    "FFNN_optimal_structure = \\\n",
    "[[15, 15, 15],\n",
    " [10, 10, 10],\n",
    " [15, 15, 15],\n",
    " [5, 5, 5],\n",
    " [15, 15, 15],\n",
    " [15, 15, 15],\n",
    " [25, 25, 25],\n",
    " [15, 15, 15],\n",
    " [5, 5, 5],\n",
    " [15, 15, 15],\n",
    " [25, 25, 25],\n",
    " [10, 10, 10],\n",
    " [25, 25, 25]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc86fff",
   "metadata": {},
   "source": [
    "# FFNN: model performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4124e11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the MSE on test set\n",
    "MSE_FFNN_testing  = []\n",
    "# keep the predicted values\n",
    "FFNN_predict_list = []\n",
    "\n",
    "for i in range(len(region_list)):\n",
    "    \n",
    "    # whole training set = train + val\n",
    "    FFNN_whole_train_x = np.hstack((FFNN_train_x[i],FFNN_val_x[i]))\n",
    "    FFNN_whole_train_y = np.hstack((FFNN_train_y[i],FFNN_val_y[i]))\n",
    "    \n",
    "    # fit model on training set\n",
    "    W1, W2, W3, W4, b1, b2, b3, b4, loss_bp, initial_loss = FFNN_MBSGD(\n",
    "        FFNN_optimal_structure[i],\n",
    "        FFNN_whole_train_x, \n",
    "        FFNN_whole_train_y)\n",
    "    # test performance on test set\n",
    "    a0, s1, a1, s2, a2, s3, a3, s4 = forward_prop(\n",
    "        FFNN_test_x[i], \n",
    "        W1, W2, W3, W4, b1, b2, b3, b4)\n",
    "    \n",
    "    MSE_FFNN_testing.append(MSE(FFNN_test_y[i], s4))\n",
    "    FFNN_predict_list.append(s4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7282d3c4",
   "metadata": {},
   "source": [
    "# ARDL: hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aedb063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test of 'ardl_select_order'\n",
    "test_y = array_list_y_ARDL_sta[1]\n",
    "test_x = array_list_x_ARDL_sta[1].T\n",
    "\n",
    "# start  = time.process_time()\n",
    "# sel_res = ardl_select_order(endog  = test_y, \n",
    "                            # maxlag = 2, \n",
    "                            # exog     = test_x,\n",
    "                            # maxorder = 2, \n",
    "                            # ic = \"aic\",\n",
    "                            # trend = \"c\")\n",
    "# end = time.process_time()\n",
    "# print(\"time consuming : %.2fs\"%(end - start))\n",
    "\n",
    "# !!! test showed that ardl_select_order has damaged function\n",
    "# !!! thus this should not be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e358cd0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19683, 59049)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create all possible dictionaries for parameters in ARDL\n",
    "dict_8 = [[k[0], dict(zip([0,1,2,3,4,5,6,7],k[1:]))]   for k in \\\n",
    "          [i for i in itertools.product([0,1,2],repeat = 9)]]\n",
    "\n",
    "dict_9 = [[k[0], dict(zip([0,1,2,3,4,5,6,7,8],k[1:]))] for k in \\\n",
    "          [i for i in itertools.product([0,1,2],repeat = 10)]]\n",
    "\n",
    "len(dict_8), len(dict_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "8f5856f9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 done\n",
      "1000 done\n",
      "2000 done\n",
      "3000 done\n",
      "4000 done\n",
      "5000 done\n",
      "6000 done\n",
      "7000 done\n",
      "8000 done\n",
      "9000 done\n",
      "10000 done\n",
      "11000 done\n",
      "12000 done\n",
      "13000 done\n",
      "14000 done\n",
      "15000 done\n",
      "16000 done\n",
      "17000 done\n",
      "18000 done\n",
      "19000 done\n",
      "time consuming : 639.25s\n",
      "0 done\n",
      "1000 done\n",
      "2000 done\n",
      "3000 done\n",
      "4000 done\n",
      "5000 done\n",
      "6000 done\n",
      "7000 done\n",
      "8000 done\n",
      "9000 done\n",
      "10000 done\n",
      "11000 done\n",
      "12000 done\n",
      "13000 done\n",
      "14000 done\n",
      "15000 done\n",
      "16000 done\n",
      "17000 done\n",
      "18000 done\n",
      "19000 done\n",
      "time consuming : 648.77s\n",
      "0 done\n",
      "1000 done\n",
      "2000 done\n",
      "3000 done\n",
      "4000 done\n",
      "5000 done\n",
      "6000 done\n",
      "7000 done\n",
      "8000 done\n",
      "9000 done\n",
      "10000 done\n",
      "11000 done\n",
      "12000 done\n",
      "13000 done\n",
      "14000 done\n",
      "15000 done\n",
      "16000 done\n",
      "17000 done\n",
      "18000 done\n",
      "19000 done\n",
      "time consuming : 633.26s\n",
      "0 done\n",
      "1000 done\n",
      "2000 done\n",
      "3000 done\n",
      "4000 done\n",
      "5000 done\n",
      "6000 done\n",
      "7000 done\n",
      "8000 done\n",
      "9000 done\n",
      "10000 done\n",
      "11000 done\n",
      "12000 done\n",
      "13000 done\n",
      "14000 done\n",
      "15000 done\n",
      "16000 done\n",
      "17000 done\n",
      "18000 done\n",
      "19000 done\n",
      "20000 done\n",
      "21000 done\n",
      "22000 done\n",
      "23000 done\n",
      "24000 done\n",
      "25000 done\n",
      "26000 done\n",
      "27000 done\n",
      "28000 done\n",
      "29000 done\n",
      "30000 done\n",
      "31000 done\n",
      "32000 done\n",
      "33000 done\n",
      "34000 done\n",
      "35000 done\n",
      "36000 done\n",
      "37000 done\n",
      "38000 done\n",
      "39000 done\n",
      "40000 done\n",
      "41000 done\n",
      "42000 done\n",
      "43000 done\n",
      "44000 done\n",
      "45000 done\n",
      "46000 done\n",
      "47000 done\n",
      "48000 done\n",
      "49000 done\n",
      "50000 done\n",
      "51000 done\n",
      "52000 done\n",
      "53000 done\n",
      "54000 done\n",
      "55000 done\n",
      "56000 done\n",
      "57000 done\n",
      "58000 done\n",
      "59000 done\n",
      "time consuming : 2016.83s\n",
      "0 done\n",
      "1000 done\n",
      "2000 done\n",
      "3000 done\n",
      "4000 done\n",
      "5000 done\n",
      "6000 done\n",
      "7000 done\n",
      "8000 done\n",
      "9000 done\n",
      "10000 done\n",
      "11000 done\n",
      "12000 done\n",
      "13000 done\n",
      "14000 done\n",
      "15000 done\n",
      "16000 done\n",
      "17000 done\n",
      "18000 done\n",
      "19000 done\n",
      "20000 done\n",
      "21000 done\n",
      "22000 done\n",
      "23000 done\n",
      "24000 done\n",
      "25000 done\n",
      "26000 done\n",
      "27000 done\n",
      "28000 done\n",
      "29000 done\n",
      "30000 done\n",
      "31000 done\n",
      "32000 done\n",
      "33000 done\n",
      "34000 done\n",
      "35000 done\n",
      "36000 done\n",
      "37000 done\n",
      "38000 done\n",
      "39000 done\n",
      "40000 done\n",
      "41000 done\n",
      "42000 done\n",
      "43000 done\n",
      "44000 done\n",
      "45000 done\n",
      "46000 done\n",
      "47000 done\n",
      "48000 done\n",
      "49000 done\n",
      "50000 done\n",
      "51000 done\n",
      "52000 done\n",
      "53000 done\n",
      "54000 done\n",
      "55000 done\n",
      "56000 done\n",
      "57000 done\n",
      "58000 done\n",
      "59000 done\n",
      "time consuming : 2002.39s\n",
      "0 done\n",
      "1000 done\n",
      "2000 done\n",
      "3000 done\n",
      "4000 done\n",
      "5000 done\n",
      "6000 done\n",
      "7000 done\n",
      "8000 done\n",
      "9000 done\n",
      "10000 done\n",
      "11000 done\n",
      "12000 done\n",
      "13000 done\n",
      "14000 done\n",
      "15000 done\n",
      "16000 done\n",
      "17000 done\n",
      "18000 done\n",
      "19000 done\n",
      "20000 done\n",
      "21000 done\n",
      "22000 done\n",
      "23000 done\n",
      "24000 done\n",
      "25000 done\n",
      "26000 done\n",
      "27000 done\n",
      "28000 done\n",
      "29000 done\n",
      "30000 done\n",
      "31000 done\n",
      "32000 done\n",
      "33000 done\n",
      "34000 done\n",
      "35000 done\n",
      "36000 done\n",
      "37000 done\n",
      "38000 done\n",
      "39000 done\n",
      "40000 done\n",
      "41000 done\n",
      "42000 done\n",
      "43000 done\n",
      "44000 done\n",
      "45000 done\n",
      "46000 done\n",
      "47000 done\n",
      "48000 done\n",
      "49000 done\n",
      "50000 done\n",
      "51000 done\n",
      "52000 done\n",
      "53000 done\n",
      "54000 done\n",
      "55000 done\n",
      "56000 done\n",
      "57000 done\n",
      "58000 done\n",
      "59000 done\n",
      "time consuming : 1983.79s\n",
      "0 done\n",
      "1000 done\n",
      "2000 done\n",
      "3000 done\n",
      "4000 done\n",
      "5000 done\n",
      "6000 done\n",
      "7000 done\n",
      "8000 done\n",
      "9000 done\n",
      "10000 done\n",
      "11000 done\n",
      "12000 done\n",
      "13000 done\n",
      "14000 done\n",
      "15000 done\n",
      "16000 done\n",
      "17000 done\n",
      "18000 done\n",
      "19000 done\n",
      "time consuming : 630.48s\n",
      "0 done\n",
      "1000 done\n",
      "2000 done\n",
      "3000 done\n",
      "4000 done\n",
      "5000 done\n",
      "6000 done\n",
      "7000 done\n",
      "8000 done\n",
      "9000 done\n",
      "10000 done\n",
      "11000 done\n",
      "12000 done\n",
      "13000 done\n",
      "14000 done\n",
      "15000 done\n",
      "16000 done\n",
      "17000 done\n",
      "18000 done\n",
      "19000 done\n",
      "time consuming : 629.51s\n",
      "0 done\n",
      "1000 done\n",
      "2000 done\n",
      "3000 done\n",
      "4000 done\n",
      "5000 done\n",
      "6000 done\n",
      "7000 done\n",
      "8000 done\n",
      "9000 done\n",
      "10000 done\n",
      "11000 done\n",
      "12000 done\n",
      "13000 done\n",
      "14000 done\n",
      "15000 done\n",
      "16000 done\n",
      "17000 done\n",
      "18000 done\n",
      "19000 done\n",
      "20000 done\n",
      "21000 done\n",
      "22000 done\n",
      "23000 done\n",
      "24000 done\n",
      "25000 done\n",
      "26000 done\n",
      "27000 done\n",
      "28000 done\n",
      "29000 done\n",
      "30000 done\n",
      "31000 done\n",
      "32000 done\n",
      "33000 done\n",
      "34000 done\n",
      "35000 done\n",
      "36000 done\n",
      "37000 done\n",
      "38000 done\n",
      "39000 done\n",
      "40000 done\n",
      "41000 done\n",
      "42000 done\n",
      "43000 done\n",
      "44000 done\n",
      "45000 done\n",
      "46000 done\n",
      "47000 done\n",
      "48000 done\n",
      "49000 done\n",
      "50000 done\n",
      "51000 done\n",
      "52000 done\n",
      "53000 done\n",
      "54000 done\n",
      "55000 done\n",
      "56000 done\n",
      "57000 done\n",
      "58000 done\n",
      "59000 done\n",
      "time consuming : 2005.27s\n",
      "0 done\n",
      "1000 done\n",
      "2000 done\n",
      "3000 done\n",
      "4000 done\n",
      "5000 done\n",
      "6000 done\n",
      "7000 done\n",
      "8000 done\n",
      "9000 done\n",
      "10000 done\n",
      "11000 done\n",
      "12000 done\n",
      "13000 done\n",
      "14000 done\n",
      "15000 done\n",
      "16000 done\n",
      "17000 done\n",
      "18000 done\n",
      "19000 done\n",
      "time consuming : 623.38s\n",
      "0 done\n",
      "1000 done\n",
      "2000 done\n",
      "3000 done\n",
      "4000 done\n",
      "5000 done\n",
      "6000 done\n",
      "7000 done\n",
      "8000 done\n",
      "9000 done\n",
      "10000 done\n",
      "11000 done\n",
      "12000 done\n",
      "13000 done\n",
      "14000 done\n",
      "15000 done\n",
      "16000 done\n",
      "17000 done\n",
      "18000 done\n",
      "19000 done\n",
      "20000 done\n",
      "21000 done\n",
      "22000 done\n",
      "23000 done\n",
      "24000 done\n",
      "25000 done\n",
      "26000 done\n",
      "27000 done\n",
      "28000 done\n",
      "29000 done\n",
      "30000 done\n",
      "31000 done\n",
      "32000 done\n",
      "33000 done\n",
      "34000 done\n",
      "35000 done\n",
      "36000 done\n",
      "37000 done\n",
      "38000 done\n",
      "39000 done\n",
      "40000 done\n",
      "41000 done\n",
      "42000 done\n",
      "43000 done\n",
      "44000 done\n",
      "45000 done\n",
      "46000 done\n",
      "47000 done\n",
      "48000 done\n",
      "49000 done\n",
      "50000 done\n",
      "51000 done\n",
      "52000 done\n",
      "53000 done\n",
      "54000 done\n",
      "55000 done\n",
      "56000 done\n",
      "57000 done\n",
      "58000 done\n",
      "59000 done\n",
      "time consuming : 1998.19s\n",
      "0 done\n",
      "1000 done\n",
      "2000 done\n",
      "3000 done\n",
      "4000 done\n",
      "5000 done\n",
      "6000 done\n",
      "7000 done\n",
      "8000 done\n",
      "9000 done\n",
      "10000 done\n",
      "11000 done\n",
      "12000 done\n",
      "13000 done\n",
      "14000 done\n",
      "15000 done\n",
      "16000 done\n",
      "17000 done\n",
      "18000 done\n",
      "19000 done\n",
      "20000 done\n",
      "21000 done\n",
      "22000 done\n",
      "23000 done\n",
      "24000 done\n",
      "25000 done\n",
      "26000 done\n",
      "27000 done\n",
      "28000 done\n",
      "29000 done\n",
      "30000 done\n",
      "31000 done\n",
      "32000 done\n",
      "33000 done\n",
      "34000 done\n",
      "35000 done\n",
      "36000 done\n",
      "37000 done\n",
      "38000 done\n",
      "39000 done\n",
      "40000 done\n",
      "41000 done\n",
      "42000 done\n",
      "43000 done\n",
      "44000 done\n",
      "45000 done\n",
      "46000 done\n",
      "47000 done\n",
      "48000 done\n",
      "49000 done\n",
      "50000 done\n",
      "51000 done\n",
      "52000 done\n",
      "53000 done\n",
      "54000 done\n",
      "55000 done\n",
      "56000 done\n",
      "57000 done\n",
      "58000 done\n",
      "59000 done\n",
      "time consuming : 1979.59s\n",
      "0 done\n",
      "1000 done\n",
      "2000 done\n",
      "3000 done\n",
      "4000 done\n",
      "5000 done\n",
      "6000 done\n",
      "7000 done\n",
      "8000 done\n",
      "9000 done\n",
      "10000 done\n",
      "11000 done\n",
      "12000 done\n",
      "13000 done\n",
      "14000 done\n",
      "15000 done\n",
      "16000 done\n",
      "17000 done\n",
      "18000 done\n",
      "19000 done\n",
      "time consuming : 610.72s\n",
      "[array([0, {0: 1, 1: 2, 2: 0, 3: 0, 4: 0, 5: 0, 6: 1, 7: 1}], dtype=object), array([0, {0: 1, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0}], dtype=object), array([1, {0: 2, 1: 0, 2: 0, 3: 1, 4: 2, 5: 1, 6: 0, 7: 2}], dtype=object), array([1, {0: 0, 1: 2, 2: 0, 3: 1, 4: 1, 5: 0, 6: 0, 7: 0, 8: 1}],\n",
      "      dtype=object), array([0, {0: 0, 1: 0, 2: 2, 3: 0, 4: 2, 5: 0, 6: 0, 7: 2, 8: 0}],\n",
      "      dtype=object), array([0, {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}],\n",
      "      dtype=object), array([0, {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0}], dtype=object), array([0, {0: 0, 1: 0, 2: 2, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0}], dtype=object), array([0, {0: 1, 1: 1, 2: 0, 3: 2, 4: 0, 5: 0, 6: 1, 7: 0, 8: 0}],\n",
      "      dtype=object), array([0, {0: 2, 1: 2, 2: 2, 3: 1, 4: 2, 5: 0, 6: 0, 7: 0}], dtype=object), array([0, {0: 0, 1: 0, 2: 1, 3: 0, 4: 1, 5: 0, 6: 0, 7: 0, 8: 1}],\n",
      "      dtype=object), array([2, {0: 1, 1: 2, 2: 1, 3: 2, 4: 0, 5: 1, 6: 0, 7: 1, 8: 0}],\n",
      "      dtype=object), array([0, {0: 0, 1: 1, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 2}], dtype=object)]\n"
     ]
    }
   ],
   "source": [
    "# determine the optimal parameters of each\n",
    "ARDL_optimal_params = []\n",
    "\n",
    "for i in range(len(region_list)):\n",
    "    # data\n",
    "    ARDL_array_y = ARDL_train_y[i]\n",
    "    ARDL_array_x = ARDL_train_x[i].T\n",
    "    # pin down the number of PCs and dict\n",
    "    count_PC  = ARDL_array_x.shape[1]\n",
    "    ARDL_dict = [dict_8,dict_9][count_PC-8]\n",
    "    # create the list of AICs\n",
    "    AICs = np.empty(len(ARDL_dict))\n",
    "    # track the time\n",
    "    start  = time.process_time()\n",
    "    for i in range(len(AICs)):\n",
    "        AICs[i] = ARDL(endog = ARDL_array_y,\n",
    "                       lags  = ARDL_dict[i][0],\n",
    "                       exog  = ARDL_array_x,\n",
    "                       order = ARDL_dict[i][1],\n",
    "                       trend = \"c\").fit().aic\n",
    "        if i%1000 == 0:\n",
    "            print(str(i)+\" done\")\n",
    "    # track the time\n",
    "    end = time.process_time()\n",
    "    # print the time\n",
    "    print(\"time consuming : %.2fs\"%(end - start))\n",
    "    # record the index of the smallest AIC\n",
    "    index_AIC = np.argsort(AICs)[0]\n",
    "    # record the parameters\n",
    "    ARDL_optimal_params.append(np.array(ARDL_dict[index_AIC]))\n",
    "    \n",
    "print(ARDL_optimal_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "625939be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually keep the results since the computational cost is too much\n",
    "ARDL_optimal_params = [np.array([0, {0: 1, 1: 2, 2: 0, 3: 0, 4: 0, 5: 0, 6: 1, 7: 1}], dtype=object), \n",
    "                       np.array([0, {0: 1, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0}], dtype=object), \n",
    "                       np.array([1, {0: 2, 1: 0, 2: 0, 3: 1, 4: 2, 5: 1, 6: 0, 7: 2}], dtype=object), \n",
    "                       np.array([1, {0: 0, 1: 2, 2: 0, 3: 1, 4: 1, 5: 0, 6: 0, 7: 0, 8: 1}], dtype=object), \n",
    "                       np.array([0, {0: 0, 1: 0, 2: 2, 3: 0, 4: 2, 5: 0, 6: 0, 7: 2, 8: 0}], dtype=object), \n",
    "                       np.array([0, {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0}], dtype=object), \n",
    "                       np.array([0, {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0}], dtype=object), \n",
    "                       np.array([0, {0: 0, 1: 0, 2: 2, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0}], dtype=object), \n",
    "                       np.array([0, {0: 1, 1: 1, 2: 0, 3: 2, 4: 0, 5: 0, 6: 1, 7: 0, 8: 0}], dtype=object), \n",
    "                       np.array([0, {0: 2, 1: 2, 2: 2, 3: 1, 4: 2, 5: 0, 6: 0, 7: 0}], dtype=object), \n",
    "                       np.array([0, {0: 0, 1: 0, 2: 1, 3: 0, 4: 1, 5: 0, 6: 0, 7: 0, 8: 1}], dtype=object), \n",
    "                       np.array([2, {0: 1, 1: 2, 2: 1, 3: 2, 4: 0, 5: 1, 6: 0, 7: 1, 8: 0}], dtype=object), \n",
    "                       np.array([0, {0: 0, 1: 1, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 2}], dtype=object)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6a85fe",
   "metadata": {},
   "source": [
    "# ARDL: model performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c7cc9138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep the MSE on test set\n",
    "MSE_ARDL_testing  = []\n",
    "# keep the predicted values\n",
    "ARDL_predict_list = []\n",
    "\n",
    "for i in range(len(region_list)):\n",
    "    \n",
    "    # whole training set = train + val\n",
    "    ARDL_whole_train_x = np.hstack((ARDL_train_x[i],ARDL_val_x[i])).T\n",
    "    ARDL_whole_train_y = np.hstack((ARDL_train_y[i],ARDL_val_y[i]))\n",
    "    \n",
    "    ARDL_whole_x = np.hstack((ARDL_train_x[i],ARDL_val_x[i],ARDL_test_x[i])).T\n",
    "    \n",
    "    # fit model on training set\n",
    "    ARDL_model = ARDL(endog = ARDL_whole_train_y,\n",
    "                      lags  = ARDL_optimal_params[i][0], \n",
    "                      exog  = ARDL_whole_train_x,\n",
    "                      order = ARDL_optimal_params[i][1],\n",
    "                      trend = \"c\").fit()\n",
    "    # test performance on test set\n",
    "    ARDL_predict = ARDL_model.predict(start = len(ARDL_whole_train_y), \n",
    "                                      end   = len(ARDL_whole_x) - 1,\n",
    "                                      exog_oos = ARDL_whole_x)\n",
    "    \n",
    "    MSE_ARDL_testing.append(MSE(ARDL_test_y[i], ARDL_predict))\n",
    "    ARDL_predict_list.append(ARDL_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "00156c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_comparison = pd.DataFrame({'Region'  : region_list,\n",
    "                               'MSE_FFNN': MSE_FFNN_testing,\n",
    "                               'MSE_ARDL': MSE_ARDL_testing},\n",
    "                              columns=['Region','MSE_FFNN','MSE_ARDL'])\n",
    "MSE_comparison['ARDL-FFNN'] = MSE_comparison['MSE_ARDL'] - MSE_comparison['MSE_FFNN']\n",
    "# MSE_comparison.round(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b4c0f8",
   "metadata": {},
   "source": [
    "# Summary of model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "87d4a458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE(true_array, predicted_array):\n",
    "    error = true_array - predicted_array\n",
    "    square_error = error**2\n",
    "    mean_square_error = np.mean(square_error)\n",
    "    root_mean_square_error = mean_square_error**(1/2)\n",
    "    return root_mean_square_error\n",
    "\n",
    "def MAPE(true_array, predicted_array):\n",
    "    abs_error = np.abs(true_array - predicted_array)\n",
    "    # delete the case of 0 at denominater\n",
    "    abs_error  = abs_error [true_array!=0]\n",
    "    true_array = true_array[true_array!=0]\n",
    "    perc_abs_error = abs_error / true_array\n",
    "    mean_perc_abs_error = np.mean(perc_abs_error)\n",
    "    return mean_perc_abs_error\n",
    "\n",
    "def RMSE_single(x):\n",
    "    square = x**2\n",
    "    mean_square = np.mean(square)\n",
    "    root_mean_square = mean_square**(1/2) \n",
    "    return root_mean_square\n",
    "\n",
    "def UofTheil(true_array, predicted_array):\n",
    "    numerator_1 = RMSE(true_array, predicted_array)\n",
    "    denominator_1 = RMSE_single(true_array) + RMSE_single(predicted_array)\n",
    "    return numerator_1 / denominator_1\n",
    "\n",
    "def SSE(true_array, predicted_array):\n",
    "    square_error = (true_array - predicted_array)**2\n",
    "    sum_square_error = np.sum(square_error)\n",
    "    return sum_square_error\n",
    "\n",
    "# a small typo in the paper!!! correct here\n",
    "def ARV(true_array, predicted_array):\n",
    "    numerator_2 = SSE(true_array, predicted_array)\n",
    "    denominator_2 = SSE(np.mean(true_array), true_array)\n",
    "    return numerator_2 / denominator_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "d09d1746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set_true_values\n",
    "# FFNN_test_y \n",
    "# ARDL_test_y \n",
    "\n",
    "# test_set_predicted_values\n",
    "# FFNN_predict_list\n",
    "# ARDL_predict_list\n",
    "\n",
    "# create np.array to store the metrics\n",
    "evluation_metrics = np.empty([len(region_list)*3,4])\n",
    "\n",
    "for i in range(len(region_list)):\n",
    "    \n",
    "    ## FFNN get values\n",
    "    true_array      = np.array([FFNN_test_y[i].T])\n",
    "    predicted_array = FFNN_predict_list[i]\n",
    "    \n",
    "    # calculate evaluation metrics\n",
    "    RMSE_i = RMSE(true_array, predicted_array)\n",
    "    MAPE_i = MAPE(true_array, predicted_array)\n",
    "    UofTheil_i = UofTheil(true_array, predicted_array)\n",
    "    ARV_i  = ARV(true_array, predicted_array)\n",
    "    \n",
    "    # record the values\n",
    "    evluation_metrics[3*i] = np.array([RMSE_i,MAPE_i,UofTheil_i,ARV_i])\n",
    "    \n",
    "    ## ARDL get values\n",
    "    true_array      = ARDL_test_y[i]\n",
    "    predicted_array = ARDL_predict_list[i]\n",
    "    \n",
    "    # calculate evaluation metrics\n",
    "    RMSE_i = RMSE(true_array, predicted_array)\n",
    "    MAPE_i = MAPE(true_array, predicted_array)\n",
    "    UofTheil_i = UofTheil(true_array, predicted_array)\n",
    "    ARV_i  = ARV(true_array, predicted_array)\n",
    "    \n",
    "    # record the values\n",
    "    evluation_metrics[3*i+1] = np.array([RMSE_i,MAPE_i,UofTheil_i,ARV_i])\n",
    "    evluation_metrics[3*i+2] = evluation_metrics[3*i+1] - evluation_metrics[3*i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "99e54dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>UofTheil</th>\n",
       "      <th>ARV</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">US</th>\n",
       "      <th>FFNN</th>\n",
       "      <td>0.08974</td>\n",
       "      <td>0.11063</td>\n",
       "      <td>0.07594</td>\n",
       "      <td>1.00177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARDL</th>\n",
       "      <td>0.08971</td>\n",
       "      <td>0.11047</td>\n",
       "      <td>0.07586</td>\n",
       "      <td>1.00111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compare</th>\n",
       "      <td>-0.00003</td>\n",
       "      <td>-0.00016</td>\n",
       "      <td>-0.00007</td>\n",
       "      <td>-0.00066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Canada</th>\n",
       "      <th>FFNN</th>\n",
       "      <td>0.07439</td>\n",
       "      <td>0.11564</td>\n",
       "      <td>0.06529</td>\n",
       "      <td>0.99612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARDL</th>\n",
       "      <td>0.07459</td>\n",
       "      <td>0.11551</td>\n",
       "      <td>0.06543</td>\n",
       "      <td>1.00163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compare</th>\n",
       "      <td>0.00021</td>\n",
       "      <td>-0.00013</td>\n",
       "      <td>0.00014</td>\n",
       "      <td>0.00550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Japan</th>\n",
       "      <th>FFNN</th>\n",
       "      <td>0.10331</td>\n",
       "      <td>0.19568</td>\n",
       "      <td>0.10798</td>\n",
       "      <td>1.00151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARDL</th>\n",
       "      <td>0.10355</td>\n",
       "      <td>0.19880</td>\n",
       "      <td>0.10801</td>\n",
       "      <td>1.00604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compare</th>\n",
       "      <td>0.00023</td>\n",
       "      <td>0.00312</td>\n",
       "      <td>0.00004</td>\n",
       "      <td>0.00454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Hong_Kong</th>\n",
       "      <th>FFNN</th>\n",
       "      <td>0.11978</td>\n",
       "      <td>0.17656</td>\n",
       "      <td>0.09870</td>\n",
       "      <td>0.99979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARDL</th>\n",
       "      <td>0.12487</td>\n",
       "      <td>0.18455</td>\n",
       "      <td>0.10296</td>\n",
       "      <td>1.08656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compare</th>\n",
       "      <td>0.00509</td>\n",
       "      <td>0.00799</td>\n",
       "      <td>0.00426</td>\n",
       "      <td>0.08677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Spain</th>\n",
       "      <th>FFNN</th>\n",
       "      <td>0.05395</td>\n",
       "      <td>0.12881</td>\n",
       "      <td>0.05890</td>\n",
       "      <td>1.00144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARDL</th>\n",
       "      <td>0.05529</td>\n",
       "      <td>0.13093</td>\n",
       "      <td>0.06051</td>\n",
       "      <td>1.05197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compare</th>\n",
       "      <td>0.00134</td>\n",
       "      <td>0.00212</td>\n",
       "      <td>0.00161</td>\n",
       "      <td>0.05054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">France</th>\n",
       "      <th>FFNN</th>\n",
       "      <td>0.08552</td>\n",
       "      <td>0.10161</td>\n",
       "      <td>0.07075</td>\n",
       "      <td>0.99983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARDL</th>\n",
       "      <td>0.08570</td>\n",
       "      <td>0.10182</td>\n",
       "      <td>0.07109</td>\n",
       "      <td>1.00416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compare</th>\n",
       "      <td>0.00019</td>\n",
       "      <td>0.00021</td>\n",
       "      <td>0.00034</td>\n",
       "      <td>0.00434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Netherlands</th>\n",
       "      <th>FFNN</th>\n",
       "      <td>0.08492</td>\n",
       "      <td>0.10795</td>\n",
       "      <td>0.07288</td>\n",
       "      <td>1.00009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARDL</th>\n",
       "      <td>0.08501</td>\n",
       "      <td>0.10785</td>\n",
       "      <td>0.07304</td>\n",
       "      <td>1.00221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compare</th>\n",
       "      <td>0.00009</td>\n",
       "      <td>-0.00010</td>\n",
       "      <td>0.00016</td>\n",
       "      <td>0.00212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Brazil</th>\n",
       "      <th>FFNN</th>\n",
       "      <td>0.08983</td>\n",
       "      <td>0.18547</td>\n",
       "      <td>0.08014</td>\n",
       "      <td>1.00057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARDL</th>\n",
       "      <td>0.08993</td>\n",
       "      <td>0.18543</td>\n",
       "      <td>0.08039</td>\n",
       "      <td>1.00276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compare</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>-0.00004</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>0.00219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Mexico</th>\n",
       "      <th>FFNN</th>\n",
       "      <td>0.12266</td>\n",
       "      <td>0.24884</td>\n",
       "      <td>0.09409</td>\n",
       "      <td>1.00132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARDL</th>\n",
       "      <td>0.12386</td>\n",
       "      <td>0.24758</td>\n",
       "      <td>0.09549</td>\n",
       "      <td>1.02097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compare</th>\n",
       "      <td>0.00120</td>\n",
       "      <td>-0.00126</td>\n",
       "      <td>0.00139</td>\n",
       "      <td>0.01965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">China</th>\n",
       "      <th>FFNN</th>\n",
       "      <td>0.06150</td>\n",
       "      <td>0.07430</td>\n",
       "      <td>0.05048</td>\n",
       "      <td>1.00197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARDL</th>\n",
       "      <td>0.11021</td>\n",
       "      <td>0.08453</td>\n",
       "      <td>0.08953</td>\n",
       "      <td>3.21724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compare</th>\n",
       "      <td>0.04870</td>\n",
       "      <td>0.01023</td>\n",
       "      <td>0.03905</td>\n",
       "      <td>2.21527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Turkey</th>\n",
       "      <th>FFNN</th>\n",
       "      <td>0.04852</td>\n",
       "      <td>0.00685</td>\n",
       "      <td>0.02531</td>\n",
       "      <td>1.00163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARDL</th>\n",
       "      <td>0.04852</td>\n",
       "      <td>0.00704</td>\n",
       "      <td>0.02532</td>\n",
       "      <td>1.00191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compare</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.00018</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Indonesia</th>\n",
       "      <th>FFNN</th>\n",
       "      <td>0.09608</td>\n",
       "      <td>0.21698</td>\n",
       "      <td>0.09739</td>\n",
       "      <td>1.00650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARDL</th>\n",
       "      <td>0.10808</td>\n",
       "      <td>0.22404</td>\n",
       "      <td>0.11017</td>\n",
       "      <td>1.27358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compare</th>\n",
       "      <td>0.01200</td>\n",
       "      <td>0.00706</td>\n",
       "      <td>0.01278</td>\n",
       "      <td>0.26708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">India</th>\n",
       "      <th>FFNN</th>\n",
       "      <td>0.08882</td>\n",
       "      <td>0.10519</td>\n",
       "      <td>0.07168</td>\n",
       "      <td>1.00093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARDL</th>\n",
       "      <td>0.08868</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.07156</td>\n",
       "      <td>0.99779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compare</th>\n",
       "      <td>-0.00014</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>-0.00012</td>\n",
       "      <td>-0.00314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        RMSE     MAPE  UofTheil      ARV\n",
       "US          FFNN     0.08974  0.11063   0.07594  1.00177\n",
       "            ARDL     0.08971  0.11047   0.07586  1.00111\n",
       "            compare -0.00003 -0.00016  -0.00007 -0.00066\n",
       "Canada      FFNN     0.07439  0.11564   0.06529  0.99612\n",
       "            ARDL     0.07459  0.11551   0.06543  1.00163\n",
       "            compare  0.00021 -0.00013   0.00014  0.00550\n",
       "Japan       FFNN     0.10331  0.19568   0.10798  1.00151\n",
       "            ARDL     0.10355  0.19880   0.10801  1.00604\n",
       "            compare  0.00023  0.00312   0.00004  0.00454\n",
       "Hong_Kong   FFNN     0.11978  0.17656   0.09870  0.99979\n",
       "            ARDL     0.12487  0.18455   0.10296  1.08656\n",
       "            compare  0.00509  0.00799   0.00426  0.08677\n",
       "Spain       FFNN     0.05395  0.12881   0.05890  1.00144\n",
       "            ARDL     0.05529  0.13093   0.06051  1.05197\n",
       "            compare  0.00134  0.00212   0.00161  0.05054\n",
       "France      FFNN     0.08552  0.10161   0.07075  0.99983\n",
       "            ARDL     0.08570  0.10182   0.07109  1.00416\n",
       "            compare  0.00019  0.00021   0.00034  0.00434\n",
       "Netherlands FFNN     0.08492  0.10795   0.07288  1.00009\n",
       "            ARDL     0.08501  0.10785   0.07304  1.00221\n",
       "            compare  0.00009 -0.00010   0.00016  0.00212\n",
       "Brazil      FFNN     0.08983  0.18547   0.08014  1.00057\n",
       "            ARDL     0.08993  0.18543   0.08039  1.00276\n",
       "            compare  0.00010 -0.00004   0.00025  0.00219\n",
       "Mexico      FFNN     0.12266  0.24884   0.09409  1.00132\n",
       "            ARDL     0.12386  0.24758   0.09549  1.02097\n",
       "            compare  0.00120 -0.00126   0.00139  0.01965\n",
       "China       FFNN     0.06150  0.07430   0.05048  1.00197\n",
       "            ARDL     0.11021  0.08453   0.08953  3.21724\n",
       "            compare  0.04870  0.01023   0.03905  2.21527\n",
       "Turkey      FFNN     0.04852  0.00685   0.02531  1.00163\n",
       "            ARDL     0.04852  0.00704   0.02532  1.00191\n",
       "            compare  0.00001  0.00018   0.00000  0.00029\n",
       "Indonesia   FFNN     0.09608  0.21698   0.09739  1.00650\n",
       "            ARDL     0.10808  0.22404   0.11017  1.27358\n",
       "            compare  0.01200  0.00706   0.01278  0.26708\n",
       "India       FFNN     0.08882  0.10519   0.07168  1.00093\n",
       "            ARDL     0.08868  0.10520   0.07156  0.99779\n",
       "            compare -0.00014  0.00001  -0.00012 -0.00314"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evaluation = pd.DataFrame(evluation_metrics,columns=['RMSE','MAPE','UofTheil','ARV'],\n",
    "                             index=[[i for i in region_list for _ in range(3)],\n",
    "                                    ['FFNN','ARDL','compare']*len(region_list)])\n",
    "df_evaluation.to_excel('evaluation_0403.xlsx')\n",
    "df_evaluation.round(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4b535e",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e9acee",
   "metadata": {},
   "source": [
    "## data extraction at the first semester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c55cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_datareader.data as pdr\n",
    "\n",
    "import datetime   \n",
    "import fix_yahoo_finance as yf   \n",
    "  \n",
    "start = datetime.datetime(2009,  1,  1)   \n",
    "end   = datetime.datetime(2021,  9,  1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a1aeb5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_dict = {'US':'^GSPC',        'Canada':'^GSPTSE',  'Japan':'^N225',     'Hong Kong':'^HSI',\n",
    "               'Spain':'^OMX',      'France':'^FCHI',    'Netherlands':'^AEX','Spain':'^IBEX',\n",
    "               # advanced\n",
    "               'Brazil':'^BVSP',    'Mexico':'^MXX',     'China':'000001.SS', 'Turkey':'XU100.IS',\n",
    "               'Indonesia':'^JKII', 'India':'^BSESN'}\n",
    "               # emerging\n",
    "\n",
    "data_list   = [i for i in range(14)]\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e5bc84b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "US ^GSPC 2008-12-31 2021-09-01\n",
      "Canada ^GSPTSE 2008-12-31 2021-09-01\n",
      "Japan ^N225 2009-01-05 2021-09-01\n",
      "Hong Kong ^HSI 2009-01-02 2021-09-01\n",
      "Spain ^IBEX 2009-01-02 2021-09-01\n",
      "France ^FCHI 2008-12-31 2021-09-01\n",
      "Netherlands ^AEX 2008-12-31 2021-09-01\n",
      "Brazil ^BVSP 2009-01-02 2021-09-01\n",
      "Mexico ^MXX 2008-12-31 2021-09-01\n",
      "China 000001.SS 2009-01-05 2021-09-01\n",
      "Turkey XU100.IS 2008-12-31 2021-09-01\n",
      "Indonesia ^JKII 2009-01-05 2021-09-01\n",
      "India ^BSESN 2009-01-02 2021-09-01\n"
     ]
    }
   ],
   "source": [
    "for con, sym in symbol_dict.items():\n",
    "    data_list[i] = pd.DataFrame(pdr.get_data_yahoo(sym,start,end))\n",
    "    i = i + 1\n",
    "    #print(\"country: {}, symbol: {}, first record: {}, last record: {}\".format(\n",
    "    #con, sym, con.index[0].date(), con.index[-1].date()))\n",
    "    print(con, sym, pdr.get_data_yahoo(sym,start,end).index[0].date(), pdr.get_data_yahoo(sym,start,end).index[-1].date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "549ea2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_US          = data_list[0]\n",
    "df_Canada      = data_list[1]\n",
    "df_Japan       = data_list[2]\n",
    "df_Hong_Kong   = data_list[3]\n",
    "df_Spain       = data_list[4]\n",
    "df_France      = data_list[5]\n",
    "df_Netherlands = data_list[6]\n",
    "df_Brazil      = data_list[7]\n",
    "df_Mexico      = data_list[8]\n",
    "df_China       = data_list[9]\n",
    "df_Turkey      = data_list[10]\n",
    "df_Indonesia   = data_list[11]\n",
    "df_India       = data_list[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c8cea17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_US.to_csv('./US.csv')\n",
    "df_Canada.to_csv('./Canada.csv')\n",
    "df_Japan.to_csv('./Japan.csv')\n",
    "df_Hong_Kong.to_csv('./Hong_Kong.csv')\n",
    "df_Spain.to_csv('./Spain.csv')\n",
    "df_France.to_csv('./France.csv')\n",
    "df_Netherlands.to_csv('./Netherlands.csv')\n",
    "df_Brazil.to_csv('./Brazil.csv')\n",
    "df_Mexico.to_csv('./Mexico.csv')\n",
    "df_China.to_csv('./China.csv')\n",
    "df_Turkey.to_csv('./Turkey.csv')\n",
    "df_Indonesia.to_csv('./Indonesia.csv')\n",
    "df_India.to_csv('./India.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d7c7cb",
   "metadata": {},
   "source": [
    "## generate 20 technical indicators (dirty work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d6ae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10\n",
    "k = 15\n",
    "\n",
    "# 1.SMAt(n)\n",
    "for region in region_list: \n",
    "    df_full.loc[df_full.Region == region,'SMA'] = \\\n",
    "    df_full[df_full.Region == region]['Close'].rolling(window = n).mean()\n",
    "\n",
    "# 2.EMAt(n)\n",
    "alpha_1 = 2/(n+1)\n",
    "for region in region_list: \n",
    "    df_full.loc[df_full.Region == region,'EMA_10'] = \\\n",
    "    df_full.loc[df_full.Region == region,'Close'].ewm(alpha=alpha_1,adjust=False).mean()\n",
    "\n",
    "alpha_2 = 2/(k+1)\n",
    "for region in region_list: \n",
    "    df_full.loc[df_full.Region == region,'EMA_15'] = \\\n",
    "    df_full.loc[df_full.Region == region,'Close'].ewm(alpha=alpha_2,adjust=False).mean()\n",
    "    \n",
    "# 3.MACDt(n,k)\n",
    "for region in region_list: \n",
    "    df_full.loc[df_full.Region == region,'MACD'] = \\\n",
    "    df_full.loc[df_full.Region == region,'EMA_10'] - \\\n",
    "    df_full.loc[df_full.Region == region,'EMA_15']\n",
    "\n",
    "# 4.ATRt(n)\n",
    "# TR\n",
    "for region in region_list: \n",
    "    df_full.loc[df_full.Region == region, 'TR'] = \\\n",
    "    pd.concat([(df_full.loc[df_full.Region == region]['High'] - df_full.loc[df_full.Region == region]['Low']),\n",
    "                abs(df_full.loc[df_full.Region == region]['High'] - df_full.loc[df_full.Region == region]['Close'].shift(+1)),\n",
    "                abs(df_full.loc[df_full.Region == region]['Low']  - df_full.loc[df_full.Region == region]['Close'].shift(+1))], \\\n",
    "              axis = 1).max(axis=1)\n",
    "    \n",
    "df_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff7011f",
   "metadata": {},
   "source": [
    "## code of MBSGD with plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "60d7959b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f852672b460>]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# volatility is pretty HIGH...\n",
    "# but it may be because PSO has find a good solution\n",
    "\n",
    "# optimze FFNN using MBSGD\n",
    "\n",
    "# test using US dataset\n",
    "array_x = FFNN_train_x[0]\n",
    "array_y = FFNN_train_y[0]\n",
    "\n",
    "# 1.MBSGD parameters setting\n",
    "learn_rate      = 0.01\n",
    "total_iteration = 1000\n",
    "batch_size      = len(array_y)+1\n",
    "seed_bp         = 0\n",
    "\n",
    "# 2.initialize parameters using _pso\n",
    "W1, W2, W3, W4, b1, b2, b3, b4 = pso(array_x, array_y)\n",
    "\n",
    "# 3.read ANN parameters from PSO and feedforeward\n",
    "a0, s1, a1, s2, a2, s3, a3, s4 = forward_prop(array_x, W1, W2, W3, W4, b1, b2, b3, b4)\n",
    "\n",
    "# 4.keep the initial loss\n",
    "loss_bp = [MSE(array_y, s4)]\n",
    "\n",
    "# 5.reset dataset for MINIBATCH algorithm\n",
    "df_X  = pd.DataFrame(array_x.T)\n",
    "df_y  = pd.DataFrame(array_y.T)\n",
    "\n",
    "# 6.backpropogation using MBSGD\n",
    "for i in range (total_iteration): \n",
    "    # split data into batches\n",
    "    for Batch in batches (df_X, df_y, size = batch_size, seed = seed_bp):\n",
    "        \n",
    "        # get data batches\n",
    "        (Batch_X, Batch_y) = Batch\n",
    "        # transform datches into arrays\n",
    "        Batch_X = np.array(Batch_X).T\n",
    "        Batch_y = np.array(Batch_y).T\n",
    "        \n",
    "        # parameters update\n",
    "        a0, s1, a1, s2, a2, s3, a3, s4 = forward_prop(Batch_X, W1, W2, W3, W4, b1, b2, b3, b4)\n",
    "        W1 -= learn_rate * J_W1 (a0, Batch_y)\n",
    "        W2 -= learn_rate * J_W2 (a0, Batch_y)\n",
    "        W3 -= learn_rate * J_W3 (a0, Batch_y)\n",
    "        W4 -= learn_rate * J_W4 (a0, Batch_y)\n",
    "        b1 -= learn_rate * J_b1 (a0, Batch_y)\n",
    "        b2 -= learn_rate * J_b2 (a0, Batch_y)\n",
    "        b3 -= learn_rate * J_b3 (a0, Batch_y)\n",
    "        b4 -= learn_rate * J_b4 (a0, Batch_y)\n",
    "        \n",
    "    # at the end of each iteration, we keep the loss for the all the instances\n",
    "    a0, s1, a1, s2, a2, s3, a3, s4 = forward_prop(array_x, W1, W2, W3, W4, b1, b2, b3, b4)\n",
    "    loss = MSE(array_y, s4)\n",
    "    loss_bp.append(loss)\n",
    "    \n",
    "    seed_bp += 1\n",
    "    # print('at the interation of {:}, MBSGD finds {:.4f} as MSE'.format(i+1, loss))\n",
    "\n",
    "# 6.plot the diagram of iteration and lossfuntion\n",
    "plt.figure(figsize = (7,4))\n",
    "plt.title ('Loss Diminishing of BackPropagation Using MBSGD')\n",
    "plt.xlabel('Number of Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(range(1001), loss_bp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "44c0f499",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0006978562233113494,\n",
       " 0.0006978562200246018,\n",
       " 0.0006978562181791946,\n",
       " 0.0006978562169961003,\n",
       " 0.0006978562161173471,\n",
       " 0.0006978562153784428,\n",
       " 0.000697856214703801,\n",
       " 0.0006978562140586891,\n",
       " 0.0006978562134271472,\n",
       " 0.0006978562128018416,\n",
       " 0.000697856212179402,\n",
       " 0.0006978562115582796,\n",
       " 0.0006978562109377637,\n",
       " 0.0006978562103175268,\n",
       " 0.0006978562096974183,\n",
       " 0.00069785620907737,\n",
       " 0.0006978562084573494,\n",
       " 0.0006978562078373426,\n",
       " 0.0006978562072173422,\n",
       " 0.0006978562065973456,\n",
       " 0.0006978562059773511,\n",
       " 0.0006978562053573584,\n",
       " 0.0006978562047373668,\n",
       " 0.0006978562041173766,\n",
       " 0.0006978562034973874,\n",
       " 0.0006978562028773993,\n",
       " 0.0006978562022574122,\n",
       " 0.000697856201637426,\n",
       " 0.000697856201017441,\n",
       " 0.0006978562003974574,\n",
       " 0.0006978561997774745,\n",
       " 0.0006978561991574925,\n",
       " 0.0006978561985375114,\n",
       " 0.0006978561979175319,\n",
       " 0.0006978561972975532,\n",
       " 0.0006978561966775757,\n",
       " 0.0006978561960575991,\n",
       " 0.0006978561954376235,\n",
       " 0.0006978561948176487,\n",
       " 0.0006978561941976755,\n",
       " 0.000697856193577703,\n",
       " 0.0006978561929577318,\n",
       " 0.0006978561923377611,\n",
       " 0.0006978561917177921,\n",
       " 0.0006978561910978238,\n",
       " 0.0006978561904778565,\n",
       " 0.0006978561898578905,\n",
       " 0.0006978561892379256,\n",
       " 0.0006978561886179615,\n",
       " 0.0006978561879979982,\n",
       " 0.0006978561873780364,\n",
       " 0.0006978561867580754,\n",
       " 0.0006978561861381155,\n",
       " 0.0006978561855181566,\n",
       " 0.000697856184898199,\n",
       " 0.0006978561842782424,\n",
       " 0.0006978561836582869,\n",
       " 0.0006978561830383324,\n",
       " 0.0006978561824183785,\n",
       " 0.0006978561817984258,\n",
       " 0.0006978561811784742,\n",
       " 0.0006978561805585238,\n",
       " 0.0006978561799385743,\n",
       " 0.0006978561793186263,\n",
       " 0.000697856178698679,\n",
       " 0.0006978561780787325,\n",
       " 0.0006978561774587873,\n",
       " 0.0006978561768388433,\n",
       " 0.0006978561762188998,\n",
       " 0.000697856175598958,\n",
       " 0.0006978561749790168,\n",
       " 0.0006978561743590769,\n",
       " 0.0006978561737391378,\n",
       " 0.0006978561731192004,\n",
       " 0.0006978561724992634,\n",
       " 0.0006978561718793276,\n",
       " 0.0006978561712593927,\n",
       " 0.0006978561706394588,\n",
       " 0.0006978561700195264,\n",
       " 0.000697856169399595,\n",
       " 0.0006978561687796642,\n",
       " 0.0006978561681597345,\n",
       " 0.0006978561675398063,\n",
       " 0.0006978561669198788,\n",
       " 0.0006978561662999525,\n",
       " 0.0006978561656800273,\n",
       " 0.0006978561650601027,\n",
       " 0.0006978561644401796,\n",
       " 0.0006978561638202573,\n",
       " 0.0006978561632003366,\n",
       " 0.0006978561625804162,\n",
       " 0.000697856161960497,\n",
       " 0.0006978561613405789,\n",
       " 0.0006978561607206622,\n",
       " 0.0006978561601007465,\n",
       " 0.0006978561594808315,\n",
       " 0.0006978561588609175,\n",
       " 0.0006978561582410048,\n",
       " 0.0006978561576210928,\n",
       " 0.0006978561570011824,\n",
       " 0.0006978561563812725,\n",
       " 0.0006978561557613642,\n",
       " 0.0006978561551414566,\n",
       " 0.0006978561545215501,\n",
       " 0.0006978561539016446,\n",
       " 0.0006978561532817401,\n",
       " 0.0006978561526618368,\n",
       " 0.0006978561520419348,\n",
       " 0.0006978561514220332,\n",
       " 0.0006978561508021329,\n",
       " 0.0006978561501822337,\n",
       " 0.0006978561495623359,\n",
       " 0.0006978561489424384,\n",
       " 0.0006978561483225425,\n",
       " 0.0006978561477026477,\n",
       " 0.0006978561470827536,\n",
       " 0.0006978561464628609,\n",
       " 0.0006978561458429688,\n",
       " 0.0006978561452230782,\n",
       " 0.0006978561446031884,\n",
       " 0.0006978561439832995,\n",
       " 0.000697856143363412,\n",
       " 0.0006978561427435254,\n",
       " 0.0006978561421236399,\n",
       " 0.0006978561415037552,\n",
       " 0.0006978561408838719,\n",
       " 0.0006978561402639891,\n",
       " 0.000697856139644108,\n",
       " 0.0006978561390242275,\n",
       " 0.0006978561384043486,\n",
       " 0.0006978561377844701,\n",
       " 0.0006978561371645926,\n",
       " 0.0006978561365447168,\n",
       " 0.0006978561359248416,\n",
       " 0.0006978561353049674,\n",
       " 0.0006978561346850943,\n",
       " 0.0006978561340652227,\n",
       " 0.0006978561334453516,\n",
       " 0.0006978561328254819,\n",
       " 0.000697856132205613,\n",
       " 0.0006978561315857452,\n",
       " 0.0006978561309658782,\n",
       " 0.0006978561303460126,\n",
       " 0.0006978561297261481,\n",
       " 0.0006978561291062846,\n",
       " 0.0006978561284864219,\n",
       " 0.0006978561278665603,\n",
       " 0.0006978561272466997,\n",
       " 0.0006978561266268409,\n",
       " 0.0006978561260069823,\n",
       " 0.0006978561253871249,\n",
       " 0.0006978561247672686,\n",
       " 0.0006978561241474131,\n",
       " 0.0006978561235275594,\n",
       " 0.0006978561229077061,\n",
       " 0.0006978561222878538,\n",
       " 0.0006978561216680029,\n",
       " 0.0006978561210481527,\n",
       " 0.0006978561204283043,\n",
       " 0.0006978561198084562,\n",
       " 0.0006978561191886091,\n",
       " 0.0006978561185687633,\n",
       " 0.0006978561179489186,\n",
       " 0.0006978561173290751,\n",
       " 0.0006978561167092325,\n",
       " 0.0006978561160893903,\n",
       " 0.0006978561154695501,\n",
       " 0.0006978561148497104,\n",
       " 0.000697856114229872,\n",
       " 0.0006978561136100345,\n",
       " 0.0006978561129901982,\n",
       " 0.0006978561123703625,\n",
       " 0.0006978561117505282,\n",
       " 0.0006978561111306952,\n",
       " 0.0006978561105108632,\n",
       " 0.0006978561098910321,\n",
       " 0.000697856109271202,\n",
       " 0.0006978561086513728,\n",
       " 0.0006978561080315445,\n",
       " 0.0006978561074117178,\n",
       " 0.0006978561067918915,\n",
       " 0.0006978561061720668,\n",
       " 0.000697856105552243,\n",
       " 0.0006978561049324199,\n",
       " 0.0006978561043125988,\n",
       " 0.0006978561036927778,\n",
       " 0.0006978561030729581,\n",
       " 0.0006978561024531395,\n",
       " 0.0006978561018333218,\n",
       " 0.0006978561012135054,\n",
       " 0.0006978561005936899,\n",
       " 0.0006978560999738754,\n",
       " 0.0006978560993540619,\n",
       " 0.0006978560987342495,\n",
       " 0.0006978560981144386,\n",
       " 0.0006978560974946282,\n",
       " 0.0006978560968748191,\n",
       " 0.0006978560962550108,\n",
       " 0.0006978560956352036,\n",
       " 0.0006978560950153975,\n",
       " 0.0006978560943955926,\n",
       " 0.0006978560937757886,\n",
       " 0.0006978560931559859,\n",
       " 0.0006978560925361839,\n",
       " 0.000697856091916383,\n",
       " 0.0006978560912965833,\n",
       " 0.000697856090676785,\n",
       " 0.000697856090056987,\n",
       " 0.0006978560894371905,\n",
       " 0.0006978560888173947,\n",
       " 0.0006978560881976003,\n",
       " 0.0006978560875778069,\n",
       " 0.0006978560869580139,\n",
       " 0.0006978560863382227,\n",
       " 0.0006978560857184323,\n",
       " 0.0006978560850986433,\n",
       " 0.0006978560844788546,\n",
       " 0.0006978560838590676,\n",
       " 0.0006978560832392814,\n",
       " 0.0006978560826194964,\n",
       " 0.0006978560819997123,\n",
       " 0.0006978560813799291,\n",
       " 0.0006978560807601473,\n",
       " 0.0006978560801403663,\n",
       " 0.0006978560795205865,\n",
       " 0.0006978560789008075,\n",
       " 0.0006978560782810295,\n",
       " 0.0006978560776612528,\n",
       " 0.0006978560770414771,\n",
       " 0.0006978560764217024,\n",
       " 0.000697856075801929,\n",
       " 0.000697856075182156,\n",
       " 0.0006978560745623846,\n",
       " 0.000697856073942614,\n",
       " 0.0006978560733228448,\n",
       " 0.0006978560727030764,\n",
       " 0.0006978560720833089,\n",
       " 0.0006978560714635426,\n",
       " 0.0006978560708437775,\n",
       " 0.0006978560702240131,\n",
       " 0.00069785606960425,\n",
       " 0.000697856068984488,\n",
       " 0.0006978560683647269,\n",
       " 0.0006978560677449667,\n",
       " 0.0006978560671252077,\n",
       " 0.0006978560665054496,\n",
       " 0.000697856065885693,\n",
       " 0.000697856065265937,\n",
       " 0.0006978560646461823,\n",
       " 0.0006978560640264285,\n",
       " 0.0006978560634066759,\n",
       " 0.0006978560627869245,\n",
       " 0.0006978560621671736,\n",
       " 0.000697856061547424,\n",
       " 0.0006978560609276753,\n",
       " 0.000697856060307928,\n",
       " 0.0006978560596881817,\n",
       " 0.0006978560590684363,\n",
       " 0.0006978560584486921,\n",
       " 0.0006978560578289483,\n",
       " 0.0006978560572092062,\n",
       " 0.0006978560565894653,\n",
       " 0.0006978560559697248,\n",
       " 0.0006978560553499859,\n",
       " 0.000697856054730248,\n",
       " 0.0006978560541105104,\n",
       " 0.0006978560534907747,\n",
       " 0.0006978560528710401,\n",
       " 0.0006978560522513062,\n",
       " 0.0006978560516315732,\n",
       " 0.0006978560510118414,\n",
       " 0.0006978560503921108,\n",
       " 0.000697856049772381,\n",
       " 0.0006978560491526525,\n",
       " 0.0006978560485329246,\n",
       " 0.0006978560479131981,\n",
       " 0.0006978560472934726,\n",
       " 0.000697856046673748,\n",
       " 0.0006978560460540251,\n",
       " 0.0006978560454343025,\n",
       " 0.0006978560448145811,\n",
       " 0.0006978560441948609,\n",
       " 0.0006978560435751419,\n",
       " 0.0006978560429554235,\n",
       " 0.0006978560423357062,\n",
       " 0.0006978560417159903,\n",
       " 0.0006978560410962752,\n",
       " 0.0006978560404765612,\n",
       " 0.0006978560398568481,\n",
       " 0.0006978560392371364,\n",
       " 0.0006978560386174253,\n",
       " 0.0006978560379977156,\n",
       " 0.0006978560373780067,\n",
       " 0.000697856036758299,\n",
       " 0.0006978560361385925,\n",
       " 0.0006978560355188866,\n",
       " 0.0006978560348991822,\n",
       " 0.0006978560342794785,\n",
       " 0.0006978560336597761,\n",
       " 0.0006978560330400745,\n",
       " 0.0006978560324203742,\n",
       " 0.0006978560318006748,\n",
       " 0.0006978560311809765,\n",
       " 0.0006978560305612794,\n",
       " 0.0006978560299415831,\n",
       " 0.0006978560293218876,\n",
       " 0.0006978560287021938,\n",
       " 0.0006978560280825006,\n",
       " 0.0006978560274628086,\n",
       " 0.0006978560268431177,\n",
       " 0.0006978560262234278,\n",
       " 0.0006978560256037386,\n",
       " 0.0006978560249840512,\n",
       " 0.0006978560243643641,\n",
       " 0.0006978560237446785,\n",
       " 0.0006978560231249936,\n",
       " 0.00069785602250531,\n",
       " 0.0006978560218856275,\n",
       " 0.0006978560212659457,\n",
       " 0.0006978560206462651,\n",
       " 0.0006978560200265858,\n",
       " 0.0006978560194069072,\n",
       " 0.0006978560187872296,\n",
       " 0.0006978560181675535,\n",
       " 0.0006978560175478784,\n",
       " 0.0006978560169282038,\n",
       " 0.0006978560163085305,\n",
       " 0.0006978560156888587,\n",
       " 0.0006978560150691876,\n",
       " 0.0006978560144495172,\n",
       " 0.0006978560138298486,\n",
       " 0.0006978560132101803,\n",
       " 0.0006978560125905135,\n",
       " 0.0006978560119708477,\n",
       " 0.000697856011351183,\n",
       " 0.0006978560107315188,\n",
       " 0.0006978560101118561,\n",
       " 0.0006978560094921947,\n",
       " 0.0006978560088725335,\n",
       " 0.0006978560082528739,\n",
       " 0.0006978560076332155,\n",
       " 0.0006978560070135578,\n",
       " 0.0006978560063939017,\n",
       " 0.000697856005774246,\n",
       " 0.0006978560051545916,\n",
       " 0.0006978560045349384,\n",
       " 0.0006978560039152859,\n",
       " 0.0006978560032956349,\n",
       " 0.0006978560026759844,\n",
       " 0.0006978560020563354,\n",
       " 0.0006978560014366876,\n",
       " 0.00069785600081704,\n",
       " 0.0006978560001973942,\n",
       " 0.0006978559995777494,\n",
       " 0.0006978559989581054,\n",
       " 0.0006978559983384624,\n",
       " 0.0006978559977188207,\n",
       " 0.0006978559970991801,\n",
       " 0.0006978559964795399,\n",
       " 0.0006978559958599012,\n",
       " 0.0006978559952402636,\n",
       " 0.0006978559946206271,\n",
       " 0.0006978559940009915,\n",
       " 0.0006978559933813569,\n",
       " 0.0006978559927617236,\n",
       " 0.0006978559921420911,\n",
       " 0.0006978559915224598,\n",
       " 0.0006978559909028295,\n",
       " 0.0006978559902832002,\n",
       " 0.0006978559896635718,\n",
       " 0.0006978559890439442,\n",
       " 0.0006978559884243186,\n",
       " 0.0006978559878046931,\n",
       " 0.0006978559871850693,\n",
       " 0.0006978559865654467,\n",
       " 0.0006978559859458243,\n",
       " 0.0006978559853262035,\n",
       " 0.0006978559847065837,\n",
       " 0.0006978559840869644,\n",
       " 0.0006978559834673469,\n",
       " 0.00069785598284773,\n",
       " 0.0006978559822281145,\n",
       " 0.0006978559816084998,\n",
       " 0.0006978559809888862,\n",
       " 0.0006978559803692738,\n",
       " 0.0006978559797496622,\n",
       " 0.0006978559791300516,\n",
       " 0.0006978559785104422,\n",
       " 0.0006978559778908338,\n",
       " 0.0006978559772712267,\n",
       " 0.0006978559766516202,\n",
       " 0.000697855976032015,\n",
       " 0.0006978559754124109,\n",
       " 0.0006978559747928078,\n",
       " 0.0006978559741732056,\n",
       " 0.0006978559735536045,\n",
       " 0.0006978559729340045,\n",
       " 0.0006978559723144058,\n",
       " 0.0006978559716948079,\n",
       " 0.000697855971075211,\n",
       " 0.0006978559704556152,\n",
       " 0.00069785596983602,\n",
       " 0.0006978559692164266,\n",
       " 0.0006978559685968338,\n",
       " 0.0006978559679772421,\n",
       " 0.0006978559673576516,\n",
       " 0.0006978559667380619,\n",
       " 0.0006978559661184737,\n",
       " 0.0006978559654988862,\n",
       " 0.0006978559648792999,\n",
       " 0.000697855964259714,\n",
       " 0.0006978559636401299,\n",
       " 0.0006978559630205469,\n",
       " 0.0006978559624009644,\n",
       " 0.000697855961781383,\n",
       " 0.000697855961161803,\n",
       " 0.000697855960542224,\n",
       " 0.0006978559599226458,\n",
       " 0.0006978559593030688,\n",
       " 0.0006978559586834932,\n",
       " 0.0006978559580639183,\n",
       " 0.0006978559574443442,\n",
       " 0.0006978559568247713,\n",
       " 0.0006978559562051997,\n",
       " 0.0006978559555856287,\n",
       " 0.0006978559549660591,\n",
       " 0.0006978559543464904,\n",
       " 0.0006978559537269231,\n",
       " 0.0006978559531073563,\n",
       " 0.0006978559524877908,\n",
       " 0.0006978559518682265,\n",
       " 0.0006978559512486633,\n",
       " 0.0006978559506291008,\n",
       " 0.0006978559500095395,\n",
       " 0.0006978559493899791,\n",
       " 0.0006978559487704196,\n",
       " 0.0006978559481508613,\n",
       " 0.0006978559475313047,\n",
       " 0.0006978559469117486,\n",
       " 0.0006978559462921932,\n",
       " 0.0006978559456726394,\n",
       " 0.0006978559450530865,\n",
       " 0.0006978559444335344,\n",
       " 0.0006978559438139839,\n",
       " 0.0006978559431944337,\n",
       " 0.0006978559425748848,\n",
       " 0.0006978559419553376,\n",
       " 0.000697855941335791,\n",
       " 0.0006978559407162447,\n",
       " 0.0006978559400967006,\n",
       " 0.0006978559394771569,\n",
       " 0.0006978559388576143,\n",
       " 0.000697855938238073,\n",
       " 0.0006978559376185326,\n",
       " 0.0006978559369989932,\n",
       " 0.0006978559363794549,\n",
       " 0.0006978559357599176,\n",
       " 0.0006978559351403815,\n",
       " 0.0006978559345208462,\n",
       " 0.0006978559339013122,\n",
       " 0.000697855933281779,\n",
       " 0.000697855932662247,\n",
       " 0.0006978559320427157,\n",
       " 0.000697855931423186,\n",
       " 0.0006978559308036571,\n",
       " 0.0006978559301841293,\n",
       " 0.0006978559295646026,\n",
       " 0.0006978559289450766,\n",
       " 0.0006978559283255521,\n",
       " 0.0006978559277060285,\n",
       " 0.0006978559270865056,\n",
       " 0.0006978559264669841,\n",
       " 0.0006978559258474636,\n",
       " 0.0006978559252279442,\n",
       " 0.0006978559246084255,\n",
       " 0.000697855923988908,\n",
       " 0.0006978559233693918,\n",
       " 0.0006978559227498768,\n",
       " 0.0006978559221303621,\n",
       " 0.000697855921510849,\n",
       " 0.0006978559208913371,\n",
       " 0.0006978559202718258,\n",
       " 0.0006978559196523158,\n",
       " 0.0006978559190328066,\n",
       " 0.0006978559184132987,\n",
       " 0.0006978559177937916,\n",
       " 0.0006978559171742855,\n",
       " 0.0006978559165547807,\n",
       " 0.000697855915935277,\n",
       " 0.0006978559153157743,\n",
       " 0.0006978559146962725,\n",
       " 0.0006978559140767719,\n",
       " 0.0006978559134572722,\n",
       " 0.0006978559128377736,\n",
       " 0.0006978559122182761,\n",
       " 0.0006978559115987796,\n",
       " 0.0006978559109792838,\n",
       " 0.0006978559103597901,\n",
       " 0.0006978559097402965,\n",
       " 0.0006978559091208043,\n",
       " 0.0006978559085013131,\n",
       " 0.0006978559078818225,\n",
       " 0.0006978559072623336,\n",
       " 0.0006978559066428453,\n",
       " 0.0006978559060233584,\n",
       " 0.0006978559054038721,\n",
       " 0.0006978559047843873,\n",
       " 0.0006978559041649034,\n",
       " 0.0006978559035454204,\n",
       " 0.0006978559029259386,\n",
       " 0.0006978559023064576,\n",
       " 0.0006978559016869781,\n",
       " 0.0006978559010674995,\n",
       " 0.0006978559004480217,\n",
       " 0.0006978558998285451,\n",
       " 0.0006978558992090697,\n",
       " 0.0006978558985895946,\n",
       " 0.0006978558979701212,\n",
       " 0.0006978558973506489,\n",
       " 0.0006978558967311775,\n",
       " 0.0006978558961117073,\n",
       " 0.0006978558954922381,\n",
       " 0.0006978558948727696,\n",
       " 0.0006978558942533024,\n",
       " 0.0006978558936338362,\n",
       " 0.0006978558930143709,\n",
       " 0.000697855892394907,\n",
       " 0.000697855891775444,\n",
       " 0.000697855891155982,\n",
       " 0.000697855890536521,\n",
       " 0.000697855889917061,\n",
       " 0.0006978558892976023,\n",
       " 0.0006978558886781445,\n",
       " 0.0006978558880586878,\n",
       " 0.000697855887439232,\n",
       " 0.0006978558868197774,\n",
       " 0.0006978558862003236,\n",
       " 0.0006978558855808709,\n",
       " 0.0006978558849614196,\n",
       " 0.000697855884341969,\n",
       " 0.0006978558837225195,\n",
       " 0.0006978558831030712,\n",
       " 0.0006978558824836237,\n",
       " 0.0006978558818641778,\n",
       " 0.0006978558812447325,\n",
       " 0.0006978558806252882,\n",
       " 0.000697855880005845,\n",
       " 0.0006978558793864029,\n",
       " 0.0006978558787669617,\n",
       " 0.0006978558781475217,\n",
       " 0.0006978558775280826,\n",
       " 0.0006978558769086448,\n",
       " 0.0006978558762892076,\n",
       " 0.0006978558756697719,\n",
       " 0.0006978558750503371,\n",
       " 0.0006978558744309036,\n",
       " 0.0006978558738114707,\n",
       " 0.0006978558731920388,\n",
       " 0.0006978558725726085,\n",
       " 0.0006978558719531788,\n",
       " 0.0006978558713337503,\n",
       " 0.0006978558707143231,\n",
       " 0.0006978558700948961,\n",
       " 0.000697855869475471,\n",
       " 0.0006978558688560467,\n",
       " 0.0006978558682366232,\n",
       " 0.000697855867617201,\n",
       " 0.0006978558669977798,\n",
       " 0.0006978558663783593,\n",
       " 0.0006978558657589405,\n",
       " 0.0006978558651395222,\n",
       " 0.0006978558645201053,\n",
       " 0.0006978558639006894,\n",
       " 0.0006978558632812743,\n",
       " 0.0006978558626618604,\n",
       " 0.0006978558620424478,\n",
       " 0.0006978558614230357,\n",
       " 0.0006978558608036251,\n",
       " 0.0006978558601842155,\n",
       " 0.0006978558595648069,\n",
       " 0.0006978558589453988,\n",
       " 0.0006978558583259925,\n",
       " 0.0006978558577065868,\n",
       " 0.0006978558570871825,\n",
       " 0.000697855856467779,\n",
       " 0.0006978558558483766,\n",
       " 0.0006978558552289749,\n",
       " 0.000697855854609575,\n",
       " 0.0006978558539901755,\n",
       " 0.0006978558533707774,\n",
       " 0.0006978558527513801,\n",
       " 0.000697855852131984,\n",
       " 0.0006978558515125887,\n",
       " 0.0006978558508931949,\n",
       " 0.0006978558502738016,\n",
       " 0.0006978558496544097,\n",
       " 0.0006978558490350189,\n",
       " 0.0006978558484156289,\n",
       " 0.0006978558477962404,\n",
       " 0.0006978558471768525,\n",
       " 0.000697855846557466,\n",
       " 0.0006978558459380803,\n",
       " 0.0006978558453186956,\n",
       " 0.000697855844699312,\n",
       " 0.0006978558440799296,\n",
       " 0.000697855843460548,\n",
       " 0.0006978558428411677,\n",
       " 0.000697855842221788,\n",
       " 0.0006978558416024097,\n",
       " 0.0006978558409830325,\n",
       " 0.0006978558403636562,\n",
       " 0.0006978558397442808,\n",
       " 0.0006978558391249069,\n",
       " 0.0006978558385055334,\n",
       " 0.0006978558378861614,\n",
       " 0.0006978558372667907,\n",
       " 0.0006978558366474203,\n",
       " 0.0006978558360280514,\n",
       " 0.0006978558354086836,\n",
       " 0.0006978558347893166,\n",
       " 0.0006978558341699506,\n",
       " 0.0006978558335505859,\n",
       " 0.0006978558329312223,\n",
       " 0.00069785583231186,\n",
       " 0.0006978558316924981,\n",
       " 0.0006978558310731374,\n",
       " 0.0006978558304537781,\n",
       " 0.0006978558298344192,\n",
       " 0.0006978558292150618,\n",
       " 0.0006978558285957054,\n",
       " 0.0006978558279763503,\n",
       " 0.0006978558273569958,\n",
       " 0.0006978558267376426,\n",
       " 0.0006978558261182905,\n",
       " 0.0006978558254989389,\n",
       " 0.000697855824879589,\n",
       " 0.0006978558242602399,\n",
       " 0.000697855823640892,\n",
       " 0.0006978558230215445,\n",
       " 0.0006978558224021987,\n",
       " 0.000697855821782854,\n",
       " 0.0006978558211635098,\n",
       " 0.0006978558205441673,\n",
       " 0.0006978558199248257,\n",
       " 0.0006978558193054843,\n",
       " 0.0006978558186861451,\n",
       " 0.0006978558180668062,\n",
       " 0.0006978558174474688,\n",
       " 0.0006978558168281322,\n",
       " 0.0006978558162087967,\n",
       " 0.0006978558155894624,\n",
       " 0.0006978558149701289,\n",
       " 0.0006978558143507964,\n",
       " 0.0006978558137314653,\n",
       " 0.0006978558131121349,\n",
       " 0.0006978558124928052,\n",
       " 0.0006978558118734772,\n",
       " 0.0006978558112541502,\n",
       " 0.0006978558106348241,\n",
       " 0.0006978558100154991,\n",
       " 0.0006978558093961753,\n",
       " 0.000697855808776852,\n",
       " 0.0006978558081575305,\n",
       " 0.0006978558075382092,\n",
       " 0.0006978558069188894,\n",
       " 0.0006978558062995709,\n",
       " 0.000697855805680253,\n",
       " 0.0006978558050609365,\n",
       " 0.0006978558044416207,\n",
       " 0.000697855803822306,\n",
       " 0.0006978558032029923,\n",
       " 0.0006978558025836796,\n",
       " 0.0006978558019643683,\n",
       " 0.000697855801345058,\n",
       " 0.0006978558007257487,\n",
       " 0.0006978558001064403,\n",
       " 0.0006978557994871328,\n",
       " 0.0006978557988678266,\n",
       " 0.0006978557982485217,\n",
       " 0.0006978557976292175,\n",
       " 0.0006978557970099144,\n",
       " 0.0006978557963906122,\n",
       " 0.0006978557957713111,\n",
       " 0.0006978557951520111,\n",
       " 0.0006978557945327127,\n",
       " 0.0006978557939134144,\n",
       " 0.0006978557932941175,\n",
       " 0.0006978557926748218,\n",
       " 0.0006978557920555272,\n",
       " 0.0006978557914362334,\n",
       " 0.0006978557908169407,\n",
       " 0.000697855790197649,\n",
       " 0.0006978557895783585,\n",
       " 0.000697855788959069,\n",
       " 0.0006978557883397803,\n",
       " 0.000697855787720493,\n",
       " 0.0006978557871012068,\n",
       " 0.0006978557864819212,\n",
       " 0.0006978557858626367,\n",
       " 0.0006978557852433537,\n",
       " 0.0006978557846240717,\n",
       " 0.0006978557840047903,\n",
       " 0.0006978557833855105,\n",
       " 0.0006978557827662313,\n",
       " 0.0006978557821469533,\n",
       " 0.0006978557815276764,\n",
       " 0.0006978557809084004,\n",
       " 0.0006978557802891255,\n",
       " 0.0006978557796698518,\n",
       " 0.0006978557790505787,\n",
       " 0.0006978557784313069,\n",
       " 0.0006978557778120363,\n",
       " 0.0006978557771927667,\n",
       " 0.0006978557765734982,\n",
       " 0.0006978557759542304,\n",
       " 0.0006978557753349638,\n",
       " 0.0006978557747156988,\n",
       " 0.0006978557740964341,\n",
       " 0.0006978557734771706,\n",
       " 0.0006978557728579085,\n",
       " 0.0006978557722386469,\n",
       " 0.0006978557716193862,\n",
       " 0.0006978557710001273,\n",
       " 0.0006978557703808691,\n",
       " 0.0006978557697616121,\n",
       " 0.0006978557691423559,\n",
       " 0.0006978557685231013,\n",
       " 0.0006978557679038469,\n",
       " 0.0006978557672845938,\n",
       " 0.0006978557666653423,\n",
       " 0.0006978557660460914,\n",
       " 0.0006978557654268416,\n",
       " 0.0006978557648075931,\n",
       " 0.000697855764188345,\n",
       " 0.000697855763569098,\n",
       " 0.0006978557629498525,\n",
       " 0.0006978557623306083,\n",
       " 0.0006978557617113647,\n",
       " 0.0006978557610921224,\n",
       " 0.0006978557604728805,\n",
       " 0.0006978557598536402,\n",
       " 0.0006978557592344011,\n",
       " 0.0006978557586151627,\n",
       " 0.0006978557579959253,\n",
       " 0.0006978557573766889,\n",
       " 0.0006978557567574541,\n",
       " 0.00069785575613822,\n",
       " 0.0006978557555189867,\n",
       " 0.0006978557548997545,\n",
       " 0.0006978557542805235,\n",
       " 0.0006978557536612938,\n",
       " 0.0006978557530420646,\n",
       " 0.0006978557524228368,\n",
       " 0.0006978557518036098,\n",
       " 0.0006978557511843844,\n",
       " 0.0006978557505651597,\n",
       " 0.0006978557499459359,\n",
       " 0.0006978557493267135,\n",
       " 0.0006978557487074915,\n",
       " 0.000697855748088271,\n",
       " 0.0006978557474690515,\n",
       " 0.0006978557468498331,\n",
       " 0.0006978557462306156,\n",
       " 0.0006978557456113995,\n",
       " 0.0006978557449921839,\n",
       " 0.0006978557443729698,\n",
       " 0.0006978557437537565,\n",
       " 0.0006978557431345443,\n",
       " 0.0006978557425153332,\n",
       " 0.000697855741896123,\n",
       " 0.0006978557412769141,\n",
       " 0.0006978557406577059,\n",
       " 0.000697855740038499,\n",
       " 0.0006978557394192932,\n",
       " 0.000697855738800088,\n",
       " 0.0006978557381808842,\n",
       " 0.0006978557375616814,\n",
       " 0.0006978557369424797,\n",
       " 0.0006978557363232792,\n",
       " 0.0006978557357040796,\n",
       " 0.0006978557350848809,\n",
       " 0.0006978557344656831,\n",
       " 0.0006978557338464868,\n",
       " 0.0006978557332272911,\n",
       " 0.0006978557326080966,\n",
       " 0.0006978557319889032,\n",
       " 0.0006978557313697113,\n",
       " 0.0006978557307505198,\n",
       " 0.0006978557301313297,\n",
       " 0.0006978557295121403,\n",
       " 0.0006978557288929524,\n",
       " 0.0006978557282737655,\n",
       " 0.0006978557276545792,\n",
       " 0.0006978557270353943,\n",
       " 0.0006978557264162102,\n",
       " 0.0006978557257970271,\n",
       " 0.0006978557251778451,\n",
       " 0.0006978557245586643,\n",
       " 0.0006978557239394843,\n",
       " 0.0006978557233203057,\n",
       " 0.0006978557227011283,\n",
       " 0.0006978557220819515,\n",
       " 0.0006978557214627759,\n",
       " 0.0006978557208436012,\n",
       " 0.0006978557202244278,\n",
       " 0.0006978557196052555,\n",
       " 0.0006978557189860838,\n",
       " 0.0006978557183669133,\n",
       " 0.0006978557177477439,\n",
       " 0.000697855717128576,\n",
       " 0.0006978557165094085,\n",
       " 0.0006978557158902424,\n",
       " 0.0006978557152710769,\n",
       " 0.000697855714651913,\n",
       " 0.0006978557140327499,\n",
       " 0.0006978557134135878,\n",
       " 0.0006978557127944267,\n",
       " 0.0006978557121752667,\n",
       " 0.000697855711556108,\n",
       " 0.0006978557109369503,\n",
       " 0.0006978557103177932,\n",
       " 0.0006978557096986375,\n",
       " 0.0006978557090794829,\n",
       " 0.0006978557084603295,\n",
       " 0.0006978557078411763,\n",
       " 0.0006978557072220249,\n",
       " 0.0006978557066028745,\n",
       " 0.0006978557059837245,\n",
       " 0.0006978557053645765,\n",
       " 0.0006978557047454288,\n",
       " 0.0006978557041262825,\n",
       " 0.0006978557035071372,\n",
       " 0.0006978557028879932,\n",
       " 0.0006978557022688499,\n",
       " 0.0006978557016497076,\n",
       " 0.0006978557010305663,\n",
       " 0.0006978557004114265,\n",
       " 0.0006978556997922873,\n",
       " 0.0006978556991731492,\n",
       " 0.0006978556985540125,\n",
       " 0.0006978556979348766,\n",
       " 0.0006978556973157413,\n",
       " 0.0006978556966966076,\n",
       " 0.0006978556960774748,\n",
       " 0.0006978556954583432,\n",
       " 0.0006978556948392126,\n",
       " 0.0006978556942200828,\n",
       " 0.0006978556936009542,\n",
       " 0.000697855692981827,\n",
       " 0.0006978556923627003,\n",
       " 0.0006978556917435747,\n",
       " 0.0006978556911244503,\n",
       " 0.000697855690505327,\n",
       " 0.0006978556898862047,\n",
       " 0.0006978556892670832,\n",
       " 0.0006978556886479632,\n",
       " 0.0006978556880288436,\n",
       " 0.0006978556874097257,\n",
       " 0.0006978556867906085,\n",
       " 0.0006978556861714921,\n",
       " 0.0006978556855523775,\n",
       " 0.0006978556849332635,\n",
       " 0.0006978556843141504,\n",
       " 0.0006978556836950388,\n",
       " 0.0006978556830759277,\n",
       " 0.0006978556824568181,\n",
       " 0.0006978556818377092,\n",
       " 0.0006978556812186014,\n",
       " 0.0006978556805994946,\n",
       " 0.0006978556799803893,\n",
       " 0.0006978556793612846,\n",
       " 0.0006978556787421812,\n",
       " 0.0006978556781230786,\n",
       " 0.0006978556775039771,\n",
       " 0.0006978556768848769,\n",
       " 0.0006978556762657774,\n",
       " 0.000697855675646679,\n",
       " 0.0006978556750275815,\n",
       " 0.0006978556744084856,\n",
       " 0.0006978556737893903,\n",
       " 0.0006978556731702962,\n",
       " 0.0006978556725512033,\n",
       " 0.0006978556719321111,\n",
       " 0.0006978556713130202,\n",
       " 0.0006978556706939301,\n",
       " 0.0006978556700748415,\n",
       " 0.0006978556694557536,\n",
       " 0.0006978556688366669,\n",
       " 0.0006978556682175806,\n",
       " 0.0006978556675984962,\n",
       " 0.0006978556669794124,\n",
       " 0.0006978556663603299,\n",
       " 0.0006978556657412481,\n",
       " 0.0006978556651221677,\n",
       " 0.0006978556645030882,\n",
       " 0.0006978556638840097,\n",
       " 0.0006978556632649321,\n",
       " 0.0006978556626458559,\n",
       " 0.0006978556620267807,\n",
       " 0.0006978556614077066,\n",
       " 0.0006978556607886333,\n",
       " 0.0006978556601695609,\n",
       " 0.0006978556595504896,\n",
       " 0.0006978556589314196,\n",
       " 0.0006978556583123504,\n",
       " 0.0006978556576932823,\n",
       " 0.0006978556570742156,\n",
       " 0.0006978556564551497,\n",
       " 0.0006978556558360847,\n",
       " 0.0006978556552170209,\n",
       " 0.0006978556545979581,\n",
       " 0.0006978556539788965,\n",
       " 0.0006978556533598357,\n",
       " 0.0006978556527407764,\n",
       " 0.0006978556521217172,\n",
       " 0.0006978556515026601,\n",
       " 0.0006978556508836035,\n",
       " 0.0006978556502645481,\n",
       " 0.0006978556496454934,\n",
       " 0.00069785564902644,\n",
       " 0.0006978556484073877,\n",
       " 0.0006978556477883365,\n",
       " 0.0006978556471692862,\n",
       " 0.000697855646550237,\n",
       " 0.0006978556459311887,\n",
       " 0.0006978556453121416,\n",
       " 0.0006978556446930957,\n",
       " 0.0006978556440740503,\n",
       " 0.0006978556434550067,\n",
       " 0.0006978556428359637,\n",
       " 0.0006978556422169215,\n",
       " 0.0006978556415978807,\n",
       " 0.0006978556409788408,\n",
       " 0.0006978556403598024,\n",
       " 0.0006978556397407646,\n",
       " 0.0006978556391217279,\n",
       " 0.0006978556385026924,\n",
       " 0.0006978556378836577,\n",
       " 0.0006978556372646244,\n",
       " 0.0006978556366455918,\n",
       " 0.0006978556360265602,\n",
       " 0.0006978556354075298,\n",
       " 0.0006978556347885008,\n",
       " 0.0006978556341694722,\n",
       " 0.0006978556335504449,\n",
       " 0.0006978556329314188,\n",
       " 0.0006978556323123931,\n",
       " 0.0006978556316933694,\n",
       " 0.0006978556310743461,\n",
       " 0.0006978556304553244,\n",
       " 0.0006978556298363033,\n",
       " 0.0006978556292172831,\n",
       " 0.0006978556285982644,\n",
       " 0.0006978556279792463,\n",
       " 0.0006978556273602294,\n",
       " 0.0006978556267412139,\n",
       " 0.0006978556261221987,\n",
       " 0.0006978556255031854,\n",
       " 0.0006978556248841727,\n",
       " 0.0006978556242651611,\n",
       " 0.0006978556236461505,\n",
       " 0.0006978556230271412,\n",
       " 0.0006978556224081327,\n",
       " 0.0006978556217891253,\n",
       " 0.000697855621170119,\n",
       " 0.0006978556205511136,\n",
       " 0.0006978556199321091,\n",
       " 0.000697855619313106,\n",
       " 0.0006978556186941038,\n",
       " 0.0006978556180751021,\n",
       " 0.0006978556174561026,\n",
       " 0.0006978556168371031,\n",
       " 0.0006978556162181052,\n",
       " 0.0006978556155991086,\n",
       " 0.0006978556149801127,\n",
       " 0.0006978556143611176,\n",
       " 0.0006978556137421235,\n",
       " 0.0006978556131231307,\n",
       " 0.000697855612504139,\n",
       " 0.0006978556118851484,\n",
       " 0.0006978556112661585,\n",
       " 0.0006978556106471701,\n",
       " 0.0006978556100281826,\n",
       " 0.0006978556094091959,\n",
       " 0.0006978556087902105,\n",
       " 0.0006978556081712259,\n",
       " 0.0006978556075522425,\n",
       " 0.0006978556069332603,\n",
       " 0.0006978556063142791,\n",
       " 0.0006978556056952984,\n",
       " 0.0006978556050763194,\n",
       " 0.0006978556044573411,\n",
       " 0.0006978556038383641,\n",
       " 0.0006978556032193879,\n",
       " 0.0006978556026004129,\n",
       " 0.000697855601981439,\n",
       " 0.0006978556013624658,\n",
       " 0.0006978556007434941,\n",
       " 0.0006978556001245233,\n",
       " 0.0006978555995055536,\n",
       " ...]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e846d30a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "254px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
